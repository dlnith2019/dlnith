{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "BJT_Amp_Current_Gain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0z3yq-AZmaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5isTD3WhJuk3",
        "colab_type": "text"
      },
      "source": [
        "  Import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNlRgByIJqgu",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBXXhp7M0OcB",
        "colab_type": "code",
        "outputId": "35c683fd-fd3a-4d67-c957-c0942ad636a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "d = pd.read_csv('datatrain.csv')\n",
        "X = d[d.columns[0:4]].values\n",
        "y = d[d.columns[4]]\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSvBATWpJ35m",
        "colab_type": "text"
      },
      "source": [
        "Fit data in Sequential Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qw0s23EHE9w",
        "colab_type": "code",
        "outputId": "a69006c5-cd16-4c9c-f048-d6d0c706ea13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X=X/10\n",
        "y=y/1000\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "model = Sequential()\n",
        "model.add(Dense(20, activation='relu',input_dim = 4))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, y_train,batch_size=100,epochs=500,verbose=1,validation_data=(X_test, y_test))\n",
        "print(history.history.keys())\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 20)                100       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 431\n",
            "Trainable params: 431\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 63 samples, validate on 16 samples\n",
            "Epoch 1/500\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0348 - val_loss: 0.0274\n",
            "Epoch 2/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 0.0300 - val_loss: 0.0229\n",
            "Epoch 3/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 0.0251 - val_loss: 0.0188\n",
            "Epoch 4/500\n",
            "63/63 [==============================] - 0s 43us/step - loss: 0.0205 - val_loss: 0.0152\n",
            "Epoch 5/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 0.0165 - val_loss: 0.0121\n",
            "Epoch 6/500\n",
            "63/63 [==============================] - 0s 39us/step - loss: 0.0130 - val_loss: 0.0094\n",
            "Epoch 7/500\n",
            "63/63 [==============================] - 0s 44us/step - loss: 0.0101 - val_loss: 0.0072\n",
            "Epoch 8/500\n",
            "63/63 [==============================] - 0s 40us/step - loss: 0.0078 - val_loss: 0.0054\n",
            "Epoch 9/500\n",
            "63/63 [==============================] - 0s 41us/step - loss: 0.0060 - val_loss: 0.0040\n",
            "Epoch 10/500\n",
            "63/63 [==============================] - 0s 39us/step - loss: 0.0046 - val_loss: 0.0031\n",
            "Epoch 11/500\n",
            "63/63 [==============================] - 0s 41us/step - loss: 0.0036 - val_loss: 0.0024\n",
            "Epoch 12/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 0.0029 - val_loss: 0.0020\n",
            "Epoch 13/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 14/500\n",
            "63/63 [==============================] - 0s 43us/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 15/500\n",
            "63/63 [==============================] - 0s 40us/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 16/500\n",
            "63/63 [==============================] - 0s 40us/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 17/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 18/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 19/500\n",
            "63/63 [==============================] - 0s 38us/step - loss: 0.0029 - val_loss: 0.0027\n",
            "Epoch 20/500\n",
            "63/63 [==============================] - 0s 39us/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 21/500\n",
            "63/63 [==============================] - 0s 47us/step - loss: 0.0033 - val_loss: 0.0030\n",
            "Epoch 22/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 23/500\n",
            "63/63 [==============================] - 0s 38us/step - loss: 0.0036 - val_loss: 0.0031\n",
            "Epoch 24/500\n",
            "63/63 [==============================] - 0s 41us/step - loss: 0.0036 - val_loss: 0.0031\n",
            "Epoch 25/500\n",
            "63/63 [==============================] - 0s 42us/step - loss: 0.0036 - val_loss: 0.0031\n",
            "Epoch 26/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 0.0035 - val_loss: 0.0030\n",
            "Epoch 27/500\n",
            "63/63 [==============================] - 0s 38us/step - loss: 0.0035 - val_loss: 0.0029\n",
            "Epoch 28/500\n",
            "63/63 [==============================] - 0s 42us/step - loss: 0.0033 - val_loss: 0.0027\n",
            "Epoch 29/500\n",
            "63/63 [==============================] - 0s 40us/step - loss: 0.0032 - val_loss: 0.0026\n",
            "Epoch 30/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 31/500\n",
            "63/63 [==============================] - 0s 44us/step - loss: 0.0029 - val_loss: 0.0023\n",
            "Epoch 32/500\n",
            "63/63 [==============================] - 0s 48us/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 33/500\n",
            "63/63 [==============================] - 0s 48us/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 34/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 35/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 0.0024 - val_loss: 0.0018\n",
            "Epoch 36/500\n",
            "63/63 [==============================] - 0s 48us/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 37/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 38/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 39/500\n",
            "63/63 [==============================] - 0s 47us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 40/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 41/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 42/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 43/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 44/500\n",
            "63/63 [==============================] - 0s 74us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 45/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 46/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 47/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 48/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 49/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 50/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 51/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 52/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 53/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 54/500\n",
            "63/63 [==============================] - 0s 72us/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 55/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 56/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 57/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 58/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 59/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 60/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 61/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 62/500\n",
            "63/63 [==============================] - 0s 82us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 63/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 64/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 65/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 66/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 67/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 68/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 69/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 70/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 71/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 72/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 73/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 74/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 75/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0019 - val_loss: 0.0016\n",
            "Epoch 76/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 77/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 78/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 79/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 80/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 81/500\n",
            "63/63 [==============================] - 0s 70us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 82/500\n",
            "63/63 [==============================] - 0s 73us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 83/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 84/500\n",
            "63/63 [==============================] - 0s 128us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 85/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 86/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 87/500\n",
            "63/63 [==============================] - 0s 75us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 88/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 89/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 90/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 91/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 92/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 93/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 94/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 95/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 96/500\n",
            "63/63 [==============================] - 0s 47us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 97/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 98/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 99/500\n",
            "63/63 [==============================] - 0s 69us/step - loss: 0.0018 - val_loss: 0.0015\n",
            "Epoch 100/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 101/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 102/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 103/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 104/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 105/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 106/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 107/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 108/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 109/500\n",
            "63/63 [==============================] - 0s 69us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 110/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 111/500\n",
            "63/63 [==============================] - 0s 69us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 112/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 113/500\n",
            "63/63 [==============================] - 0s 70us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 114/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 115/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 116/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 117/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 118/500\n",
            "63/63 [==============================] - 0s 47us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 119/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 120/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 121/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 122/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 123/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 124/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 125/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 126/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 127/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 128/500\n",
            "63/63 [==============================] - 0s 77us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 129/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 130/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 131/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 132/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 133/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 134/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 135/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 136/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 137/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 138/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 139/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 140/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 141/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 142/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 143/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 144/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 145/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 146/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 147/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 148/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 149/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 150/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 151/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 152/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 153/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 154/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 155/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 156/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 157/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 158/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 159/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 160/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 161/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 162/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 163/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 164/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 165/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 166/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 167/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 168/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 169/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 170/500\n",
            "63/63 [==============================] - 0s 75us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 171/500\n",
            "63/63 [==============================] - 0s 69us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 172/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 173/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 174/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 175/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 176/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 177/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 178/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 179/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 180/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 181/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 182/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 183/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 184/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 185/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 186/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 187/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 188/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 189/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 190/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 191/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 192/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 193/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 194/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 195/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 196/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 197/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 198/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 199/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 200/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 201/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 202/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 203/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 204/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 205/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 206/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 207/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 208/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 209/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 210/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 211/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 212/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 213/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 214/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 215/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 216/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 217/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 218/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 219/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 220/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 221/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 222/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 223/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 224/500\n",
            "63/63 [==============================] - 0s 77us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 225/500\n",
            "63/63 [==============================] - 0s 72us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 226/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 227/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 228/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 229/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 230/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 9.9722e-04 - val_loss: 0.0011\n",
            "Epoch 231/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 9.9174e-04 - val_loss: 0.0011\n",
            "Epoch 232/500\n",
            "63/63 [==============================] - 0s 70us/step - loss: 9.8524e-04 - val_loss: 0.0011\n",
            "Epoch 233/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 9.7742e-04 - val_loss: 0.0011\n",
            "Epoch 234/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 9.6733e-04 - val_loss: 0.0011\n",
            "Epoch 235/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 9.5555e-04 - val_loss: 0.0011\n",
            "Epoch 236/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 9.3868e-04 - val_loss: 0.0010\n",
            "Epoch 237/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 9.1312e-04 - val_loss: 0.0010\n",
            "Epoch 238/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 8.7635e-04 - val_loss: 9.8749e-04\n",
            "Epoch 239/500\n",
            "63/63 [==============================] - 0s 43us/step - loss: 8.2902e-04 - val_loss: 9.4404e-04\n",
            "Epoch 240/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 7.7868e-04 - val_loss: 8.8312e-04\n",
            "Epoch 241/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 7.3284e-04 - val_loss: 8.2242e-04\n",
            "Epoch 242/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 6.9853e-04 - val_loss: 7.6996e-04\n",
            "Epoch 243/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 6.7732e-04 - val_loss: 7.2645e-04\n",
            "Epoch 244/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 6.6609e-04 - val_loss: 6.9701e-04\n",
            "Epoch 245/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 6.4837e-04 - val_loss: 6.6614e-04\n",
            "Epoch 246/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 6.2204e-04 - val_loss: 6.3562e-04\n",
            "Epoch 247/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 5.8905e-04 - val_loss: 6.1350e-04\n",
            "Epoch 248/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 5.5653e-04 - val_loss: 6.0433e-04\n",
            "Epoch 249/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 5.3419e-04 - val_loss: 6.0634e-04\n",
            "Epoch 250/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 5.2344e-04 - val_loss: 6.1168e-04\n",
            "Epoch 251/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 5.1889e-04 - val_loss: 6.1195e-04\n",
            "Epoch 252/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 5.1310e-04 - val_loss: 6.0114e-04\n",
            "Epoch 253/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 5.0061e-04 - val_loss: 5.7964e-04\n",
            "Epoch 254/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 4.8215e-04 - val_loss: 5.5583e-04\n",
            "Epoch 255/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 4.6265e-04 - val_loss: 5.3644e-04\n",
            "Epoch 256/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 4.4658e-04 - val_loss: 5.1860e-04\n",
            "Epoch 257/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 4.3654e-04 - val_loss: 5.0612e-04\n",
            "Epoch 258/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 4.2967e-04 - val_loss: 4.9669e-04\n",
            "Epoch 259/500\n",
            "63/63 [==============================] - 0s 70us/step - loss: 4.2287e-04 - val_loss: 4.8762e-04\n",
            "Epoch 260/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 4.1457e-04 - val_loss: 4.7910e-04\n",
            "Epoch 261/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 4.0101e-04 - val_loss: 4.7359e-04\n",
            "Epoch 262/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 3.8522e-04 - val_loss: 4.7292e-04\n",
            "Epoch 263/500\n",
            "63/63 [==============================] - 0s 73us/step - loss: 3.7302e-04 - val_loss: 4.7545e-04\n",
            "Epoch 264/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 3.6566e-04 - val_loss: 4.7642e-04\n",
            "Epoch 265/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 3.6060e-04 - val_loss: 4.7081e-04\n",
            "Epoch 266/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 3.5284e-04 - val_loss: 4.5845e-04\n",
            "Epoch 267/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 3.4396e-04 - val_loss: 4.4141e-04\n",
            "Epoch 268/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 3.3426e-04 - val_loss: 4.2309e-04\n",
            "Epoch 269/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 3.2556e-04 - val_loss: 4.0640e-04\n",
            "Epoch 270/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 3.1810e-04 - val_loss: 3.9262e-04\n",
            "Epoch 271/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 3.1129e-04 - val_loss: 3.8175e-04\n",
            "Epoch 272/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 3.0392e-04 - val_loss: 3.7272e-04\n",
            "Epoch 273/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 2.9835e-04 - val_loss: 3.6530e-04\n",
            "Epoch 274/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 2.9244e-04 - val_loss: 3.5906e-04\n",
            "Epoch 275/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 2.8635e-04 - val_loss: 3.5383e-04\n",
            "Epoch 276/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 2.8059e-04 - val_loss: 3.4879e-04\n",
            "Epoch 277/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 2.7545e-04 - val_loss: 3.4338e-04\n",
            "Epoch 278/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 2.7047e-04 - val_loss: 3.3720e-04\n",
            "Epoch 279/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 2.6553e-04 - val_loss: 3.3024e-04\n",
            "Epoch 280/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 2.6067e-04 - val_loss: 3.2285e-04\n",
            "Epoch 281/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 2.5603e-04 - val_loss: 3.1543e-04\n",
            "Epoch 282/500\n",
            "63/63 [==============================] - 0s 75us/step - loss: 2.5172e-04 - val_loss: 3.0839e-04\n",
            "Epoch 283/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 2.4768e-04 - val_loss: 3.0194e-04\n",
            "Epoch 284/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 2.4373e-04 - val_loss: 2.9616e-04\n",
            "Epoch 285/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 2.3974e-04 - val_loss: 2.9100e-04\n",
            "Epoch 286/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 2.3568e-04 - val_loss: 2.8634e-04\n",
            "Epoch 287/500\n",
            "63/63 [==============================] - 0s 69us/step - loss: 2.3166e-04 - val_loss: 2.8198e-04\n",
            "Epoch 288/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 2.2803e-04 - val_loss: 2.7764e-04\n",
            "Epoch 289/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 2.2463e-04 - val_loss: 2.7309e-04\n",
            "Epoch 290/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 2.2128e-04 - val_loss: 2.6827e-04\n",
            "Epoch 291/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 2.1785e-04 - val_loss: 2.6323e-04\n",
            "Epoch 292/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 2.1444e-04 - val_loss: 2.5813e-04\n",
            "Epoch 293/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 2.1114e-04 - val_loss: 2.5344e-04\n",
            "Epoch 294/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 2.0794e-04 - val_loss: 2.4912e-04\n",
            "Epoch 295/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 2.0481e-04 - val_loss: 2.4516e-04\n",
            "Epoch 296/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 2.0173e-04 - val_loss: 2.4155e-04\n",
            "Epoch 297/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.9866e-04 - val_loss: 2.3829e-04\n",
            "Epoch 298/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 1.9566e-04 - val_loss: 2.3514e-04\n",
            "Epoch 299/500\n",
            "63/63 [==============================] - 0s 71us/step - loss: 1.9275e-04 - val_loss: 2.3192e-04\n",
            "Epoch 300/500\n",
            "63/63 [==============================] - 0s 69us/step - loss: 1.9003e-04 - val_loss: 2.2856e-04\n",
            "Epoch 301/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 1.8737e-04 - val_loss: 2.2506e-04\n",
            "Epoch 302/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 1.8478e-04 - val_loss: 2.2150e-04\n",
            "Epoch 303/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 1.8225e-04 - val_loss: 2.1797e-04\n",
            "Epoch 304/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 1.7979e-04 - val_loss: 2.1456e-04\n",
            "Epoch 305/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 1.7739e-04 - val_loss: 2.1137e-04\n",
            "Epoch 306/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 1.7507e-04 - val_loss: 2.0839e-04\n",
            "Epoch 307/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 1.7280e-04 - val_loss: 2.0563e-04\n",
            "Epoch 308/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 1.7059e-04 - val_loss: 2.0304e-04\n",
            "Epoch 309/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.6842e-04 - val_loss: 2.0054e-04\n",
            "Epoch 310/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 1.6632e-04 - val_loss: 1.9807e-04\n",
            "Epoch 311/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 1.6426e-04 - val_loss: 1.9556e-04\n",
            "Epoch 312/500\n",
            "63/63 [==============================] - 0s 32us/step - loss: 1.6226e-04 - val_loss: 1.9298e-04\n",
            "Epoch 313/500\n",
            "63/63 [==============================] - 0s 31us/step - loss: 1.6030e-04 - val_loss: 1.9034e-04\n",
            "Epoch 314/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 1.5839e-04 - val_loss: 1.8747e-04\n",
            "Epoch 315/500\n",
            "63/63 [==============================] - 0s 39us/step - loss: 1.5652e-04 - val_loss: 1.8451e-04\n",
            "Epoch 316/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 1.5471e-04 - val_loss: 1.8164e-04\n",
            "Epoch 317/500\n",
            "63/63 [==============================] - 0s 47us/step - loss: 1.5292e-04 - val_loss: 1.7928e-04\n",
            "Epoch 318/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 1.5117e-04 - val_loss: 1.7718e-04\n",
            "Epoch 319/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 1.4943e-04 - val_loss: 1.7498e-04\n",
            "Epoch 320/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 1.4770e-04 - val_loss: 1.7271e-04\n",
            "Epoch 321/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 1.4601e-04 - val_loss: 1.7041e-04\n",
            "Epoch 322/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 1.4434e-04 - val_loss: 1.6812e-04\n",
            "Epoch 323/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 1.4272e-04 - val_loss: 1.6593e-04\n",
            "Epoch 324/500\n",
            "63/63 [==============================] - 0s 43us/step - loss: 1.4113e-04 - val_loss: 1.6388e-04\n",
            "Epoch 325/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 1.3957e-04 - val_loss: 1.6198e-04\n",
            "Epoch 326/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 1.3804e-04 - val_loss: 1.6021e-04\n",
            "Epoch 327/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 1.3654e-04 - val_loss: 1.5871e-04\n",
            "Epoch 328/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 1.3507e-04 - val_loss: 1.5717e-04\n",
            "Epoch 329/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 1.3363e-04 - val_loss: 1.5550e-04\n",
            "Epoch 330/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 1.3223e-04 - val_loss: 1.5385e-04\n",
            "Epoch 331/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 1.3086e-04 - val_loss: 1.5217e-04\n",
            "Epoch 332/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 1.2952e-04 - val_loss: 1.5047e-04\n",
            "Epoch 333/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 1.2820e-04 - val_loss: 1.4874e-04\n",
            "Epoch 334/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.2693e-04 - val_loss: 1.4697e-04\n",
            "Epoch 335/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 1.2568e-04 - val_loss: 1.4520e-04\n",
            "Epoch 336/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 1.2446e-04 - val_loss: 1.4348e-04\n",
            "Epoch 337/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 1.2327e-04 - val_loss: 1.4183e-04\n",
            "Epoch 338/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 1.2209e-04 - val_loss: 1.4027e-04\n",
            "Epoch 339/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 1.2094e-04 - val_loss: 1.3878e-04\n",
            "Epoch 340/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 1.1980e-04 - val_loss: 1.3733e-04\n",
            "Epoch 341/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 1.1869e-04 - val_loss: 1.3589e-04\n",
            "Epoch 342/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 1.1759e-04 - val_loss: 1.3442e-04\n",
            "Epoch 343/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 1.1652e-04 - val_loss: 1.3292e-04\n",
            "Epoch 344/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 1.1546e-04 - val_loss: 1.3139e-04\n",
            "Epoch 345/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 1.1442e-04 - val_loss: 1.2971e-04\n",
            "Epoch 346/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.1339e-04 - val_loss: 1.2797e-04\n",
            "Epoch 347/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.1239e-04 - val_loss: 1.2642e-04\n",
            "Epoch 348/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 1.1140e-04 - val_loss: 1.2493e-04\n",
            "Epoch 349/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 1.1043e-04 - val_loss: 1.2354e-04\n",
            "Epoch 350/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.0947e-04 - val_loss: 1.2224e-04\n",
            "Epoch 351/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 1.0853e-04 - val_loss: 1.2100e-04\n",
            "Epoch 352/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 1.0760e-04 - val_loss: 1.1979e-04\n",
            "Epoch 353/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.0669e-04 - val_loss: 1.1858e-04\n",
            "Epoch 354/500\n",
            "63/63 [==============================] - 0s 65us/step - loss: 1.0579e-04 - val_loss: 1.1733e-04\n",
            "Epoch 355/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 1.0490e-04 - val_loss: 1.1604e-04\n",
            "Epoch 356/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 1.0403e-04 - val_loss: 1.1473e-04\n",
            "Epoch 357/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 1.0317e-04 - val_loss: 1.1343e-04\n",
            "Epoch 358/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 1.0232e-04 - val_loss: 1.1216e-04\n",
            "Epoch 359/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 1.0149e-04 - val_loss: 1.1095e-04\n",
            "Epoch 360/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 1.0066e-04 - val_loss: 1.0980e-04\n",
            "Epoch 361/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 9.9850e-05 - val_loss: 1.0871e-04\n",
            "Epoch 362/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 9.9050e-05 - val_loss: 1.0765e-04\n",
            "Epoch 363/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 9.8260e-05 - val_loss: 1.0660e-04\n",
            "Epoch 364/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 9.7482e-05 - val_loss: 1.0554e-04\n",
            "Epoch 365/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 9.6714e-05 - val_loss: 1.0446e-04\n",
            "Epoch 366/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 9.5956e-05 - val_loss: 1.0338e-04\n",
            "Epoch 367/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 9.5208e-05 - val_loss: 1.0229e-04\n",
            "Epoch 368/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 9.4470e-05 - val_loss: 1.0123e-04\n",
            "Epoch 369/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 9.3742e-05 - val_loss: 1.0019e-04\n",
            "Epoch 370/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 9.3023e-05 - val_loss: 9.9192e-05\n",
            "Epoch 371/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 9.2314e-05 - val_loss: 9.8232e-05\n",
            "Epoch 372/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 9.1614e-05 - val_loss: 9.7301e-05\n",
            "Epoch 373/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 9.0922e-05 - val_loss: 9.6386e-05\n",
            "Epoch 374/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 9.0239e-05 - val_loss: 9.5473e-05\n",
            "Epoch 375/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 8.9565e-05 - val_loss: 9.4555e-05\n",
            "Epoch 376/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 8.8899e-05 - val_loss: 9.3631e-05\n",
            "Epoch 377/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 8.8242e-05 - val_loss: 9.2709e-05\n",
            "Epoch 378/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 8.7592e-05 - val_loss: 9.1796e-05\n",
            "Epoch 379/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 8.6951e-05 - val_loss: 9.0901e-05\n",
            "Epoch 380/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 8.6317e-05 - val_loss: 9.0032e-05\n",
            "Epoch 381/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 8.5691e-05 - val_loss: 8.9188e-05\n",
            "Epoch 382/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 8.5072e-05 - val_loss: 8.8364e-05\n",
            "Epoch 383/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 8.4461e-05 - val_loss: 8.7555e-05\n",
            "Epoch 384/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 8.3853e-05 - val_loss: 8.6455e-05\n",
            "Epoch 385/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 8.3228e-05 - val_loss: 8.5210e-05\n",
            "Epoch 386/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 8.2610e-05 - val_loss: 8.4006e-05\n",
            "Epoch 387/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 8.2009e-05 - val_loss: 8.2985e-05\n",
            "Epoch 388/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 8.1418e-05 - val_loss: 8.2210e-05\n",
            "Epoch 389/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 8.0823e-05 - val_loss: 8.1662e-05\n",
            "Epoch 390/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 8.0223e-05 - val_loss: 8.1256e-05\n",
            "Epoch 391/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 7.9630e-05 - val_loss: 8.0863e-05\n",
            "Epoch 392/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 7.9050e-05 - val_loss: 8.0360e-05\n",
            "Epoch 393/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 7.8480e-05 - val_loss: 7.9676e-05\n",
            "Epoch 394/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 7.7911e-05 - val_loss: 7.8822e-05\n",
            "Epoch 395/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 7.7343e-05 - val_loss: 7.7878e-05\n",
            "Epoch 396/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 7.6781e-05 - val_loss: 7.6951e-05\n",
            "Epoch 397/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 7.6240e-05 - val_loss: 7.6202e-05\n",
            "Epoch 398/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 7.5704e-05 - val_loss: 7.5639e-05\n",
            "Epoch 399/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 7.5167e-05 - val_loss: 7.5206e-05\n",
            "Epoch 400/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 7.4636e-05 - val_loss: 7.4806e-05\n",
            "Epoch 401/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 7.4116e-05 - val_loss: 7.4338e-05\n",
            "Epoch 402/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 7.3606e-05 - val_loss: 7.3734e-05\n",
            "Epoch 403/500\n",
            "63/63 [==============================] - 0s 48us/step - loss: 7.3100e-05 - val_loss: 7.2989e-05\n",
            "Epoch 404/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 7.2596e-05 - val_loss: 7.2158e-05\n",
            "Epoch 405/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 7.2098e-05 - val_loss: 7.1323e-05\n",
            "Epoch 406/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 7.1608e-05 - val_loss: 7.0559e-05\n",
            "Epoch 407/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 7.1126e-05 - val_loss: 6.9907e-05\n",
            "Epoch 408/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 7.0648e-05 - val_loss: 6.9362e-05\n",
            "Epoch 409/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 7.0173e-05 - val_loss: 6.8887e-05\n",
            "Epoch 410/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 6.9703e-05 - val_loss: 6.8422e-05\n",
            "Epoch 411/500\n",
            "63/63 [==============================] - 0s 68us/step - loss: 6.9241e-05 - val_loss: 6.7908e-05\n",
            "Epoch 412/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 6.8785e-05 - val_loss: 6.7314e-05\n",
            "Epoch 413/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.8333e-05 - val_loss: 6.6647e-05\n",
            "Epoch 414/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.7885e-05 - val_loss: 6.5942e-05\n",
            "Epoch 415/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 6.7443e-05 - val_loss: 6.5250e-05\n",
            "Epoch 416/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.7006e-05 - val_loss: 6.4611e-05\n",
            "Epoch 417/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.6575e-05 - val_loss: 6.4042e-05\n",
            "Epoch 418/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.6147e-05 - val_loss: 6.3536e-05\n",
            "Epoch 419/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 6.5724e-05 - val_loss: 6.3066e-05\n",
            "Epoch 420/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.5306e-05 - val_loss: 6.2593e-05\n",
            "Epoch 421/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.4893e-05 - val_loss: 6.2089e-05\n",
            "Epoch 422/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 6.4485e-05 - val_loss: 6.1540e-05\n",
            "Epoch 423/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 6.4080e-05 - val_loss: 6.0957e-05\n",
            "Epoch 424/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 6.3680e-05 - val_loss: 6.0364e-05\n",
            "Epoch 425/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.3285e-05 - val_loss: 5.9791e-05\n",
            "Epoch 426/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 6.2894e-05 - val_loss: 5.9256e-05\n",
            "Epoch 427/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 6.2507e-05 - val_loss: 5.8767e-05\n",
            "Epoch 428/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 6.2125e-05 - val_loss: 5.8313e-05\n",
            "Epoch 429/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 6.1746e-05 - val_loss: 5.7874e-05\n",
            "Epoch 430/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 6.1372e-05 - val_loss: 5.7430e-05\n",
            "Epoch 431/500\n",
            "63/63 [==============================] - 0s 55us/step - loss: 6.1001e-05 - val_loss: 5.6965e-05\n",
            "Epoch 432/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 6.0635e-05 - val_loss: 5.6478e-05\n",
            "Epoch 433/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 6.0273e-05 - val_loss: 5.5978e-05\n",
            "Epoch 434/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 5.9914e-05 - val_loss: 5.5482e-05\n",
            "Epoch 435/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 5.9559e-05 - val_loss: 5.5005e-05\n",
            "Epoch 436/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 5.9208e-05 - val_loss: 5.4556e-05\n",
            "Epoch 437/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 5.8861e-05 - val_loss: 5.4135e-05\n",
            "Epoch 438/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 5.8517e-05 - val_loss: 5.3731e-05\n",
            "Epoch 439/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 5.8177e-05 - val_loss: 5.3332e-05\n",
            "Epoch 440/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 5.7841e-05 - val_loss: 5.2927e-05\n",
            "Epoch 441/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 5.7508e-05 - val_loss: 5.2511e-05\n",
            "Epoch 442/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 5.7179e-05 - val_loss: 5.2085e-05\n",
            "Epoch 443/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 5.6853e-05 - val_loss: 5.1658e-05\n",
            "Epoch 444/500\n",
            "63/63 [==============================] - 0s 43us/step - loss: 5.6521e-05 - val_loss: 5.1199e-05\n",
            "Epoch 445/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 5.6186e-05 - val_loss: 5.0734e-05\n",
            "Epoch 446/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 5.5853e-05 - val_loss: 5.0285e-05\n",
            "Epoch 447/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 5.5521e-05 - val_loss: 4.9865e-05\n",
            "Epoch 448/500\n",
            "63/63 [==============================] - 0s 47us/step - loss: 5.5191e-05 - val_loss: 4.9473e-05\n",
            "Epoch 449/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 5.4862e-05 - val_loss: 4.9014e-05\n",
            "Epoch 450/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 5.4534e-05 - val_loss: 4.8514e-05\n",
            "Epoch 451/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 5.4207e-05 - val_loss: 4.7990e-05\n",
            "Epoch 452/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 5.3870e-05 - val_loss: 4.7490e-05\n",
            "Epoch 453/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 5.3535e-05 - val_loss: 4.7047e-05\n",
            "Epoch 454/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 5.3201e-05 - val_loss: 4.6663e-05\n",
            "Epoch 455/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 5.2867e-05 - val_loss: 4.6321e-05\n",
            "Epoch 456/500\n",
            "63/63 [==============================] - 0s 53us/step - loss: 5.2535e-05 - val_loss: 4.5985e-05\n",
            "Epoch 457/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 5.2206e-05 - val_loss: 4.5621e-05\n",
            "Epoch 458/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 5.1880e-05 - val_loss: 4.5215e-05\n",
            "Epoch 459/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 5.1557e-05 - val_loss: 4.4770e-05\n",
            "Epoch 460/500\n",
            "63/63 [==============================] - 0s 45us/step - loss: 5.1236e-05 - val_loss: 4.4312e-05\n",
            "Epoch 461/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 5.0918e-05 - val_loss: 4.3871e-05\n",
            "Epoch 462/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 5.0604e-05 - val_loss: 4.3467e-05\n",
            "Epoch 463/500\n",
            "63/63 [==============================] - 0s 46us/step - loss: 5.0293e-05 - val_loss: 4.3108e-05\n",
            "Epoch 464/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 4.9985e-05 - val_loss: 4.2784e-05\n",
            "Epoch 465/500\n",
            "63/63 [==============================] - 0s 52us/step - loss: 4.9679e-05 - val_loss: 4.2476e-05\n",
            "Epoch 466/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 4.9378e-05 - val_loss: 4.2161e-05\n",
            "Epoch 467/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 4.9079e-05 - val_loss: 4.1829e-05\n",
            "Epoch 468/500\n",
            "63/63 [==============================] - 0s 96us/step - loss: 4.8784e-05 - val_loss: 4.1477e-05\n",
            "Epoch 469/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 4.8491e-05 - val_loss: 4.1118e-05\n",
            "Epoch 470/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 4.8202e-05 - val_loss: 4.0770e-05\n",
            "Epoch 471/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 4.7916e-05 - val_loss: 4.0446e-05\n",
            "Epoch 472/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 4.7632e-05 - val_loss: 4.0151e-05\n",
            "Epoch 473/500\n",
            "63/63 [==============================] - 0s 54us/step - loss: 4.7352e-05 - val_loss: 3.9881e-05\n",
            "Epoch 474/500\n",
            "63/63 [==============================] - 0s 49us/step - loss: 4.7074e-05 - val_loss: 3.9625e-05\n",
            "Epoch 475/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 4.6799e-05 - val_loss: 3.9370e-05\n",
            "Epoch 476/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 4.6527e-05 - val_loss: 3.9106e-05\n",
            "Epoch 477/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 4.6258e-05 - val_loss: 3.8832e-05\n",
            "Epoch 478/500\n",
            "63/63 [==============================] - 0s 60us/step - loss: 4.5985e-05 - val_loss: 3.8542e-05\n",
            "Epoch 479/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 4.5714e-05 - val_loss: 3.8255e-05\n",
            "Epoch 480/500\n",
            "63/63 [==============================] - 0s 50us/step - loss: 4.5445e-05 - val_loss: 3.7984e-05\n",
            "Epoch 481/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 4.5178e-05 - val_loss: 3.7739e-05\n",
            "Epoch 482/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 4.4912e-05 - val_loss: 3.7491e-05\n",
            "Epoch 483/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 4.4632e-05 - val_loss: 3.7243e-05\n",
            "Epoch 484/500\n",
            "63/63 [==============================] - 0s 56us/step - loss: 4.4352e-05 - val_loss: 3.6994e-05\n",
            "Epoch 485/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 4.4073e-05 - val_loss: 3.6745e-05\n",
            "Epoch 486/500\n",
            "63/63 [==============================] - 0s 51us/step - loss: 4.3794e-05 - val_loss: 3.6496e-05\n",
            "Epoch 487/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 4.3517e-05 - val_loss: 3.6246e-05\n",
            "Epoch 488/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 4.3240e-05 - val_loss: 3.5998e-05\n",
            "Epoch 489/500\n",
            "63/63 [==============================] - 0s 70us/step - loss: 4.2966e-05 - val_loss: 3.5750e-05\n",
            "Epoch 490/500\n",
            "63/63 [==============================] - 0s 67us/step - loss: 4.2693e-05 - val_loss: 3.5505e-05\n",
            "Epoch 491/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 4.2422e-05 - val_loss: 3.5264e-05\n",
            "Epoch 492/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 4.2153e-05 - val_loss: 3.5027e-05\n",
            "Epoch 493/500\n",
            "63/63 [==============================] - 0s 61us/step - loss: 4.1887e-05 - val_loss: 3.4795e-05\n",
            "Epoch 494/500\n",
            "63/63 [==============================] - 0s 63us/step - loss: 4.1624e-05 - val_loss: 3.4516e-05\n",
            "Epoch 495/500\n",
            "63/63 [==============================] - 0s 64us/step - loss: 4.1365e-05 - val_loss: 3.4222e-05\n",
            "Epoch 496/500\n",
            "63/63 [==============================] - 0s 62us/step - loss: 4.1110e-05 - val_loss: 3.3947e-05\n",
            "Epoch 497/500\n",
            "63/63 [==============================] - 0s 57us/step - loss: 4.0854e-05 - val_loss: 3.3650e-05\n",
            "Epoch 498/500\n",
            "63/63 [==============================] - 0s 66us/step - loss: 4.0579e-05 - val_loss: 3.3380e-05\n",
            "Epoch 499/500\n",
            "63/63 [==============================] - 0s 59us/step - loss: 4.0304e-05 - val_loss: 3.3078e-05\n",
            "Epoch 500/500\n",
            "63/63 [==============================] - 0s 58us/step - loss: 4.0016e-05 - val_loss: 3.2784e-05\n",
            "dict_keys(['val_loss', 'loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw8zMauLgMDOzmjg4zMysJg4OMzOryf8HRfTYpQwZ/OYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lJ-M940-G24"
      },
      "source": [
        "Plot the predicted data(in red colour) with real output(in green colour)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A4lcOIqJBOp",
        "colab_type": "code",
        "outputId": "d72c4847-8869-481a-d252-1af5ed581937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "result=model.predict(X_test)\n",
        "plt.scatter(range(16),result,c='r')\n",
        "plt.scatter(range(16),y_test,c='g')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n56A3M8vc/wPpWTIe3a0TswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}