{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_basics(LSTM).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiOG8-DPNvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cddb26b5-287c-45e7-c719-ab5165d49402"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten\n",
        "#from keras.layers import SimpleRNN,LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DiBgBpuPt9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = [[[(i*j +j)/500] for i in range (5)] for j in range(100)]\n",
        "#Y = [(i*6)/500 for i in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8q1bztQTGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [[[(5*i*i*i +j)] for i in range (5)] for j in range(100)]\n",
        "Y = [(i*6) for i in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guraDXW9QgFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c8e3c8b-f86a-4639-a764-f005b503dc6c"
      },
      "source": [
        "   for i in range(100):\n",
        "      print(X[i], Y[i])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0], [5], [40], [135], [320]] 0\n",
            "[[1], [6], [41], [136], [321]] 6\n",
            "[[2], [7], [42], [137], [322]] 12\n",
            "[[3], [8], [43], [138], [323]] 18\n",
            "[[4], [9], [44], [139], [324]] 24\n",
            "[[5], [10], [45], [140], [325]] 30\n",
            "[[6], [11], [46], [141], [326]] 36\n",
            "[[7], [12], [47], [142], [327]] 42\n",
            "[[8], [13], [48], [143], [328]] 48\n",
            "[[9], [14], [49], [144], [329]] 54\n",
            "[[10], [15], [50], [145], [330]] 60\n",
            "[[11], [16], [51], [146], [331]] 66\n",
            "[[12], [17], [52], [147], [332]] 72\n",
            "[[13], [18], [53], [148], [333]] 78\n",
            "[[14], [19], [54], [149], [334]] 84\n",
            "[[15], [20], [55], [150], [335]] 90\n",
            "[[16], [21], [56], [151], [336]] 96\n",
            "[[17], [22], [57], [152], [337]] 102\n",
            "[[18], [23], [58], [153], [338]] 108\n",
            "[[19], [24], [59], [154], [339]] 114\n",
            "[[20], [25], [60], [155], [340]] 120\n",
            "[[21], [26], [61], [156], [341]] 126\n",
            "[[22], [27], [62], [157], [342]] 132\n",
            "[[23], [28], [63], [158], [343]] 138\n",
            "[[24], [29], [64], [159], [344]] 144\n",
            "[[25], [30], [65], [160], [345]] 150\n",
            "[[26], [31], [66], [161], [346]] 156\n",
            "[[27], [32], [67], [162], [347]] 162\n",
            "[[28], [33], [68], [163], [348]] 168\n",
            "[[29], [34], [69], [164], [349]] 174\n",
            "[[30], [35], [70], [165], [350]] 180\n",
            "[[31], [36], [71], [166], [351]] 186\n",
            "[[32], [37], [72], [167], [352]] 192\n",
            "[[33], [38], [73], [168], [353]] 198\n",
            "[[34], [39], [74], [169], [354]] 204\n",
            "[[35], [40], [75], [170], [355]] 210\n",
            "[[36], [41], [76], [171], [356]] 216\n",
            "[[37], [42], [77], [172], [357]] 222\n",
            "[[38], [43], [78], [173], [358]] 228\n",
            "[[39], [44], [79], [174], [359]] 234\n",
            "[[40], [45], [80], [175], [360]] 240\n",
            "[[41], [46], [81], [176], [361]] 246\n",
            "[[42], [47], [82], [177], [362]] 252\n",
            "[[43], [48], [83], [178], [363]] 258\n",
            "[[44], [49], [84], [179], [364]] 264\n",
            "[[45], [50], [85], [180], [365]] 270\n",
            "[[46], [51], [86], [181], [366]] 276\n",
            "[[47], [52], [87], [182], [367]] 282\n",
            "[[48], [53], [88], [183], [368]] 288\n",
            "[[49], [54], [89], [184], [369]] 294\n",
            "[[50], [55], [90], [185], [370]] 300\n",
            "[[51], [56], [91], [186], [371]] 306\n",
            "[[52], [57], [92], [187], [372]] 312\n",
            "[[53], [58], [93], [188], [373]] 318\n",
            "[[54], [59], [94], [189], [374]] 324\n",
            "[[55], [60], [95], [190], [375]] 330\n",
            "[[56], [61], [96], [191], [376]] 336\n",
            "[[57], [62], [97], [192], [377]] 342\n",
            "[[58], [63], [98], [193], [378]] 348\n",
            "[[59], [64], [99], [194], [379]] 354\n",
            "[[60], [65], [100], [195], [380]] 360\n",
            "[[61], [66], [101], [196], [381]] 366\n",
            "[[62], [67], [102], [197], [382]] 372\n",
            "[[63], [68], [103], [198], [383]] 378\n",
            "[[64], [69], [104], [199], [384]] 384\n",
            "[[65], [70], [105], [200], [385]] 390\n",
            "[[66], [71], [106], [201], [386]] 396\n",
            "[[67], [72], [107], [202], [387]] 402\n",
            "[[68], [73], [108], [203], [388]] 408\n",
            "[[69], [74], [109], [204], [389]] 414\n",
            "[[70], [75], [110], [205], [390]] 420\n",
            "[[71], [76], [111], [206], [391]] 426\n",
            "[[72], [77], [112], [207], [392]] 432\n",
            "[[73], [78], [113], [208], [393]] 438\n",
            "[[74], [79], [114], [209], [394]] 444\n",
            "[[75], [80], [115], [210], [395]] 450\n",
            "[[76], [81], [116], [211], [396]] 456\n",
            "[[77], [82], [117], [212], [397]] 462\n",
            "[[78], [83], [118], [213], [398]] 468\n",
            "[[79], [84], [119], [214], [399]] 474\n",
            "[[80], [85], [120], [215], [400]] 480\n",
            "[[81], [86], [121], [216], [401]] 486\n",
            "[[82], [87], [122], [217], [402]] 492\n",
            "[[83], [88], [123], [218], [403]] 498\n",
            "[[84], [89], [124], [219], [404]] 504\n",
            "[[85], [90], [125], [220], [405]] 510\n",
            "[[86], [91], [126], [221], [406]] 516\n",
            "[[87], [92], [127], [222], [407]] 522\n",
            "[[88], [93], [128], [223], [408]] 528\n",
            "[[89], [94], [129], [224], [409]] 534\n",
            "[[90], [95], [130], [225], [410]] 540\n",
            "[[91], [96], [131], [226], [411]] 546\n",
            "[[92], [97], [132], [227], [412]] 552\n",
            "[[93], [98], [133], [228], [413]] 558\n",
            "[[94], [99], [134], [229], [414]] 564\n",
            "[[95], [100], [135], [230], [415]] 570\n",
            "[[96], [101], [136], [231], [416]] 576\n",
            "[[97], [102], [137], [232], [417]] 582\n",
            "[[98], [103], [138], [233], [418]] 588\n",
            "[[99], [104], [139], [234], [419]] 594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WdHnLTEQoGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(X, dtype=\"float32\")\n",
        "Y = np.array(Y, dtype=\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "26gL1K3DybuX",
        "colab": {}
      },
      "source": [
        "X /=500\n",
        "Y /= 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7o1FJWxRZch",
        "colab_type": "code",
        "outputId": "b73d7681-6f80-4dc5-8e67-e6f72f336443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbZlphpeRe3T",
        "colab_type": "code",
        "outputId": "ef9926fe-5ac5-419d-b9ea-666c8953e4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZEJ1iOURkIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, \n",
        "                                                random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Ij5hoFSNX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76626e81-3fc2-413a-ad12-8f716434937b"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.188],\n",
              "        [0.198],\n",
              "        [0.268],\n",
              "        [0.458],\n",
              "        [0.828]],\n",
              "\n",
              "       [[0.112],\n",
              "        [0.122],\n",
              "        [0.192],\n",
              "        [0.382],\n",
              "        [0.752]],\n",
              "\n",
              "       [[0.044],\n",
              "        [0.054],\n",
              "        [0.124],\n",
              "        [0.314],\n",
              "        [0.684]],\n",
              "\n",
              "       [[0.078],\n",
              "        [0.088],\n",
              "        [0.158],\n",
              "        [0.348],\n",
              "        [0.718]],\n",
              "\n",
              "       [[0.048],\n",
              "        [0.058],\n",
              "        [0.128],\n",
              "        [0.318],\n",
              "        [0.688]],\n",
              "\n",
              "       [[0.026],\n",
              "        [0.036],\n",
              "        [0.106],\n",
              "        [0.296],\n",
              "        [0.666]],\n",
              "\n",
              "       [[0.126],\n",
              "        [0.136],\n",
              "        [0.206],\n",
              "        [0.396],\n",
              "        [0.766]],\n",
              "\n",
              "       [[0.142],\n",
              "        [0.152],\n",
              "        [0.222],\n",
              "        [0.412],\n",
              "        [0.782]],\n",
              "\n",
              "       [[0.11 ],\n",
              "        [0.12 ],\n",
              "        [0.19 ],\n",
              "        [0.38 ],\n",
              "        [0.75 ]],\n",
              "\n",
              "       [[0.174],\n",
              "        [0.184],\n",
              "        [0.254],\n",
              "        [0.444],\n",
              "        [0.814]],\n",
              "\n",
              "       [[0.012],\n",
              "        [0.022],\n",
              "        [0.092],\n",
              "        [0.282],\n",
              "        [0.652]],\n",
              "\n",
              "       [[0.176],\n",
              "        [0.186],\n",
              "        [0.256],\n",
              "        [0.446],\n",
              "        [0.816]],\n",
              "\n",
              "       [[0.128],\n",
              "        [0.138],\n",
              "        [0.208],\n",
              "        [0.398],\n",
              "        [0.768]],\n",
              "\n",
              "       [[0.052],\n",
              "        [0.062],\n",
              "        [0.132],\n",
              "        [0.322],\n",
              "        [0.692]],\n",
              "\n",
              "       [[0.096],\n",
              "        [0.106],\n",
              "        [0.176],\n",
              "        [0.366],\n",
              "        [0.736]],\n",
              "\n",
              "       [[0.1  ],\n",
              "        [0.11 ],\n",
              "        [0.18 ],\n",
              "        [0.37 ],\n",
              "        [0.74 ]],\n",
              "\n",
              "       [[0.144],\n",
              "        [0.154],\n",
              "        [0.224],\n",
              "        [0.414],\n",
              "        [0.784]],\n",
              "\n",
              "       [[0.108],\n",
              "        [0.118],\n",
              "        [0.188],\n",
              "        [0.378],\n",
              "        [0.748]],\n",
              "\n",
              "       [[0.042],\n",
              "        [0.052],\n",
              "        [0.122],\n",
              "        [0.312],\n",
              "        [0.682]],\n",
              "\n",
              "       [[0.05 ],\n",
              "        [0.06 ],\n",
              "        [0.13 ],\n",
              "        [0.32 ],\n",
              "        [0.69 ]],\n",
              "\n",
              "       [[0.066],\n",
              "        [0.076],\n",
              "        [0.146],\n",
              "        [0.336],\n",
              "        [0.706]],\n",
              "\n",
              "       [[0.058],\n",
              "        [0.068],\n",
              "        [0.138],\n",
              "        [0.328],\n",
              "        [0.698]],\n",
              "\n",
              "       [[0.104],\n",
              "        [0.114],\n",
              "        [0.184],\n",
              "        [0.374],\n",
              "        [0.744]],\n",
              "\n",
              "       [[0.028],\n",
              "        [0.038],\n",
              "        [0.108],\n",
              "        [0.298],\n",
              "        [0.668]],\n",
              "\n",
              "       [[0.17 ],\n",
              "        [0.18 ],\n",
              "        [0.25 ],\n",
              "        [0.44 ],\n",
              "        [0.81 ]],\n",
              "\n",
              "       [[0.158],\n",
              "        [0.168],\n",
              "        [0.238],\n",
              "        [0.428],\n",
              "        [0.798]],\n",
              "\n",
              "       [[0.19 ],\n",
              "        [0.2  ],\n",
              "        [0.27 ],\n",
              "        [0.46 ],\n",
              "        [0.83 ]],\n",
              "\n",
              "       [[0.006],\n",
              "        [0.016],\n",
              "        [0.086],\n",
              "        [0.276],\n",
              "        [0.646]],\n",
              "\n",
              "       [[0.086],\n",
              "        [0.096],\n",
              "        [0.166],\n",
              "        [0.356],\n",
              "        [0.726]],\n",
              "\n",
              "       [[0.022],\n",
              "        [0.032],\n",
              "        [0.102],\n",
              "        [0.292],\n",
              "        [0.662]],\n",
              "\n",
              "       [[0.196],\n",
              "        [0.206],\n",
              "        [0.276],\n",
              "        [0.466],\n",
              "        [0.836]],\n",
              "\n",
              "       [[0.09 ],\n",
              "        [0.1  ],\n",
              "        [0.17 ],\n",
              "        [0.36 ],\n",
              "        [0.73 ]],\n",
              "\n",
              "       [[0.166],\n",
              "        [0.176],\n",
              "        [0.246],\n",
              "        [0.436],\n",
              "        [0.806]],\n",
              "\n",
              "       [[0.134],\n",
              "        [0.144],\n",
              "        [0.214],\n",
              "        [0.404],\n",
              "        [0.774]],\n",
              "\n",
              "       [[0.178],\n",
              "        [0.188],\n",
              "        [0.258],\n",
              "        [0.448],\n",
              "        [0.818]],\n",
              "\n",
              "       [[0.172],\n",
              "        [0.182],\n",
              "        [0.252],\n",
              "        [0.442],\n",
              "        [0.812]],\n",
              "\n",
              "       [[0.098],\n",
              "        [0.108],\n",
              "        [0.178],\n",
              "        [0.368],\n",
              "        [0.738]],\n",
              "\n",
              "       [[0.164],\n",
              "        [0.174],\n",
              "        [0.244],\n",
              "        [0.434],\n",
              "        [0.804]],\n",
              "\n",
              "       [[0.118],\n",
              "        [0.128],\n",
              "        [0.198],\n",
              "        [0.388],\n",
              "        [0.758]],\n",
              "\n",
              "       [[0.038],\n",
              "        [0.048],\n",
              "        [0.118],\n",
              "        [0.308],\n",
              "        [0.678]],\n",
              "\n",
              "       [[0.162],\n",
              "        [0.172],\n",
              "        [0.242],\n",
              "        [0.432],\n",
              "        [0.802]],\n",
              "\n",
              "       [[0.076],\n",
              "        [0.086],\n",
              "        [0.156],\n",
              "        [0.346],\n",
              "        [0.716]],\n",
              "\n",
              "       [[0.136],\n",
              "        [0.146],\n",
              "        [0.216],\n",
              "        [0.406],\n",
              "        [0.776]],\n",
              "\n",
              "       [[0.004],\n",
              "        [0.014],\n",
              "        [0.084],\n",
              "        [0.274],\n",
              "        [0.644]],\n",
              "\n",
              "       [[0.062],\n",
              "        [0.072],\n",
              "        [0.142],\n",
              "        [0.332],\n",
              "        [0.702]],\n",
              "\n",
              "       [[0.168],\n",
              "        [0.178],\n",
              "        [0.248],\n",
              "        [0.438],\n",
              "        [0.808]],\n",
              "\n",
              "       [[0.102],\n",
              "        [0.112],\n",
              "        [0.182],\n",
              "        [0.372],\n",
              "        [0.742]],\n",
              "\n",
              "       [[0.072],\n",
              "        [0.082],\n",
              "        [0.152],\n",
              "        [0.342],\n",
              "        [0.712]],\n",
              "\n",
              "       [[0.008],\n",
              "        [0.018],\n",
              "        [0.088],\n",
              "        [0.278],\n",
              "        [0.648]],\n",
              "\n",
              "       [[0.   ],\n",
              "        [0.01 ],\n",
              "        [0.08 ],\n",
              "        [0.27 ],\n",
              "        [0.64 ]],\n",
              "\n",
              "       [[0.116],\n",
              "        [0.126],\n",
              "        [0.196],\n",
              "        [0.386],\n",
              "        [0.756]],\n",
              "\n",
              "       [[0.01 ],\n",
              "        [0.02 ],\n",
              "        [0.09 ],\n",
              "        [0.28 ],\n",
              "        [0.65 ]],\n",
              "\n",
              "       [[0.192],\n",
              "        [0.202],\n",
              "        [0.272],\n",
              "        [0.462],\n",
              "        [0.832]],\n",
              "\n",
              "       [[0.002],\n",
              "        [0.012],\n",
              "        [0.082],\n",
              "        [0.272],\n",
              "        [0.642]],\n",
              "\n",
              "       [[0.186],\n",
              "        [0.196],\n",
              "        [0.266],\n",
              "        [0.456],\n",
              "        [0.826]],\n",
              "\n",
              "       [[0.082],\n",
              "        [0.092],\n",
              "        [0.162],\n",
              "        [0.352],\n",
              "        [0.722]],\n",
              "\n",
              "       [[0.018],\n",
              "        [0.028],\n",
              "        [0.098],\n",
              "        [0.288],\n",
              "        [0.658]],\n",
              "\n",
              "       [[0.036],\n",
              "        [0.046],\n",
              "        [0.116],\n",
              "        [0.306],\n",
              "        [0.676]],\n",
              "\n",
              "       [[0.182],\n",
              "        [0.192],\n",
              "        [0.262],\n",
              "        [0.452],\n",
              "        [0.822]],\n",
              "\n",
              "       [[0.094],\n",
              "        [0.104],\n",
              "        [0.174],\n",
              "        [0.364],\n",
              "        [0.734]],\n",
              "\n",
              "       [[0.13 ],\n",
              "        [0.14 ],\n",
              "        [0.21 ],\n",
              "        [0.4  ],\n",
              "        [0.77 ]],\n",
              "\n",
              "       [[0.15 ],\n",
              "        [0.16 ],\n",
              "        [0.23 ],\n",
              "        [0.42 ],\n",
              "        [0.79 ]],\n",
              "\n",
              "       [[0.154],\n",
              "        [0.164],\n",
              "        [0.234],\n",
              "        [0.424],\n",
              "        [0.794]],\n",
              "\n",
              "       [[0.088],\n",
              "        [0.098],\n",
              "        [0.168],\n",
              "        [0.358],\n",
              "        [0.728]],\n",
              "\n",
              "       [[0.184],\n",
              "        [0.194],\n",
              "        [0.264],\n",
              "        [0.454],\n",
              "        [0.824]],\n",
              "\n",
              "       [[0.18 ],\n",
              "        [0.19 ],\n",
              "        [0.26 ],\n",
              "        [0.45 ],\n",
              "        [0.82 ]],\n",
              "\n",
              "       [[0.106],\n",
              "        [0.116],\n",
              "        [0.186],\n",
              "        [0.376],\n",
              "        [0.746]],\n",
              "\n",
              "       [[0.03 ],\n",
              "        [0.04 ],\n",
              "        [0.11 ],\n",
              "        [0.3  ],\n",
              "        [0.67 ]],\n",
              "\n",
              "       [[0.152],\n",
              "        [0.162],\n",
              "        [0.232],\n",
              "        [0.422],\n",
              "        [0.792]],\n",
              "\n",
              "       [[0.014],\n",
              "        [0.024],\n",
              "        [0.094],\n",
              "        [0.284],\n",
              "        [0.654]],\n",
              "\n",
              "       [[0.16 ],\n",
              "        [0.17 ],\n",
              "        [0.24 ],\n",
              "        [0.43 ],\n",
              "        [0.8  ]],\n",
              "\n",
              "       [[0.06 ],\n",
              "        [0.07 ],\n",
              "        [0.14 ],\n",
              "        [0.33 ],\n",
              "        [0.7  ]],\n",
              "\n",
              "       [[0.054],\n",
              "        [0.064],\n",
              "        [0.134],\n",
              "        [0.324],\n",
              "        [0.694]],\n",
              "\n",
              "       [[0.124],\n",
              "        [0.134],\n",
              "        [0.204],\n",
              "        [0.394],\n",
              "        [0.764]],\n",
              "\n",
              "       [[0.016],\n",
              "        [0.026],\n",
              "        [0.096],\n",
              "        [0.286],\n",
              "        [0.656]],\n",
              "\n",
              "       [[0.146],\n",
              "        [0.156],\n",
              "        [0.226],\n",
              "        [0.416],\n",
              "        [0.786]],\n",
              "\n",
              "       [[0.032],\n",
              "        [0.042],\n",
              "        [0.112],\n",
              "        [0.302],\n",
              "        [0.672]],\n",
              "\n",
              "       [[0.122],\n",
              "        [0.132],\n",
              "        [0.202],\n",
              "        [0.392],\n",
              "        [0.762]],\n",
              "\n",
              "       [[0.156],\n",
              "        [0.166],\n",
              "        [0.236],\n",
              "        [0.426],\n",
              "        [0.796]],\n",
              "\n",
              "       [[0.198],\n",
              "        [0.208],\n",
              "        [0.278],\n",
              "        [0.468],\n",
              "        [0.838]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTJxxE3Sbpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "a58a042c-60cd-490e-9004-c0758927eca8"
      },
      "source": [
        "from keras.layers import SimpleRNN, LSTM\n",
        "#from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(LSTM((2), input_shape=(5,1), return_sequences=True))\n",
        "model.add(LSTM((3), input_shape=(5,1), return_sequences=True))\n",
        "#model.add(SimpleRNN(1,input_shape=(5,1), return_sequences=False))\n",
        "#model.add(SimpleRNN(2,input_shape=(5,1), return_sequences=True))\n",
        "#model.add(SimpleRNN(2,input_shape=(5,1), return_sequences=True))\n",
        "#model.add(SimpleRNN(1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['acc'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGCyrnzNTFom",
        "colab_type": "code",
        "outputId": "0c259a00-662f-4f6f-f1d7-d52a2dadcfd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 5, 2)              32        \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 5, 3)              72        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 120\n",
            "Trainable params: 120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjDtDx1qTlHh",
        "colab_type": "code",
        "outputId": "03742345-386c-469a-b9ca-7beab0b43f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "80/80 [==============================] - 2s 25ms/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 0s 250us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 0s 425us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 0s 426us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 0s 256us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 0s 360us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 0s 374us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 0s 368us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 0s 363us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 0s 419us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 0s 341us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 0s 375us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 0s 394us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 0s 407us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 0s 392us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 0s 427us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 0s 365us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 0s 351us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 0s 432us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 0s 357us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 0s 378us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 0s 248us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 0s 346us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 0s 251us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 0s 363us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 0s 361us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 0s 341us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 0s 382us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 221/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 0s 345us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 0s 363us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 0s 383us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 0s 432us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 0s 390us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 0s 244us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 0s 414us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 0s 360us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 0s 395us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 0s 260us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 0s 357us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 0s 361us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 0s 397us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 0s 361us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 0s 254us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 0s 251us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 0s 372us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 0s 375us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 380/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 381/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 382/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 383/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 384/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 385/1000\n",
            "80/80 [==============================] - 0s 341us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 386/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 387/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 388/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 389/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 390/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 391/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 392/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 393/1000\n",
            "80/80 [==============================] - 0s 365us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 394/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 395/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 396/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 397/1000\n",
            "80/80 [==============================] - 0s 507us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 398/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 399/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 400/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 401/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 402/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 403/1000\n",
            "80/80 [==============================] - 0s 250us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 404/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 405/1000\n",
            "80/80 [==============================] - 0s 249us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 406/1000\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 407/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 408/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 409/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 410/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 411/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 412/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 413/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 414/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 415/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 416/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 417/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 418/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 419/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 420/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 421/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 422/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 423/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 424/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 425/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 426/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 427/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 428/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 429/1000\n",
            "80/80 [==============================] - 0s 264us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 430/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 431/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 432/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 433/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 434/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 435/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 436/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 437/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 438/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 439/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 440/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 441/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 442/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 443/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 444/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 445/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 446/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 447/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 448/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 449/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 450/1000\n",
            "80/80 [==============================] - 0s 405us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 451/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 452/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 453/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 454/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 455/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 456/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 457/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 458/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 459/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 460/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 461/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 462/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 463/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 464/1000\n",
            "80/80 [==============================] - 0s 393us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 465/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 466/1000\n",
            "80/80 [==============================] - 0s 248us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 467/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 468/1000\n",
            "80/80 [==============================] - 0s 370us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 469/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 470/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 471/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 472/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 473/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 474/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 475/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 476/1000\n",
            "80/80 [==============================] - 0s 248us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 477/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 478/1000\n",
            "80/80 [==============================] - 0s 373us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 479/1000\n",
            "80/80 [==============================] - 0s 360us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 480/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 481/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 482/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 483/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 484/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 485/1000\n",
            "80/80 [==============================] - 0s 408us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 486/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 487/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 488/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 489/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 490/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 491/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 492/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 493/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 494/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 495/1000\n",
            "80/80 [==============================] - 0s 265us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 496/1000\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 497/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 498/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 499/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 500/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 501/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 502/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 503/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 504/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 505/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 506/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 507/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 508/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 509/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 510/1000\n",
            "80/80 [==============================] - 0s 430us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 511/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 512/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 513/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 514/1000\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 515/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 516/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 517/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 518/1000\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 519/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 520/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 521/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 522/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 523/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 524/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 525/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 526/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 527/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 528/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 529/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 530/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 531/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 532/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 533/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 534/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 535/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 536/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 537/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 538/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 539/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 540/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 541/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 542/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 543/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 544/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 545/1000\n",
            "80/80 [==============================] - 0s 264us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 546/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 547/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 548/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 549/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 550/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 551/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 552/1000\n",
            "80/80 [==============================] - 0s 450us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 553/1000\n",
            "80/80 [==============================] - 0s 371us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 554/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 555/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 556/1000\n",
            "80/80 [==============================] - 0s 258us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 557/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 558/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 559/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 560/1000\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 561/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 562/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 563/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 564/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 565/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 566/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 567/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 568/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 569/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 570/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 571/1000\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 572/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 573/1000\n",
            "80/80 [==============================] - 0s 237us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 574/1000\n",
            "80/80 [==============================] - 0s 254us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 575/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 576/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 577/1000\n",
            "80/80 [==============================] - 0s 237us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 578/1000\n",
            "80/80 [==============================] - 0s 391us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 579/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 580/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 581/1000\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 582/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 583/1000\n",
            "80/80 [==============================] - 0s 402us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 584/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 585/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 586/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 587/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 588/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 589/1000\n",
            "80/80 [==============================] - 0s 222us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 590/1000\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 591/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 592/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 593/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 594/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 595/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 596/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 597/1000\n",
            "80/80 [==============================] - 0s 394us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 598/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 599/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 600/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 601/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 602/1000\n",
            "80/80 [==============================] - 0s 243us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 603/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 604/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 605/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 606/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 607/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 608/1000\n",
            "80/80 [==============================] - 0s 265us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 609/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 610/1000\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 611/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 612/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 613/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 614/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 615/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 616/1000\n",
            "80/80 [==============================] - 0s 417us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 617/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 618/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 619/1000\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 620/1000\n",
            "80/80 [==============================] - 0s 259us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 621/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 622/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 623/1000\n",
            "80/80 [==============================] - 0s 393us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 624/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 625/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 626/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 627/1000\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 628/1000\n",
            "80/80 [==============================] - 0s 254us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 629/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 630/1000\n",
            "80/80 [==============================] - 0s 260us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 631/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 632/1000\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 633/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 634/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 635/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 636/1000\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 637/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 638/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 639/1000\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 640/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 641/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 642/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 643/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 644/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 645/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 646/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 647/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 648/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 649/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 650/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 651/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 652/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 653/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 654/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 655/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 656/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 657/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 658/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 659/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 660/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 661/1000\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 662/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 663/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 664/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 665/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 666/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 667/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 668/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 669/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 670/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 671/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 672/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 673/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 674/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 675/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 676/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 677/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 678/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 679/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 680/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 681/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 682/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 683/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 684/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 685/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 686/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 687/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 688/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 689/1000\n",
            "80/80 [==============================] - 0s 260us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 690/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 691/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 692/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 693/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 694/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 695/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 696/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 697/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 698/1000\n",
            "80/80 [==============================] - 0s 253us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 699/1000\n",
            "80/80 [==============================] - 0s 244us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 700/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 701/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 702/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 703/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 704/1000\n",
            "80/80 [==============================] - 0s 247us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 705/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 706/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 707/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 708/1000\n",
            "80/80 [==============================] - 0s 257us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 709/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 710/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 711/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 712/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 713/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 714/1000\n",
            "80/80 [==============================] - 0s 244us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 715/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 716/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 717/1000\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 718/1000\n",
            "80/80 [==============================] - 0s 247us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 719/1000\n",
            "80/80 [==============================] - 0s 439us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 720/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 721/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 722/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 723/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 724/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 725/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 726/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 727/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 728/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 729/1000\n",
            "80/80 [==============================] - 0s 363us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 730/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 731/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 732/1000\n",
            "80/80 [==============================] - 0s 260us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 733/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 734/1000\n",
            "80/80 [==============================] - 0s 379us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 735/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 736/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 737/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 738/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 739/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 740/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 741/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 742/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 743/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 744/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 745/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 746/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 747/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 748/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 749/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 750/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 751/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 752/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 753/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 754/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 755/1000\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 756/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 757/1000\n",
            "80/80 [==============================] - 0s 268us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 758/1000\n",
            "80/80 [==============================] - 0s 248us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 759/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 760/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 761/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 762/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 763/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 764/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 765/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 766/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 767/1000\n",
            "80/80 [==============================] - 0s 265us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 768/1000\n",
            "80/80 [==============================] - 0s 245us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 769/1000\n",
            "80/80 [==============================] - 0s 267us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 770/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 771/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 772/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 773/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 774/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 775/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 776/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 777/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 778/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 779/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 780/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 781/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 782/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 783/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 784/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 785/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 786/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 787/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 788/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 789/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 790/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 791/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 792/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 793/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 794/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 795/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 796/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 797/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 798/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 799/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 800/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 801/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 802/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 803/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 804/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 805/1000\n",
            "80/80 [==============================] - 0s 357us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 806/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 807/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 808/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 809/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 810/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 811/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 812/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 813/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 814/1000\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 815/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 816/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 817/1000\n",
            "80/80 [==============================] - 0s 274us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 818/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 819/1000\n",
            "80/80 [==============================] - 0s 382us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 820/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 821/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 822/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 823/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 824/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 825/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 826/1000\n",
            "80/80 [==============================] - 0s 264us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 827/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 828/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 829/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 830/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 831/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 832/1000\n",
            "80/80 [==============================] - 0s 379us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 833/1000\n",
            "80/80 [==============================] - 0s 402us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 834/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 835/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 836/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 837/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 838/1000\n",
            "80/80 [==============================] - 0s 270us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 839/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 840/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 841/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 842/1000\n",
            "80/80 [==============================] - 0s 261us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 843/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 844/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 845/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 846/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 847/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 848/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 849/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 850/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 851/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 852/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 853/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 854/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 855/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 856/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 857/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 858/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 859/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 860/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 861/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 862/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 863/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 864/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 865/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 866/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 867/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 868/1000\n",
            "80/80 [==============================] - 0s 376us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 869/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 870/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 871/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 872/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 873/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 874/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 875/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 876/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 877/1000\n",
            "80/80 [==============================] - 0s 418us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 878/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 879/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 880/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 881/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 882/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 883/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 884/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 885/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 886/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 887/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 888/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 889/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 890/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 891/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 892/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 893/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 894/1000\n",
            "80/80 [==============================] - 0s 386us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 895/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 896/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 897/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 898/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 899/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 900/1000\n",
            "80/80 [==============================] - 0s 276us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 901/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 902/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 903/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 904/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 905/1000\n",
            "80/80 [==============================] - 0s 255us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 906/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 907/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 908/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 909/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 910/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 911/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 912/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 913/1000\n",
            "80/80 [==============================] - 0s 269us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 914/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 915/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 916/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 917/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 918/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 919/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 920/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 921/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 922/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 923/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 924/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 925/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 926/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 927/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 928/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 929/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 930/1000\n",
            "80/80 [==============================] - 0s 302us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 931/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 932/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 933/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 934/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 935/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 936/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 937/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 938/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 939/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 940/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 941/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 942/1000\n",
            "80/80 [==============================] - 0s 372us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 943/1000\n",
            "80/80 [==============================] - 0s 277us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 944/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 945/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 946/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 947/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 948/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 949/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 950/1000\n",
            "80/80 [==============================] - 0s 280us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 951/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 952/1000\n",
            "80/80 [==============================] - 0s 275us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 953/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 954/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 955/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 956/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 957/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 958/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 959/1000\n",
            "80/80 [==============================] - 0s 296us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 960/1000\n",
            "80/80 [==============================] - 0s 398us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 961/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 962/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 963/1000\n",
            "80/80 [==============================] - 0s 287us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 964/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 965/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 966/1000\n",
            "80/80 [==============================] - 0s 281us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 967/1000\n",
            "80/80 [==============================] - 0s 387us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 968/1000\n",
            "80/80 [==============================] - 0s 272us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 969/1000\n",
            "80/80 [==============================] - 0s 450us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 970/1000\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 971/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 972/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 973/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 974/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 975/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 976/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 977/1000\n",
            "80/80 [==============================] - 0s 346us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 978/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 979/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 980/1000\n",
            "80/80 [==============================] - 0s 337us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 981/1000\n",
            "80/80 [==============================] - 0s 370us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 982/1000\n",
            "80/80 [==============================] - 0s 345us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 983/1000\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 984/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 985/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 986/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 987/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 988/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 989/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 990/1000\n",
            "80/80 [==============================] - 0s 386us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 991/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 992/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 993/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 994/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 995/1000\n",
            "80/80 [==============================] - 0s 283us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 996/1000\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 997/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 998/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 999/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n",
            "Epoch 1000/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.5042 - acc: 0.0125 - val_loss: 0.3474 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRlWNWHuUJgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y_predict = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e41e4166-1438-4846-e2a5-7fbcd0e62931",
        "id": "ZPo4j_HCymGb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "np.round(y_predict*500)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1uSJXCIVyuE",
        "colab_type": "code",
        "outputId": "b67d928d-b063-4cdd-82d5-49d584130113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "np.round(y_test*500)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([396., 192., 276., 168., 444., 138.,  60., 120., 102., 210., 582.,\n",
              "       222., 420., 240., 360., 204., 252., 342.,  72., 414.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiKPS0ZqV-RV",
        "colab_type": "code",
        "outputId": "94b88529-0a5f-4cf6-c277-676706890a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.scatter(range(20), y_predict, c='r')\n",
        "plt.scatter(range(20), y_test, c='g')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1YSBL0k1YeBLUk38P5Qe7xJMJ7A/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciV2zf2hWRGI",
        "colab_type": "code",
        "outputId": "b08c3c94-31aa-4a4c-994e-0d4f717d8b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "plt.plot(hist.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXm8FMW593/PnAVkXwUE5CCCiCig\nRxRBRYl7rhpjXF+NXr3GJS6JuZGYRL0x8cYsLok3GhJzSbxGYxRXjDvuigIisiiLIqtw2OGwnGWe\n94/unqnu6aW6p3qmp099Px/lTHd19dNd1U8//dTzVBEzQ6PRaDTpIlNuATQajUajHq3cNRqNJoVo\n5a7RaDQpRCt3jUajSSFauWs0Gk0K0cpdo9FoUkhZlTsR/YWI1hPRfAV1HUdEc4X/dhPRmZLHTiSi\nrcKxt3iUG0xEM4loKRH9g4hqHfu/SURMRPXm755ENIOIdhDRfUK5zg5ZNxDRPea+fc1jPiKieUR0\navS7kjvfd02ZmYh6FVufRqNJPuW23KcCOFlFRcw8g5lHM/NoAMcD2AngJWc5IlruUcVb1vHM/DOP\nMncCuJuZ9wewGcBlQr2dAVwPYKZQfjeAnwL4gUPW7cK5RgP4EsA0c/dPADzGzGMAnAfgD37XLck7\nAL5mnkej0bQByqrcmflNAJvEbUQ0hIheIKLZRPQWEQ2PUPXZAP7FzDuVCGrIRTBeGo+bm/4KQPwy\nuB2G8t9tbWDmRmZ+W9zmUu8wAHsDeMs6DEAX8++uANaY5aqI6NdE9KFp0X9HVnZm/oiZl8uW12g0\nlU+5LXc3pgC4lpkPg2HxRrFczwPwSMhjxhHRx0T0LyI6yGV/TwBbmLnF/L0KQH8AIKJDAQxk5ukR\nZf0H51OFbwPw/4hoFYDnAVxrbr8MwFZmPhzA4QD+g4gGRzifRqNpA1SXWwARIuoE4CgA/zQMZQBA\nO3PfWQDc3CWrmfkkoY5+AA4G8KKw7X8AjDd/7kNEc82//8nMvwAwB8AgZt5h+rifAjBUUuYMgLsA\nXCJT3oXzAFwk/D4fwFRm/i0RjQPwEBGNBHAigEOI6GyzXFcAQ4loBYCPPeq+lJk/jCiXRqOpYBKl\n3GF8SWwx/dA2mHka8n5pP84B8CQzNwvHXmP9TUTLnfUz8zbh7+eJ6A9E1IuZNwjFNgLoRkTVpvU+\nAMBqAJ0BjATwuvlC6gvgGSI6nZln+QlKRKMAVDPzbGHzZTDHIZj5PSJqD6AXAILxRfNiYU0Y6Xce\njUbT9kiUW8ZUsl8Q0bcAw89tKsAwnI+QLhki6mv61EFEY2Hcl40O2RjADBj+fAD4NoCnmXkrM/di\n5jpmrgPwPoBAxe4j6woAk0xZDgTQHkADjC+Rq4ioxtw3jIg6hrlOjUbTdih3KOQjAN4DcAARrSKi\nywBcCOAyIvoYwAIAZ4Sorw7AQABvhBTlbADzzXP+DsB5lg+ciJ4non3McjcB+D4RLYXhg39QQqbl\nMN025jWOEHafg0LlfiMMf/rH5r5LTFn+DGAhgDlm6OgfIfnlRUTXmT78AQDmEdGfZY7TaDSVC+kp\nfzUajSZ9JMoto9FoNBo1lG1AtVevXlxXV1eu02s0Gk1FMnv27A3M3DuoXNmUe11dHWbNkhlz1Gg0\nGo0FEUllmmu3jEaj0aQQrdw1Go0mhWjlrtFoNClEK3eNRqNJIVq5azQaTQrRyl2j0WhSiFbuGo1G\nk0K0cteklpcWfIX12z3XSdFoUk2gcieigeaanguJaAERXe9RbqK5HugCIgo7cZdGo5Tm1iyueGg2\nzp/yfrlF0WjKgkyGaguAG5l5jrlO6GwiepmZF1oFiKgbjBWTTmbmFUS0d0zyajRSZM0J8VZu2lVm\nSTSa8hBouTPzWmaeY/69HcAimMvLCVwAYBozrzDLrVctqEaj0WjkCeVzN+dLHwNgpmPXMADdieh1\nc2Hriz2Ov4KIZhHRrIaGhijyajQajUYCaeVurm/6BIAbxGXpTKoBHAbgNAAnAfgpEQ1z1sHMU5i5\nnpnre/cOnNRMo9FoNBGRXcmnBoZif9hcy9TJKgAbmbkRQCMRvQlgFIDFyiTVaDQajTQy0TIEYzm5\nRcx8l0expwFMIKJqIuoA4AgYvnmNpizoBcY0bR0Zy308gIsAfEJEc81tNwPYFwCY+QFmXkRELwCY\nByAL4M/MPD8OgTUajUYTTKByZ+a3AZBEuV8D+LUKoTQajUZTHDpDVZNKtFtG09bRyl2TShhau2va\nNlq5azQaTQrRyl2TSrRbRtPW0cpdk0q0bte0dbRy12g0mhSilbsmlbD2y2jaOFq5a1KJVu2ato5W\n7hqNRpNCtHLXpBLtldG0dbRy11Qks7/chHtfWVJuMSqeLTub8MPHP8bOppZyi6JRjFbumorkm/e/\nh7tf8ZlRWlvuUtz76hI8NmsVHv1gZblF0ShGK3dNKtHTD8ih3VfpRSt3jUajSSFauWtSibZINW0d\nrdw1qUTrdk1bRyt3jUajSSFauWtSiZ5+QNPW0cpdk0q0ag8HBS6kqak0tHLXaDR6ADqFaOWuSSVa\nWVUWbyxuwLceeBetWd1wqqgutwAaTRzoJKZwlNstc+3f52Db7hbs2N2Crh1qyitMStCWu0ajSQz6\npawOrdw16UTriIqCyv3pkEIClTsRDSSiGUS0kIgWENH1PmUPJ6IWIjpbrZgaTTi0bpcjaSGjCROn\nopHxubcAuJGZ5xBRZwCziehlZl4oFiKiKgB3AngpBjk1Gk2KsQx3rdvVEWi5M/NaZp5j/r0dwCIA\n/V2KXgvgCQDrlUqo0URAW4ByJMUdkgwp0kUonzsR1QEYA2CmY3t/AN8AcH/A8VcQ0SwimtXQ0BBO\nUo0mBHpgTo6kuWU06pBW7kTUCYZlfgMzb3PsvgfATcyc9auDmacwcz0z1/fu3Tu8tBqNJtXol406\npOLciagGhmJ/mJmnuRSpB/Co+YnXC8CpRNTCzE8pk1SjCYHWEZWJbjZ1yETLEIAHASxi5rvcyjDz\nYGauY+Y6AI8DuDpOxb51VzPqJk/HKwvXue4/54/v4YZHP5Kq64z73sZNj89TIldLaxb7/Wg6Hv1g\nhZL6VHLL0/Nx8j1vlluMyKzctBN1k6dj7sotUuWTqiSWNexA3eTpWLTW+fFbXqL4vH80bR7+7fdv\nS5dft2036iZPxz2vLEbd5OlYv313/vym7//9zzeibvJ0rNu226uaRHLbMwtw0t1vYtVmo5/OWbG5\n3CJJuWXGA7gIwPFENNf871QiupKIroxZPleWrNsOALj/jWWu+z/4YhOemrtGqq6PV23FP2apWT+y\ncU8rsgz84vlFSupTyd/e+xKffrW93GJE5q0lGwAA//gweS/OMLww/ysAwDMfy/XPJPPIByvxyeqt\n0uU/XL4JAHCPubD5nC8LX9RT31kOAJi1vPzKMQxT312Oz9Ztz/XTxz4s/5q0gW4ZZn4bIV7szHxJ\nMQJJnSPuE0RFD/nHhhXUkfUd1cmTdN9tW+wq5HPV1p5swtutkqjoDNW2+IC0Vay2lo2CSaqOSNpL\np5TSFEZdcsG+Sp83LEnNW5HKPUk3UFMackkuKWn7hISXl5SMxDWnpHkT0b4VqdwtknADNaXBGnBL\ny8OfFEr7CAWfLWlfNpVMRSv3xPaDpMpVwYT1ySa1byRNrnK6ZdzuRdLuT1iSlDxX0co9aegvifig\nlE0+4je4mFYyvg+IsS89A6rlb9+KVu5ambYdMh663eszPkkWlEgypSoNMo9rpQ+oJomKVO5J9csl\nVKxUkI+mqGy3jEXSDJNSTCBW4JZx2ZfUZ1uWJIlfkcrdInGftglq2LRhtbXz4SnXw/TDxz9G3eTp\ngeVG3voi6iZPlypbDG8sbkDd5On49Ktoma+3PrMAf3xjGV5a8BXqJk/HsoYduX0T7nwNR//qtaJl\n9HPL5EJdhfZ8e8kG1E2ejgVrjESpusnT8f3H5hYth0jd5Om45u9ziq7DyZMfrULd5On4amv5Mm0r\nUrknXYcmXb5KJKzLPe42eGzWKqlyO/a02H7H9TKyMl+Lyex86P0v8dy8tQCA+ULm6arNu7By067i\nBASk/DKiO+3lhcY1ffjFpty2aXNWFy+Hg+nmNatkd7ORbSe+JEtNRSr3HIkz3LVaj4tcKKRDO3rd\n8aR/3qvuuqq8KnEOaDpFFE+VmiSmcgsgUNnKPWEkXJ9UNG6f7ZpCVN2eOHzw/tEyBkl/KVcSWrkr\nRHfL+Mi7ZSQHVGOUpRji+rpTpYrjvG8y7wut29WhlbtCtNURHxnyGlD1CIVMelMkJFym4D6Zv2Wm\nCgiLMwBCfNFRWuLcEyR/RSr3BN0/V7SSV09aZg2MXfwiTsCcv79xRKLJZKhaPvdKHb9KktQVqdwt\nkmH75ElSw7YVvO+5bo0oWAo3jg8L3/zUkG43TTAVqdyT2gHyD0bSXjuVT1pmhYyra6iYnUGULQ4x\nZZ6L3DOUONOt8qhI5W6xqbEp9/f23c1YtXknGrbvyW3bsCP/9+7mVmzb3az0/Nt2N2NPS2vut/XS\naclmsaxhRyj3zK6mVjTuacGellas37Yb2xXLKsOOPS1YuWmn575dTa1gZtt9BYzlBbfszLfFyk07\nsaup1VlFUViKIcv2dnXe4k2NTdi6qzkXZxyW5RsabW3qZMvOJuxutu9vNq/feV+ctGYZm4U+64ZV\nx8Yde5A1fRSbG5vQ6hEjuLmxCS2tWSll2LinBTubWnzLWH3YTQ8HXZ/I1p3NaGrJt8HmxiY0OI63\nZaha24SNGwLuldgXjTZvxUYfGRvNPuyGlY9g1blu225sbmzCrqbWglyFHT71JMn4kFogO3GYN3DJ\n+h14eu5qnDG6Pw6+7aWCYvU/fwWPXnEkjtyvJ878n3fw6VfbsfyXpykT45DbXsKh+3bDtKvH27bv\nbs5i0m/fwGPfGYexg3tI1VX/85fR2NSKsYN74AMzaUOlrDKMvPVFAMDd547CN8YMKNjXpX01rps0\nFD+fvggzfjARg3t1BADc9MQneGLOKiy741RsamzC0b+agYkH9MbUS8cqk816+F9ZtA71P3dfO3fb\n7mYcevvLkc+xdP0OfO2uN3BO/QD86uxRrmVG/8xePzPj+499jGfNZfMev3Ic6uvc2/z25xbir+99\nCcDdMn532QZc8KeZuP3MkfjpU/Nx3aSh+M4x+2HM7S/jkqPqcNvpB9nK72pqxZjbX8bF4wYJ8nhf\n30G3vojqDGHpHad6lsm/QwolrP/5K5hy0WE48aC+3icxGfWzlzBp+N548JLDwcwYI9kulkH00YrN\ngclFf3lnOW5/biFm/GAijvvN67ntH9w8CXt3aV9Q/qBbX0SH2ios/NnJBftG3voilv/yNDw8cwV+\n8tT83PaeHWuxsbHJ9iyOvPVF9OxYi9k/PUHqmspFRVvuAPDeso2++61Mu7jWD52zQlgH0vFgidZs\nEI2mJfCBkI1XLrzu6bbdLXjTXCPyy42Nue1PzzWyBrPMOSvn9c8alMok80m/dWdxXztWe73/uXsb\nuH2JZRk5xQ4AC30Wvp7+ib+y+mSV0VdfmG+Ue2Xhutz9fN7l2O17mnP7ZN09LQFZQkE+99khFn5+\n9dP1tjr9EL/MAGCRxDQKby8x+tjnjizQ9du9rfedAV+Ub5v922Kjx9eD1/YkUfHK3etz1aKU/m+n\nJEGyJRU/sd0+n8XicUWzeIXmqRx/sRSf57lc5x+XP3+V0Bdlu6V1P90SgKxTi/uKj9SyomV8d4ei\n1TNc1e1lKX+C2mpDfTW3RnPBuVFVZAxokiLlKl65B+nPUg7LONs1yEpKKn4PmF9UA3N8nbsU7+jW\nrLciBdz1WpgmjqI3sjkFXrhPlFdZEpOCoICs46aEMXKskjLdqKbKUF97WtQp9zTFQlS8ck/Sm9Kp\n8CrVcve7pV7JRIBx/XFdspf6Utn81svY6wF3e+mFsTQzEbR7NidT4bFW/yrW2hSxrqaYGp2Wepg2\nsp5nmWMsy73JodyLUdAq72W5qUjlLra71ydfEkil5Z4rk98mPpCxJRmVxHI3lISn5V7kpYn1hrW1\n3UTKuWWEp7hopwz7v+Bk6ncaNWGeUauo01ByM+Jqqyy3jLo+V1Wk6Z6kJ74ilbtIoFsmJqXg1tmc\nmyxlUWn4+twDFlWI65JlJp0qlpZWf7dMsZZ7lEvwcxVZSjNDFNmNUqBEzX+Lud1Oo0bmHjkXY3H2\nQbcqLLdMk0/oaljSlKMSqNyJaCARzSCihUS0gIiudylzIRHNI6JPiOhdInKPI1OE2NBO/16pcDut\nc1MqLXfLLeOyL1bLvQS0Brhlil3QWaw2bHSLn8+9yjagKi+PGyqSiFodlnSYZ5QL/nD9CUAcULXv\nLUb2qoo3d/PIxLm3ALiRmecQUWcAs4noZWZeKJT5AsCxzLyZiE4BMAXAETHIC8CuQIKUSVzv4RYX\nE9VpzVauzz3YLSOWyQ2CgWNL4ijFBGEtgQOqxVru4Xtjq4/PPRdJU4SfuHAyr9yOyFjPhiWy13Pg\nOp971rLcJdwyls89UdEyigRRQOB7ipnXMvMc8+/tABYB6O8o8y4zWwGw7wOwZ8AoxuZzL1MopNt5\nC6JlTItiwZqtZVP0n3213TXjsqU1i4Vr3GOJ/Vwr1u10iyWuVMt9U2MTVm3emXeBZICvtu7G+u32\nJdJkvtZs+xz3IkpPdCpKse6PVxo5FqI+csrTuKelYDUgsS96+ba9ZJUJYHB+UYTp+nmfu2O74/f8\n1VtRY164c0B15eadUjkmq7cUri7VptwyIkRUB2AMgJk+xS4D8C+P468gollENKuhIXqSC9ss98jV\nFIWMy6U1y5i3agtO+93b+MOMpSWQys76bbtx0j1v4panFhTs+81Li3Hq797C0vWFyV1+Ctqyam95\nurBODji2GGRqjXrqo375KibcOcNmuR/5369i7C9eddTv8kL3eREWdJEIc7d4+dyfnrsG//n4vNw+\nr7GQy/86C5N++4Ztm1MZulGMksvdx0z4aXytkoWWe/7v9z/fiK///m08PHMFgELL/TsPzbZlrHox\n/peF68IWO6CaJKSVOxF1AvAEgBuY2dXkI6LjYCj3m9z2M/MUZq5n5vrevXtHkdesx1Zn5HqKwelX\ndKMly1izxbD+5glrUpaKbbuN7MYPlxdmXFqZu2tdFvCVGVAVyVlbHF8opJd2V5HEZM1D05qzkr0G\nVN22eZ/f+bUWzXJ397l/ti7/UvZzJbz3uZFtLD4nfnPnqHicnJa7p1vGZz73gnn7hbJrTIvbyhJ1\na4PNEbOVi3bLFHW0WqTmliGiGhiK/WFmnuZR5hAAfwZwCjP7zwlQJGJDB4VZxfUidrPc3aJlyjmb\nodVP3U5dU2XsdMvu8/e5e99QDji2GGSUeLGKPihD1a16vzM6lY740pDtlzmfu+O+i3XLRBKJ3dUv\n6ce6h8WMcVj3sSqK5S4YCl7ntXztFiqDKlJkuEtFyxCABwEsYua7PMrsC2AagIuYebFaEQsRfcLl\nylB19bk7HvVyR8vk5+solCMfRhZukNCv8xs+95BCFon9K664uoIyVMOGQjr3yRqF4mHWuE2BSEKZ\nTIYCI0TE/rrHZ8bMvHKVEtXjXFa+gPFbJjzWGQpZ4HMXNtQ4QlpU9rk0uWVkLPfxAC4C8AkRzTW3\n3QxgXwBg5gcA3AKgJ4A/mAqlhZnr1YtrILZluUIh3aNl7L/LHS3jNg+MRU2V97wc/m4ZP+0eo89d\notpiz52Pc/eQwW2bzykL3TJyikO8Dq9oGbHmKolqxTpl3DLFfAU5LXfvuWXyfzv7qp9bpqbK+yum\nWIqJPALK5yZ2I1C5M/PbCDCAmflyAJerEiqIMKGQcX1nySjuliwnYskBtwfVekDcBtdkMlS9ziMe\ny8zKog+8RGKPv6Ngvei8lLDbffF7mKO+JMXjWhxWcK6MUMg+oOpVp6jc823u/PLJu2W8ZQ/CmQwW\nbkCVXY/xs9xV6tMUGe6VmaFqS2Iq05vS1efu+G1/AcjJqXJqC+uMYS13v1vqJx+zWjdJWIq1mnaZ\ni3CEm1vGuz7nV6Ws4hCvwyuxSqxZxuduc8v4WO65NUwlXqZB58pFy0gYQs4pfwst9zxO5a7yCzlN\nbpmKVO5iU5fL5551eejckpjCWq4qJy7yijwA/KdL9fsk97seRsivqhDI1FTs6azVdTyVpUv9fvcq\n6vWLfdprHMA2oJoR3Bpwf6F4DajaltbziZcPg+WGiRbn7mW55387W0dlP9MTh5UZt0mrSk3Oryim\nfjvK2JJFJMVUOYeK9QJyu0c1PpMu+SYx+ZzPGQqpsmW8ozfkX/RBNJpL0GU8ngr3UEjv+py+ZtkX\nvZTPXai6KkOeXwXW5qzNcnd3yxgbCmUIi3O2SpkMVQuv+8k+ZVQObZViDqNSUXHL7H3esANT312e\n+92aZWzd5R3TunT9DteVhT5cvsnW6f74xjL07WoszXXYoO4Y0L0DmBkvzP8KRISTDupje8Ae+cBI\noGjJMh58+wt0aV+NlZvtGW/vLtuQs5Bnr9iMt5Y04MuNO9GhtgpL1u/A0L074Rtj+tvqFR+8p+eu\nxpiB3dHKjEc/XAEC4ZihvXDwgK6Yu3ILdjW1YtKBfWzWxrtLN6Bhxx4cMqAb5pir5qwRYtmnvLkM\nJ47om5Nr5eadeOzDlblrB4zY6GlzVqGxqRXjh/S0XZP40Le0ZvGaudoOADw3by1eXPCVa9mVm3Zi\n665mHLRPF9s9fXXRehwzrLctvG13cyveW7YRvTu3Q/uaDGZ/udnz5bh8w05sNtc13afbXgX7m1qz\neHruagzo3gG7m1tx8ICuWLp+B1Zu2ontu1twuLAk3s49he6Kqe98gZNH9sOitdtc+9m6bYV5Auu3\n78bfZ67AkN6dbNudamPR2m1oV53Bxsam3JKFQF5ZLVy7DYvMFcSsJn7/841ghi3r9J2lG7HdzGl4\n9MOV6NmxHTbs2IODB3TN1fWC0C5WtMyitdtyiUAAsGrzLvQz+wEz8PLCdTh2mD0fZfG67fjru8vR\nvsb+BmxpzWLKW59jYPcO6NWpnSFzxsjOfnruGrgx7aNVaMlmsW+PjvhiQ6Ntn3PlNL/cFi+3DzNj\nwZptWNawA32EZfc+XL4pNy7grOfVT92XcPxiQyM27thjW0Lx/c83ura/k+bWLP73nS9w9NDeWLBm\nG8bW9cC+PTsEHlcsFafcP1m91bYUXZaB7zw0y7P81HeX214GFt964D3b7//+16e238t/eRqe+XgN\nrn/UCBAS145cuGYb/mauhQkYa2O6sXjdDixeZzyEW3Y246IHPygoc+R+PV2VEoDcuUUeeGMZjh3W\nG28sNjJ8f3Lagbj86P0AGGuIXvBnv+Rh4I7nP8Udz3+KqycOAQD87zvLXct9/7GPAQDD+3a2PWhP\nCQ/qva8uwe9fy2fe3vqMPWtVfAaP/tUMAMB9F4zBd//+EQDgkqPqMPXd5bjy2CGYfMrwXNlbnp6P\nx2atstU1sIf7Pfq3+97O/X2yx9qe4n2csH8vvL00v5RaX+Ght3zu4pfLbc8uxIzPGnL328lZf3i3\nYNvpv38HX7k89OIXARFwyr1v5X7v26MDLjxiXwB25fW7V5cAALp3qEU2yzhvyvuucswzl+j7vKER\nN/7z44L9P5r2Se5vK6NTPL/FFjP5Z8an6/GPWStx7fH72/a/tWQD3nIsRQcA/zNjGe5+xYiCvufc\n0QCMr9rTfvd2QVmLd5ZuxDtL5VJixHvy1lL7+b2+Mp6euwY3/KPwGXI++xaPfLgC81e7T8lhZbyK\na6l6tYUIM3DvK0tw34ylAAwdM7auBx67clzgscVScW6Zrx3Yx/Y7yxzb+qji3BMbduTnqtiyS936\niVGWCFsiZCeKMsqklVvIfn763VvnnCVO3J65ddvyc9IsMac+WLHJbrUt37Cz4LhNO4Lv+ZebCo9z\nsnid/XpEJWx9yTmVxYYd3mtyuuGm2AF/19wKQXY3ZdV1rxplaxf4uTItd2ODec0rJO4pAKzanC9n\nvSSLDSsUEUXe6GgPL7dM6HZzydYOi1sI58rN9nsY9NyoouKUe0F2Gsc3E6GnslR4vij+QhU+RhXP\nnVsClIhs6KBMkovMJReb8+A1AK3Kpytbj9v9yDKXJG8i56PPDWyGr8MyWFRGnvj53L1eVkmYBMxN\nNJUvPT8qTrkXjJRn4xtU9VLupRgo9EPFFKcqOn6QHLJXVvAS8Jm/xg+3xLIwWEpDZrrZYvFLaPIK\nuVSV8ex3OewoI/PCNPIZ8r+t50bp8n8+Qnt90cgkd8WNm2SlCresPOXuEhIWlz1jiyoQzqLyWY9S\n1Z7m/MBfVFlURAUErYAjG3Hh1B9RJZOxbP1KeLllVIXayb4kXKNIsiw1WZ1U/T53wboHVgmZa2e2\nv6yaA1a0ioIoheyXVRxhjUEvO5nlAUsVbll5yt3xO2uk1cWCt+Wu7oRRrMLdCld7L4YgH7/fdLi2\ncoqUZ7E+aS9XRBzeED+95zWHTbFfJhZ+tymn3D3izV3rc/zOuWVi8rkXRMuU0C0T9uvJrbRXqK1q\nKk+5O9qrNRuf5e412KnUco9QV5i4YS9UWKPBbhk3n3uwLK7TCku0soxl63fZeZ97XJa7XDkv5a7K\n5+7rlim4dpn67G4Z67lROqAqfjk79nlZ03FYyKHbwKW4dst44JbMUQqfu81yUHiO+IfIPM6rQrkH\nWO7uKxe5+5NFoq6BKWe5B7skCgfs5GXw92cX4ZZR6XP32dfqsNilfO6wv5CbcgOqUSX0OIn1p0sm\nuBtxKNGgrye3aBknekBVktYIPndZxbbH03JXp5LVpujL16VCTwS6ZaR97orcMsVGy5iX45QnjDvE\n75rFXX6Pt6vlnlVpuUv43K0BVUmfu0izGUWldioN4XwF+7zcMspOnyNsG7h1nVJZ7hWXxOQkSiik\nbHmvea+VWu5l+gx4aeFXwYUCkLHc3/98I9oJ4atu11sQLOPS93f7zEFuITWgKuGWcVbjls3oXYfP\nuYuooznL+PNbn0vL4Yev5W6e/F0zq1vKLWPY7rnfObeM0gFVwS3jbJ8SumVU+NxLNaBa+co9G36A\nU7a0aLHZjimzz91+fL6CMP2WoXKlAAAgAElEQVTOypwthqYApccozKh0O0LG5y5DsW4LyyXhtGzD\nJJr5Wbq2ya9CDqi+ubgBb0pLEYDPbXK+IKWjZVx87rENqDr2ebplyuBzd+51+0oq1fw1Fe+WiWK5\nRw3Rs1A5hFsut4wKgpRe0KV5uUGiImO5+66c5BEKGSbzVyaGvJg6VOA/k6Xzd3hhmuJQ7uLfkj73\nOKgkyz0dyj1EeQ7xMihmHclyUOqFn4LdMoUCiZvyg3f2MpEHVCVugF+ZfBKTfbvbzJnedRTfCHGv\nUSATCpmTRSZ7mO1jCLHEuQtCO8X3Urhx3MagiKyCAVWX4npAVZLWLEK5SYyw+AgDfWLnSpJbRvi7\n1EsOBoZCuvnXBYlbzONVyV2sz11MYnJzM8jg63MX9oXNUFWJr8/daRVLxbk73FixZKiKfztfQO4y\nhr2PMsUDo2UKHTMF9ZYqc7bilXuUyBXpeOMS5AqVayUpFQQp00DL3cMNEhWZT2aZBa2Z7av9hPkU\nl/W5+9chfbpIhIlzl5G5cIBT/YCqiPO59HoBxXEfA33uMpa79rnLYbhl5FsxlAvHo7TKPqOyrqS9\nKILE8XTLxNj3/Z5NMYmptkr9oyGe2nmN9hW9lJ/aIYf3CQoHVGXqcw6oGgepvIW+UU6KLHeZfhf0\noi94ObrUq90ykmQ5bJKJvM/dc0BV5SCo0rqUVaWEQOVuWmClFNvPzSD63KM+fn4uJr/7IZ4v7tXF\n/MNBnb9lLHd2zC2j/pPXtvC6o8d4Kdw4BlrDWu5u909nqEoSekAVYTIF3QdxkmS5i30ncZa76/QD\n+W2t5ve1U5nFOVWrn+IUo2Wi3kn/OHfvneKnejl97k6iWe5Wu4YSK/Acub8d9Xop3JYYXjKBlrvz\nt0txHS0jSTbLoSwdDmHpe1vu0qeTkEeh5a6sJjW4Tj8gbGtR7HOXwe/ZzMe5R5fJ3+fufZyoHGMf\nFw9xbXJT/tp/W/kPSo0gn3wOL+UelIcRhaCvkoLpol3KlMotU/FJTI1N/tPOOlm6fgc2NQav6vP7\nV5dg9pebc79veXoB1m7djaOH9sLVD88OLacXd77wGe6/8NDIxz/0/pf48WkH4o7nF9mW/ksCD75d\nmFH5u9eW5P7ODahmjTVWn5izCkcP7Y03PZa0U4HfZ/WXG40Vc8TVrcLyuWMtUJFVwhq7P5++yLZP\ndGuUwnKXvcZPVm8NLPNfzy7AtDmrc78/XrkFgLEGqwpue3Yh+nssRQkAS9a7J+Td8fwi1+3F4Gw3\nJ/e8ssT2e9qcVViwxr5035uLG/DkR6vwjTEDlMsnEqjciWgggL8B6AOjX0xh5nsdZQjAvQBOBbAT\nwCXMPEe9uMVz1v3vSKWy//blxQXb7n99Ge5/fZlSeT74YhN+8tT8ouq48v9m4/XP4lOIUfm/91cU\nbBNjxq20/iwzrnhoNhat3VbwcDg5emgv1zU8k8KzH7svBh1IKQdUGbj6/9QZKKJijwv7y0juBsXh\nc7deXLJ4PZfvLN0Yu3KXccu0ALiRmUcAOBLANUQ0wlHmFABDzf+uAHC/UikVIqPYS02YDEg3Gve0\nKJIEuPCIfTGkd8fAcs7lDqMgTlK1J2DhD4tLx9cVfV5Zxgor3RfDNccNCSwjfqjHn8TEtoVoKo1S\nJ+vFwQXmYuhxEviEMvNaywpn5u0AFgHo7yh2BoC/scH7ALoRUT/l0mpcUelbNBJ4JHyCCk4p+txl\nY3+TsC5mWKolVmco9YBqJd5Hi7ijiUpBKe5+KPOLiOoAjAEw07GrP4CVwu9VKHwBaASqFQ6qqIwK\nyGblOp6KeWysaBlDucsdU6oEEJXIfOXYBlRjNqqZ1SyQXi7SYLmXoh9LK3ci6gTgCQA3MPO2oPIe\ndVxBRLOIaFZDQ/J8xKVE5Yh5mClpg5Bdqk6F8dQiuGVkO3upYoRVIvMiL6lbBpX5krRIWshvFEpx\n+6WUOxHVwFDsDzPzNJciqwEMFH4PMLfZYOYpzFzPzPW9e/eOIm9qUKmkVCaNZLMs1fFUPF72uVxk\n3TIKTlxiaiRSNcXrjz8SUq6Nk0opZ4GMi6iT44UhsNeZkTAPAljEzHd5FHsGwMVkcCSArcy8VqGc\nqUOlW6ZZ4Xd81pFt6IUKv2c+WkbeTVBSpaToXDUybhnh71JYphWs29Oh3EvQADJx7uMBXATgEyKa\na267GcC+AMDMDwB4HkYY5FIYoZCXqhc1XSTXLSNXTsUZrVklwwyoVqJbpkbGLVPiUMhKRrtl5AhU\n7sz8NgJe9GyYcdeoEqotoDIFuSxuGaVZumEs9wpU7iHdMnFT6kVdVJMKy70E304Vn6FaqahV7mpD\nIb0gisfqW71ll22dVT8qMcqjWmIC71K+syrd8I1hVgEAwO9fWxpPxS5IRMcWf474T6FxQ6V7IWoo\n5K++eQh+dsZBtm1+VlGc+scvbV+kVPNyAOquV2b64FK+s5Kq2y85qk6qXGspFlqImUQMqCaZi8cN\nKrcIkSm35X7CiD445/CBuOhI+z0ME7lSDkoZwqfKwk2cW4aTqeAH9wrOjAbC5wEM7OE9L025SEwo\npEY9Khs3aLk7N6wvB6dSac1yoiMpSimbqoE7GbdMhkqncJPqc5d9JsK2i8zLtdQkLkNVow6VoZBR\n8PL5Zdn7IUuCRV9Ky12VcpdTLlQyX3hSfe6y/SvsgGocq2qFxZmlXIpnqfxXXQTlVzXRKaXv2PX8\nHp3Ld0A1LmFCUIqBKAtVQRlSGaqUXIu6VMg+EmGVexIs9461Vbbf2i2TYsodr+3l888mPHuxFANR\nFsosd8kkptJZ7sl8ici2bXi3TPk7dIdae2CidsukGFG5luNR87Tcs94PWRKUfmktd0XKPeSskHGT\nUN0ub7lXoM+9Yzu75Z6oicM0arEp9zI8bV6dy+/BKaXV7EUpv3hURdzVVEu6ZUrUDxKq26WNh9aQ\n0WEq1h4olgLLXbtl0ku5LXcvY0Y2Q7VclHJQV1m0jITlXlq3TGnOExbpAdUUWO46QzWAju0qV/yu\ne9Xk/i7HEnlePvfO7as915itqSKEXLJWOaVaOR4AurSvCS4kgYzIa7budl3aUSXtazLY3ZzF3a/E\ne56oyLoq1m3bE6reRPrcteXuz3WThpZbhBz/cfRgHNivi3T5/zzpgBil8aa2OoOLjhyE8w4vXObr\np18fgd98a1Tu9zHD7NMyf/MwuTUfe3asRaeAF2/UUFDxpejG5FOG4y+X1AfW40zecuO+C8fg6WvG\n49hh4aenFi9PVFqnjOwbui4nI/t3wY9PPbBge5AS+9f1x/juv/nU4bj9zJGB5z9z9D6BZZyMHZxf\nsnBkf/fnJGyPqB/UHWcd2h+XHFWHg/t3BQD077YXRg3oait38bg6zzr8Ft52UswzO6D7Xrhe0Fda\nuQfQvqbKdfs3xtgXgbr2+P1jl+XHp41Aj45yll676gw61LrLHjcXHzkIt585EqMGdivYd9mEwejZ\nqV2u510zMb/259FDe6FPl/a5337KsbqKcOWx+wEAThzRx7XMr84+BEB4fygB6N7Bfp9/Liik4w7Y\nG8cPz5/zxhOGudZz+5kjAzOc9+7cHqMGdsNf/31sKBkB+1el+CD/8qxDpI7fy6NvA8DUS8fiIkH2\nvTu3AwDc8vURvgo+KAP0imOG2Nr1me+Ody133PC9Pes47gD3F+F/nZ6f5uL+Cw9zffkHea/EcMK9\naqrw+FVH4a5zRuO20w9Cz061AIDvnTAMf7vsCNtxhw3qXiBXN7MP/eS0A3HawXIrgl5zXHQ9UlOV\nwfeEvqjj3BUh4/NUgawfrSpDZUsIqpKZxMr815lZKVqgQeJboche2ZmWeyWsAe92XnGbcwIyv+uN\nNWJBcAvb7ptkV/RzP3nJTURKfble5/GTzatfi19qnklyAbKL53WKIP507vP7SiSikuScOO9lKbyL\nqVTuzvsmE62g5LySp8lQ+eJOwkWb5Ms649+DarEGI6s8XqzWC1dF9It4N9vVOJR7wkaHZV8ofg8/\nwd7XrL+JoDSA2ktUv3vqtafKptzd+3/QrXHW4XV+576qDHkGLRABpXDJO8XVE4cpQibOuJRkqHyp\n/DIDknnR8o8Es/3BDZI/Z7l7nC9vuYe7DwT/r5521XZ3hr+VGerUkcn4WJxeBMktKgfrb7V2u7cC\n8rN0PV8Ifqa2SVB0UpWM9U+FVfv1l0yJLHcn2uceFednWQJGy0WqMlS2eclllKlbCedqSUHVcM5y\n9/9MD/1ghXXL+CmiGK0nUU2JIoRx3XlBRJ6Wu0qlEcVy97LdbX3H48igvAK7W8bLNeW+z+u9kaF4\n+4EXWrkrojrmOFer08la4xnFvtEwhAklFB8I54RiQfJbVpin5W6+cMOGNrr63IW/wyj3UmGzskO4\n7jzrc1inJPyrsl9JW+Ehj3GzroF8n5F5qRR8CIQwPEQyRGWZ00e7ZaLiaCuZNSyLITd9rmT5TIZK\n5hJwIueWMcrYbmPI/m+5ZQItdwX3QbyXzhd5KdP6vYhyjf5fHE5lZhkXii33KG4Zj+0y7WApd6+k\no0ym8JoLz+++3bP7kt/O+NCWe0RaHLPGxW25Wy592QarooQrd5dtTrdMUDWBlntkn7vbNu86yjW1\nsm0qgQgi+N2XDLm79Qik9GUWaUDVY5c47OU1bmI9tl7GWHUmuP95u2XcNbhhuZcePbdMRJzpyXFn\nqIW23KmMA6ohzmt3yziiZXyqGdSzY+7YwGiZ0G4Zwn7OeG3ynrO73FMrA9HcMn7liJyWe/7f/XrL\nrWYkJYPHdv9IHo+XuUNet+uzpvL1MsbEttyww55FbXNThXLLlGdup1L0ynQqd8fEQrJx7kTAveeN\nzmW7eSXAOJF5C0+7+ij88qyDjfKZ8oVCuim7GT+YiMevHJf7bV0OM+fWtcyGiJaZctFhuQfV66PJ\n2h7Fcv/TxfW27FkC8OqNx+KR/ziy8DwRXqIvfe8YPHWNewKPLGIPtMV4S7a8KPYxw3rjTxfns269\nZ+0kTL10LK45bghm/GBiGHEDZbDvCH+MTDtz0NeeZFuGafEMUah5+2VthXvOHW1L4HO+P7RbJiJO\nyz1MtMwZo/vnBuWGS04nYClMP4U3uGdH1NcZKdgZSbfM+P17Sp0/DG63YnCvvGyFMvQCYDx4NmvR\n5xzdOtTm3DJelrNl0Ufp5N071mKSkCVJRBjYowPGDSm8X1GiYIf16YzRLhm8UZFJHHMiKsNue9Xg\nBEFReIYBAujRsRb/edJw6fVI/fHwuUdyy9j7jrvrz/g3KPHNDyOSyM0t41E+sEY7sobimWP6+399\n6QHVaGSz0dwyVqmwn/K5aBm/uoVPUSMUMvgccfjl5Hzuhb5NhsNqCaiGc/5TL7cMSctjky3kLSnX\ngKqoTGSyM52IcjvjvwsSYsh9e7FEscI9B2ELZHbzuVuWu3ufkR0vcivmjIixbimF9LmHMRT9FLhs\npnIxpFK5F1juIc23nEKT9MVZTSgb++0VClYoh3rFFObFxUL5LNtjIYMsj6A4d2t7WLeJdV6xbfxq\nSML0tlHCMcXb0tRiDwB33vtcEpNq5e6x3fdyPPbZ7oFH/y/GcrePB8nfiLA+96ihtQUv5Ei1hCNQ\n6xHRX4hoPRHN99jflYieJaKPiWgBEV2qXsxwONdYDJvEZClV2SaXUzNAq/mMVklq9zjGAqWUae7l\nli9vrNAkFAmoJihD1brHKgY8/WRJwrqkonEhe7Xii72p1aHcvSx3xSrDM9zQ13J3JyNhGFhf3J6h\nkBJ916uIZxJTJtzC5GGir/wHxZPhlpkK4GSf/dcAWMjMowBMBPBbIqotXrToOJV7WOswp9wlG13m\nzc8szrdSPreMjDIVS4gvOpksQ4vctXq8WPMKKRxutyQBoewFiC8Vm9EqKax4jNNyd/YL2S/HsESx\n3L2ur0Bml2JB4bNSC417xbn7+NzDGABe0V+udfuIm4iJw5j5TQCb/IoA6ExGq3Yyy7aoES8aTuUe\n9i0pRovIYJUKOo0ll9fESU7iCOMLY3kwODcgySFCIYFgy71YxJbxu5tJcMtEsdJslnuBW6b4+mWI\n5nN3p3BemMKSljtVJolJ/sz+EMVnufuet0IGVO8DcCCANQA+AXA9M7vOEkFEVxDRLCKa1dAQ3+pD\nhco93PGh3TLW4IxPGSLRcpd7IGNxy4SZOEx0yzA7Ut5lfe5qh3XCWu5JUO4isk0q9o9At4z5r+ov\nPe/BUZ+Xqcd2h8vd/dggn3sxbhkPyQyfe2C1eRnCuGX8BlSTYLlLcBKAuQD2ATAawH1E5BpDyMxT\nmLmemet79w6/uo0szgHVsEoyP6AqV14cKPUuI6Tkk9zEYbG4ZWQeEDFaJpN3UYWbz93/EzvteLoB\nJG+Hn1vG2xcuV7csniGXvv3c/cKdMru6ZSyfu4dBIGMnhL0FYeeWCTWg6uu+kq8mKiqU+6UAprHB\nUgBfABiuoN7IOC33sE1uLZfX21zhJgjrbAf0zb/TnMt31VRnckvEjezfVeqzTFz5SJagZcP26RZc\n54h9jOvo0ak2p9Czjjl/g6Tff+9OAIzlxdzo3N5Yiedgx5JoQch+zlorGfXrGv4eyhLne8vPLVNA\nbvzCXaAeHb2HwIKWLQySzYmMmvR6Oe3bswMAoL6uu+v+g/YJ7iuhB1RdkpgKMqAFDhvkLlsQg3va\n66wUt8wKAJMAgIj6ADgAwOcK6pVi5s2TcNPJ9neJWzbYCzccjdvPOAinjwpe//H7JwzDE1eNwyEO\nxfO788fgoH1cPkrM84lrJP7q7ENyyVD/Pn4wOrWrxuBeHfHk1Ufhln8bIZXYcvnRg3N/P3n1Ubm/\nX7vxWM9j/v4fR+CB/3cYpl83oWDfo1ccicMGuScriUw+ZTieuOooDO/bxfYVY5M4wPS4fMJ++OeV\n42yZpHd+8+Dc3/267oWnrhmPO75xsNvheP66o/HctRPw4Y+/Ztsua/G8ddNxeOX7x+Ko/Xvh0Svy\nmat3fONgfHDzJAB25fxvo/bB89cdjZnmviCmXno43hfKWseeP3ag73FOxdatg7tyFS1E55eoxZNX\nH4UXbjg6cED15e8dg5e/V7h+6nPXTnDdnpfVfbuX2+SV7x+bexaszGYA+O+z7G1McDcOTh+1Dx77\nzjjXtUqfuGocfnjSAXjiqqNcjiyU9Q8XHuouvMtx1u09emgv3HPuaDx5dT47+b0fHY97zxuNdyYf\nn7sWaw3ZIwb3wMOXH4Hnrp3gus6uJdIJI/rgW/X29YdLYbn7r2IMgIgegREF04uIVgG4FUANADDz\nAwBuBzCViD6BcT03MfOG2CR20KdLe9SZb3wL52cWARjetwuG9+2Chu35ldN7dWqHDTsKV1Kvrsrg\nsEE9sLu51ba9e4caDOvTGQvWbHOcz6AqQ+jWoQZbdjajV6d2mHTg3nj+k69sL4kx+xpvfmZ73W50\nbp9/8K3jAGC/3p08jxnUsyMG9XS3POolrY6aqkzOQrGUTNhomUyGcHhdD9tX1CED7FmfflmgI9xe\noiHo1akdenUyvrzGCtm3g3p2wN7mF5GoQEfu0yXUOSceYF9H1Dr29FH98cgHK6U/9Pfr1RFzVmwp\n2C66swq/RA2sPpGbFdLjHD07tTPWxnWwb88Ovmu1elnY4n3r0r4a23Yb8RP7790p9+z17doeA7rv\nhVWbd7kbRB7nExfSFrGMkmDL2ZDNuVh9wbc8if8aey86chBOPMi+gHm/rnvhjNH5NZnb11ThkAHd\n8NTcNTiwX5dcBvfQvTvhjcX2cUTr/h0xuEfBvSxFcl2gcmfm8wP2rwFwojKJFODnKw8z77Ps/XdL\nqAmKLJHx3an+7C8maiPMxGH248Mf40durDfEIJh4XttkjTE8YKqqFPuHM+O64JwRz+1lQTvrdeI3\njmLdX7GIWxhk3LqtoHrHLczLmY+Wke0PbsX8Wsit3lKMRKUyQ9X54Is/w7wxvTIBC84nlhHq9+s0\nMiP/6qMfwiPG/NuUu/QEWMLgrILriaKQbW0itFYcM0GrajEZt0zunLmi4c4edC+9dvutZSoqTa96\nCOoXq3HWJttNiPLPr3JjyvGv87xxkwrl7rxRzkdBfDZCDXY7OyW5RwO4Td1NcLdicnJIWe6KH4AI\n1VkPctj53F3PH/4Q5XWIbVVlU/pqyLWrbKSVx3bRFdMaOJ5qumVC3pygqae9FLAt47bg2cvncuTP\nUwo71Y7znF4RMQQKXAHKC1EXuL1/rfpc594vwT1JhXJ3UqiAhWxBx+x0fsjefreOE8XK9atDBVE6\nVFUuiclZV2nO74UKZRyLW0ZRPaJyD1o4Oq9EQlruAdLKWO5OZNwyKIFbJgg3V53016jLNr9wynKt\n3ZBS5e69r5j77HWo3Y+bL201eNTPvSQsEZdbcs8x5W8UVC+pFwWxa8SxvqqqJmuxWe5yr7KwpyYq\n3ufuLJN3cXh/5cXbrd2tcL+8g9yuIuRytdyFc5SDdCp3j+k9gXCJOAXKjNwtRrdtUQYfnSQh/yef\noRptjhQRFRZMsXWIX3XxKHfzZSj5beGldGyWu6xyD3k5gQObXpZ7lfgMefnc4VnGp+rIOE9R6Jax\nI2aV55IQQ54zqFXya9tqy10ZfgOqYW6zdFnb+Sj3/2JT35NguYs+9yjLxYkk4WVlH/wWtityuqu6\nxGbB0R48oBrN5x7olvHY7x8M4OZzd9Ybv8ILM6BqIfu8hZ6rKlRpdaREucu9pQF7AzpjYQtqLeiU\n7s0kWmn5+FmxbDJ87sXIwLDLM0K4d6cd3C+3NKEfSXhZDRAyeOOQR5XSao3klgmpdIIGVCV87gVu\nGZdomcJQyPj6gZf/3DkOJ4oQdUDVr35DhsJzlZLAOPckMu+2E+E+NZmB80bb4tDNG33CiD4Yt19P\nvP6Z/ARmXo3kGS0jXbPX+cqvDHNJTI6JwyYesDfe+uFxqK3OoHuHWrRmGQfe8oJvXbXVxdkSYvZo\n2EWNZ948CTv2tGCIkAAWZX3VIMQa5912orRLxYmo0IMutcW08tuFvL9BVy/lc3f6ts1/fQdUBR66\nbCwuevCDAEmCKQhbpkI5vI4MO6AaQqh46pWkIpV7l/b+82F4x8rkO1r/bnuF9rl7uVqCPvUToKMj\nY49zt1/IwB4d3A7xJKzycRJlrh3x2D6ObXFMqSzer6B+CngbALYB1QDtbpVtVxN2xbEAt4zHfvtY\nhbuF7B/nnqeXS+asCqxzihnWIuKzGfU5FZvFddzNvNJyuSNT4pZx4JGNBtiX0FN1z10/ycQBJUXn\nKQdeGapRaFftnepeDsSHTtWKTape5GHcMtbEYmHvb6Bx43mc94GcK+N/3qjhm7LkY/+DxhXE2HzJ\nul3KxRWhVwypUO6BSUwuce5Rvpa9Oop9wJaEf1WlxpSPKuF+FdtHi3XLqCaOaBlVyPrZgfzga2i3\nTKDlLlOH/bebz73Yc4TB+XKpEr6kvM5r3epi3Cd+uS7lcssk62lThJ8/lmyWaPgBKNdQSJ/sNPGc\nlYj4pVOslZU0ZRrnfPnSqtmjr7ZEUu5qv4xklFLBgKr5r9/c6wQSFJ8avEIhvdwy+ePyD3Vot4xQ\nq1+ce7lIp3L32WDplyzH87kUR3hdOREX66jgd5Qr8UTLqKmnNRsw54BAc2s0n3sgEa7FzedeUC3J\nu02iIhupYnPLhKxbxO9RL9ci7elU7j5x7vkBr/A+d5nyYqdix7ZKxJJdhc89aYgThymLc1d0j8JY\n7k0R3TJBRHHL5LeXxy1jYd29qvynp+f5w84KKQv5nzp20qncfTNUjX+zEUxRoyOEb6lKVoqim6GS\n3UtuxOqWCbm4upMoPnfVYxpyxowzWsb4N8gDF9eAqnX+3HrFEvW7hW+GOZfzb4ty+dotUqHcC/x+\nBZa7GOeuboDQVRZhACfKiyBpkPAyTJdqj8kto6ieMJa71c1qFc9hHOVlnp9Pyd8toxpnnVZ+AQnG\nietxUWaFdC3o3V7l0gKpUO7D+nTO/X3KyL4Fyn2wsCbiKHM1oEnD7SvpAMZKLH4M7G6P6x5qrhN6\n4RH7+h7n1Wm6tPdOMzj7sPyyXCeMyEdoWyvVnONYtgsADvdYe/K0Q/r5yueHpTAuPGKQVEzyWYf2\nd90urrATlBnspG+X9gUuBxXvTXGFrCM8VgCyOH+s0cZDettXuerTpR061OYHMruay+aJq/cAwPC+\nneGGc9lHa9UisU9Z93T8/j1d67jALFvtUO59urTzXWlJ5Nz6wuUBg3TdEYN74NzDjeMGmauhiXO2\nnGPWaS0laK0EVp3J4Ftm/7bWd73A5xk67/BC2Qb7rHMKAJ3MZ8ta9tDZHpZs3TvW4AyzDZzPtxdj\nzBXEjjugUIeIso3sb7RlX2Ed368d6My2iA8ql3VZX1/Ps2bNUlbfnpZWZIhQRYSjfzUDq7fswrPf\nnYChfTqhvaOD72lpRbvqKjw880v8+Mn5AIDFPz8FNVVUYK3UTZ6e219bncE1f5+D6fPW4u5zR+H0\nUf2RZUYVUW7gccKdr2HV5l1464fH4adPz8frnzXgL5fU4/jhhY1qZRa2ZBk7m1px6O0vAwCW/OKU\nXJ3NrVnb3xkiVGUI2Syj2Rx0qyLKLYMnRqRYsn9+x6loZUZNRMuuqSWbuzdWnct/eVpBOVFW53ZL\nNvFvp5wiYv2tWQYz25TXn978HL94fhEA4N7zRhc8vLLsaTGWOwyKNMlmGa1mxJAoi5tsmxub0GWv\nGvuCG0I563oX/exktK/JYPCPngdgtHuGCK1ZRk0VoSXLaM0yaqsyyJht7taO2SyjJcsFbhk32Vpa\ns9j/x/8CkL/Hznaz5Fv4s5Mw4pYXbXUu+cUpqKnK5NoxQ8Du5ixqqzOoyhDO/eN7mPnFJvzp4np8\n7cC90dyal2tPSyv2tGTRpX0NmDm3z63fWDK8M/l49OvS3ravpTULcvShqx+ejec/+Qr3XTAGXz/E\nUNZiv2VmNLVmccBPjA7XGOkAAAyBSURBVCzqT247Ee2qq1BbnbHJ4jy/Wz+3rkXsM5OfmIdHP1yJ\n288ciQvG7ouqjHHOjY1NNqPIqw3DQESzmbk+qFxFZqi6Id5o64VVW50pUOxiWdEnFuSvdO6vzhid\nucprvhnblATuZayHrroKNjnFhvf6O5MhtMvIWWWZDCFThMNA1pfr1WG9rkEW4yGOxykkGz5ov4d5\nWdxk625aoyJu5faqtZ/bujeW0qqpIojd16sdMxlCrYvD2O2cTutePK8Tt35rlRWPEa/DcnFUZwyl\nWludr6NddVX+2RP2+fWJvWqqCowFt2twk1Xst0Rka+sMUW6/U04ZnP3Get5rMvmXDhEVfO0W+yyG\nIRVuGScyWXIy+6OQGyEHpyIUUtN2ifJ8WAPBqnIaKmWcp1zhjn6kU7nLjthHqjyoznytnN+oUUwS\nHyYNYIbc+y6inQSUZ8YmcB6pVCr3/Oi3/50upiGCjnWbKVKjqSSiPB9Z1ZZ7EeGJvvXG9FSWO/xR\nJJXKvZzJQ/YMVW1daiqXKIrKcstUV5X46Qt5urjmtEkS6VTukpMXRem80suniedJ0reaRiNJJMvd\nSh7ym1wmjAwJsoT94HJalB4EtgAR/YWI1hPRfJ8yE4loLhEtIKI31IoYBcmkhAgNETSxP+XKJfFd\nrtHIE0VP5QZUVRk0Yd0yas4amrDz05QCmdfrVAAne+0kom4A/gDgdGY+CMC31IgWHXnLXT1iRpyY\n0KFRi353JpNWVutzlyXs2eL6mE7SV3qgcmfmNwFs8ilyAYBpzLzCLL9ekWyRKcVz79WGV00cAsDI\nqgy7CEBb5awx/bFP1/a57M+gTOFKp7Yqg8smDC63GIGIiura4/eXOiar2OcemxJWbXIl0NhQkcQ0\nDEANEb0OoDOAe5n5b24FiegKAFcAwL77+qfsF0Nu2tEA68HqvGeNkc9uDLIYz6kfmEttjm1txpRx\n17mjyy1CSVn8i1PKLYIUYq+98cQDcOOJBwQeUy7LPSyxLRKittqiUDHqUQ3gMACnATgJwE+JaJhb\nQWaewsz1zFzfu3dvBad2R3aWt5x/PMI5wjSittw1lUi0UEjjX1Vx7mFrKddYF3PyvtJVWO6rAGxk\n5kYAjUT0JoBRABYrqDsSshZz3A2h/cKaSiaK/1h1hqosoVdVi02OmCqOgArL/WkAE4iomog6ADgC\nwCIF9UYmvxqMf7n8ZPohpleNYOcnqL01mlhpzc0toygUMq6VmlTPI6+0NjUEWu5E9AiAiQB6EdEq\nALcCqAEAZn6AmRcR0QsA5gHIAvgzM3uGTZYC2bR/y7KP5JaR6Bs6RT4+9J1NJqozVONCtXRJHF8L\nVO7MfL5EmV8D+LUSiVQgGQqZKx6TpkhiYoNGEyeqB1Rla0lOKGQ89UYhnRmq5r+BOUy5GRxD1B3h\nRZCkt7lGEyeqk5iSpCz9SOKXZDqVu8QK7G7lwxFcdxIbPC3owepkYrllFLncY0O5z11yssJSkvAm\niEY+FFIuzj1uEtTeGk2sqHfLhKunXC/9JNoaqVmJSUTW1y0T5z5+/562JfK+e/z+mLNiC47cz3/N\nTQD4z5MOwLV//wgj+3cNLBsHVx47BGu27FJa53XH749lDY1K62zrXHf8/liyfkfJzveDE4fhoxVb\nPPdfPmEwNu9sBgAc3L9rqIzhu84ZjbtfXoz2kitcqUK1AfX1Q/phYA+5NVUB4Kpjh2Dm55swYf9e\nagUpgnQq99wK7P7lSEK7P3z5kbbfhwzohlk/+ZqUHIfX9cD7N0+SKhsHk08ZrrzO70tkKWrCUep7\n+t3jh/ru/8nXR+T+fvbaCaHqPvXgfjj14OgLsjsp11fvfRccGqr8yP5dpfVCqUinWya3Koqkzz2R\nH1UajSYs+lnOk07lbv4bPP2AGeeu+4NGU9HoYa1C0qncJaNl8hmqcUukUY220NoGOhghOilV7nLl\ndL/RaDRpJZ3K3fxX2nLXVqBGk0gqJRQyiaRTuUtPv6l97pWKbjONxp90KnfzX+kM1fhE0Wg0RRDW\n56599HnSqdwl1y7VHUGjSRf6iy5PKpOY/nnlODz50epA5Z3LYdIdoqI5cUTfcougiQlZ++umU4Zj\nZ1MrTh6p+4JFKpX74XU9cHhd8PQAJJOiqkk0V00cgr1qS5vqrkke/bruhSkX15dbjESRSrdMWLTl\nrtEkkyTNslhptGnlru32ykc/+ulGt2902rZyj7CGqkaj0VQCWrlrNJrEop/R6LRt5V7EAtma8qK/\ntjQaf9q0coeeOEyjSTR6QDU6bVu5m2jdrtFo0kabVu415iq+tVVt+jZUJDVmm9Xotksl7ap1uxZL\nKpOYZDlqSE9cc9wQXDp+cLlFAQDcd8EYdGrXpptEmm8fVYdNjU248tgh5RZFEwPPXjsBby5uUF7v\nM98dj49XbVVebxKhoIEpIvoLgK8DWM/MI33KHQ7gPQDnMfPjQSeur6/nWbNmhRRXo9Fo2jZENJuZ\nA9NxZb59pgI4OeBkVQDuBPCSlHQajUajiZVA5c7MbwLYFFDsWgBPAFivQiiNRqPRFEfRoxZE1B/A\nNwDcL1H2CiKaRUSzGhrU+9M0Go1GY6BiSPoeADcxczaoIDNPYeZ6Zq7v3bu3glNrNBqNxg0VoRn1\nAB41kw16ATiViFqY+SkFdWs0Go0mAkUrd2bOxRES0VQAz2nFrtFoNOUlULkT0SMAJgLoRUSrANwK\noAYAmPmBWKXTaDQaTSQClTszny9bGTNfUpQ0Go1Go1FCYBJTbCcmagDwZcTDewHYoFCcSkBfc9tA\nX3PboJhrHsTMgREpZVPuxUBEs2QytNKEvua2gb7mtkEprlnPzqPRaDQpRCt3jUajSSGVqtynlFuA\nMqCvuW2gr7ltEPs1V6TPXaPRaDT+VKrlrtFoNBoftHLXaDSaFFJxyp2ITiaiz4hoKRFNLrc8qiCi\ngUQ0g4gWEtECIrre3N6DiF4moiXmv93N7UREvzPvwzwiOrS8VxANIqoioo+I6Dnz92Aimmle1z+I\nqNbc3s78vdTcX1dOuYuBiLoR0eNE9CkRLSKicWluZyL6ntmn5xPRI0TUPo3tTER/IaL1RDRf2Ba6\nXYno22b5JUT07ajyVJRyNxcF+R8ApwAYAeB8IhpRXqmU0QLgRmYeAeBIANeY1zYZwKvMPBTAq+Zv\nwLgHQ83/roDElMsJ5XoAi4TfdwK4m5n3B7AZwGXm9ssAbDa3322Wq1TuBfACMw8HMArG9aeync0p\nwa8DUG+u5FYF4Dyks52nonBho1DtSkQ9YEzxcgSAsQButV4IoWHmivkPwDgALwq/fwTgR+WWK6Zr\nfRrACQA+A9DP3NYPwGfm338EcL5QPleuUv4DMMDs8McDeA4Awcjaq3a2N4AXAYwz/642y1G5ryHC\nNXcF8IVT9rS2M4D+AFYC6GG223MATkprOwOoAzA/arsCOB/AH4XttnJh/qsoyx35jmKxytyWKsxP\n0TEAZgLow8xrzV1fAehj/p2Ge3EPgB8CsNYC6AlgCzO3mL/Fa8pdr7l/q1m+0hgMoAHA/5ruqD8T\nUUektJ2ZeTWA3wBYAWAtjHabjfS3s0XYdlXW3pWm3FMPEXWCsWThDcy8TdzHxqs8FbGrRGQtuj67\n3LKUmGoAhwK4n5nHAGhE/lMdQOrauTuAM2C81PYB0BEBazKnlVK3a6Up99UABgq/B5jbUgER1cBQ\n7A8z8zRz8zoi6mfu74f8OrWVfi/GAzidiJYDeBSGa+ZeAN2IyJqtVLym3PWa+7sC2FhKgRWxCsAq\nZp5p/n4chrJPazt/DcAXzNzAzM0ApsFo+7S3s0XYdlXW3pWm3D8EMNQcaa+FMTDzTJllUgIREYAH\nASxi5ruEXc8AsEbMvw3DF29tv9gcdT8SwFbh8y/xMPOPmHkAM9fBaMfXmPlCADMAnG0Wc16vdR/O\nNstXnHXLzF8BWElEB5ibJgFYiJS2Mwx3zJFE1MHs49b1prqdBcK264sATiSi7uZXz4nmtvCUewAi\nwoDFqQAWA1gG4MfllkfhdU2A8ck2D8Bc879TYfgbXwWwBMArAHqY5QlG5NAyAJ/AiEYo+3VEvPaJ\nMFbwAoD9AHwAYCmAfwJoZ25vb/5eau7fr9xyF3G9owHMMtv6KQDd09zOAP4LwKcA5gN4CEC7NLYz\ngEdgjCs0w/hCuyxKuwL4d/P6lwK4NKo8evoBjUajSSGV5pbRaDQajQRauWs0Gk0K0cpdo9FoUohW\n7hqNRpNCtHLXaDSaFKKVu0aj0aQQrdw1Go0mhfx/vQEIq2K7o0AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br5IX6u3XdPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "`"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}