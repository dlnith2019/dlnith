{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anshul.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwkDYZlKKd7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8P7R0wBK4G2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=[]\n",
        "Y1=[]\n",
        "for i in range(100):\n",
        "  X.append(i)\n",
        "  Y1.append(i*i-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0V0FVCBLyMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y2=[]\n",
        "for i in range(100):\n",
        "  Y2.append(i*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGoORbOENY7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "innnput = np.column_stack((Y1, Y2))\n",
        "test = [Y1-Y2 for Y1,Y2 in zip(Y1,Y2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP5mFr44NgR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "innnput = np.array(innnput, dtype=\"float32\")\n",
        "taargetttttt = np.array(test, dtype=\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNHZUjYoPX71",
        "colab_type": "code",
        "outputId": "a8b10c61-396b-4668-aaa4-519ead741bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "innnput= np.array(innnput).reshape(100, 2,1)\n",
        "innnput.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmrwEw6DkEWc",
        "colab_type": "code",
        "outputId": "b954cd1d-1553-481a-fb40-695c4b90477c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "taargetttttt.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medTU78GkGQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(innnput,taargetttttt,test_size=0.2,random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C8wYedGkNQP",
        "colab_type": "code",
        "outputId": "95a47fd8-c953-4fe1-d1b2-3eeeca1c5804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(2,1)))\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(25, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 2, 200)            161600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 2, 100)            120400    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 2, 50)             30200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 25)                7600      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                520       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 320,541\n",
            "Trainable params: 320,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhEyK56qkdZC",
        "colab_type": "code",
        "outputId": "9ad31bbe-e044-48be-92c0-4326ccfc3f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs=2000,validation_data=(x_test,y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/2000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "80/80 [==============================] - 4s 49ms/step - loss: 19513159.2000 - val_loss: 14406042.0000\n",
            "Epoch 2/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19493414.6000 - val_loss: 14400637.0000\n",
            "Epoch 3/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19498764.4000 - val_loss: 14394598.0000\n",
            "Epoch 4/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19477645.2000 - val_loss: 14384939.0000\n",
            "Epoch 5/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19462887.2000 - val_loss: 14379355.0000\n",
            "Epoch 6/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19458838.4000 - val_loss: 14377125.0000\n",
            "Epoch 7/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19442306.4000 - val_loss: 14326075.0000\n",
            "Epoch 8/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 19345337.2000 - val_loss: 14251224.0000\n",
            "Epoch 9/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19268114.0000 - val_loss: 14201408.0000\n",
            "Epoch 10/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 19187108.0000 - val_loss: 14112206.0000\n",
            "Epoch 11/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 19005556.8000 - val_loss: 13923438.0000\n",
            "Epoch 12/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 18696383.2000 - val_loss: 13414370.0000\n",
            "Epoch 13/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 17961281.8000 - val_loss: 12947137.0000\n",
            "Epoch 14/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 17376127.2000 - val_loss: 12626350.0000\n",
            "Epoch 15/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 15139088.8000 - val_loss: 9604801.0000\n",
            "Epoch 16/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 12737153.4000 - val_loss: 7847449.5000\n",
            "Epoch 17/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 10089775.4000 - val_loss: 6094021.0000\n",
            "Epoch 18/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 7485468.3000 - val_loss: 3733248.5000\n",
            "Epoch 19/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 4438251.7250 - val_loss: 1967239.6250\n",
            "Epoch 20/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 1999434.2250 - val_loss: 352253.5625\n",
            "Epoch 21/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 205297.4266 - val_loss: 348909.6250\n",
            "Epoch 22/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 684566.5062 - val_loss: 732649.5000\n",
            "Epoch 23/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 895747.8750 - val_loss: 301059.5000\n",
            "Epoch 24/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 290243.1734 - val_loss: 21075.0957\n",
            "Epoch 25/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 15106.9137 - val_loss: 25610.4883\n",
            "Epoch 26/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 60792.0203 - val_loss: 91899.6641\n",
            "Epoch 27/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 123415.1781 - val_loss: 91589.0938\n",
            "Epoch 28/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 103059.5023 - val_loss: 41367.5273\n",
            "Epoch 29/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 37162.0440 - val_loss: 4296.7505\n",
            "Epoch 30/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 2463.9743 - val_loss: 6824.3516\n",
            "Epoch 31/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 6199.3699 - val_loss: 3532.8469\n",
            "Epoch 32/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 5586.3553 - val_loss: 5747.8579\n",
            "Epoch 33/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 6705.5522 - val_loss: 2273.0513\n",
            "Epoch 34/2000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 2109.5249 - val_loss: 1652.1299\n",
            "Epoch 35/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1151.9220 - val_loss: 1849.9818\n",
            "Epoch 36/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 1409.6468 - val_loss: 2445.4060\n",
            "Epoch 37/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 1760.5040 - val_loss: 2275.5774\n",
            "Epoch 38/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1497.5302 - val_loss: 1878.3617\n",
            "Epoch 39/2000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 1194.8537 - val_loss: 1590.8556\n",
            "Epoch 40/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 38029.7904 - val_loss: 26574.9434\n",
            "Epoch 41/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 25046.7362 - val_loss: 5195.5415\n",
            "Epoch 42/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 2847.4117 - val_loss: 2942.9817\n",
            "Epoch 43/2000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 6356.3133 - val_loss: 9519.2383\n",
            "Epoch 44/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 13220.2195 - val_loss: 7302.9204\n",
            "Epoch 45/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 8226.3535 - val_loss: 2277.7788\n",
            "Epoch 46/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 1846.2521 - val_loss: 1574.8757\n",
            "Epoch 47/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 1192.5549 - val_loss: 2191.9355\n",
            "Epoch 48/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 2634.2107 - val_loss: 4390.8350\n",
            "Epoch 49/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 3753.6866 - val_loss: 2732.9182\n",
            "Epoch 50/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 1945.7614 - val_loss: 1147.5009\n",
            "Epoch 51/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 786.0270 - val_loss: 1650.8398\n",
            "Epoch 52/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 1078.0277 - val_loss: 1511.2054\n",
            "Epoch 53/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 1239.9992 - val_loss: 2311.2507\n",
            "Epoch 54/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 1484.8947 - val_loss: 1796.9476\n",
            "Epoch 55/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 1146.7875 - val_loss: 1582.8975\n",
            "Epoch 56/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 1134.6897 - val_loss: 1564.3143\n",
            "Epoch 57/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1205.6608 - val_loss: 1628.8794\n",
            "Epoch 58/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 1246.7023 - val_loss: 1602.5509\n",
            "Epoch 59/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 1086.2603 - val_loss: 1535.0902\n",
            "Epoch 60/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1801.1529 - val_loss: 6836.9507\n",
            "Epoch 61/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 5603.8377 - val_loss: 2832.6321\n",
            "Epoch 62/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 1518.0019 - val_loss: 1836.5938\n",
            "Epoch 63/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 1145.0338 - val_loss: 1525.1318\n",
            "Epoch 64/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 1183.2885 - val_loss: 1316.5089\n",
            "Epoch 65/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 1092.6958 - val_loss: 1490.0505\n",
            "Epoch 66/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 1191.4459 - val_loss: 1406.4332\n",
            "Epoch 67/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1143.8258 - val_loss: 3967.7090\n",
            "Epoch 68/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3712.6736 - val_loss: 1927.7490\n",
            "Epoch 69/2000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 1215.0839 - val_loss: 1586.9421\n",
            "Epoch 70/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 1560.1323 - val_loss: 1532.9315\n",
            "Epoch 71/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 1215.4834 - val_loss: 1780.7347\n",
            "Epoch 72/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 1181.9145 - val_loss: 1654.6917\n",
            "Epoch 73/2000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 1347.3844 - val_loss: 1575.8688\n",
            "Epoch 74/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1009.0141 - val_loss: 1361.3428\n",
            "Epoch 75/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 1303.1727 - val_loss: 1481.8059\n",
            "Epoch 76/2000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 1567.6044 - val_loss: 1557.7981\n",
            "Epoch 77/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 1308.4212 - val_loss: 1474.5308\n",
            "Epoch 78/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1010.9427 - val_loss: 1551.6740\n",
            "Epoch 79/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 9871.6327 - val_loss: 1469.2454\n",
            "Epoch 80/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1579.9004 - val_loss: 1843.4547\n",
            "Epoch 81/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 1706.3731 - val_loss: 1717.4648\n",
            "Epoch 82/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 1686.0885 - val_loss: 2928.2744\n",
            "Epoch 83/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 4041.0167 - val_loss: 5121.7407\n",
            "Epoch 84/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3489.2187 - val_loss: 1609.9320\n",
            "Epoch 85/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 1121.9725 - val_loss: 1536.6454\n",
            "Epoch 86/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 1101.7247 - val_loss: 2361.5996\n",
            "Epoch 87/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 2809.7445 - val_loss: 1934.1667\n",
            "Epoch 88/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 1721.5821 - val_loss: 1496.0876\n",
            "Epoch 89/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1580.4857 - val_loss: 4438.0674\n",
            "Epoch 90/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 3539.8178 - val_loss: 12606.1670\n",
            "Epoch 91/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 11063.4581 - val_loss: 2696.3479\n",
            "Epoch 92/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1653.0469 - val_loss: 2598.8083\n",
            "Epoch 93/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 4274.5831 - val_loss: 4440.5884\n",
            "Epoch 94/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 5432.7436 - val_loss: 2359.8098\n",
            "Epoch 95/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 1961.4009 - val_loss: 1507.9755\n",
            "Epoch 96/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 2094.5355 - val_loss: 9617.3125\n",
            "Epoch 97/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 10549.3313 - val_loss: 5472.6826\n",
            "Epoch 98/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 3360.3687 - val_loss: 1428.8840\n",
            "Epoch 99/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 5041.1805 - val_loss: 4559.5693\n",
            "Epoch 100/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 2833.4284 - val_loss: 2897.8237\n",
            "Epoch 101/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4778.3679 - val_loss: 3847.7273\n",
            "Epoch 102/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 4473.8168 - val_loss: 1457.8240\n",
            "Epoch 103/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 1152.0121 - val_loss: 2248.0588\n",
            "Epoch 104/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 1796.0848 - val_loss: 6410.8643\n",
            "Epoch 105/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 4968.9989 - val_loss: 2953.6021\n",
            "Epoch 106/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 1827.2642 - val_loss: 1677.8528\n",
            "Epoch 107/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 2186.1349 - val_loss: 2523.5808\n",
            "Epoch 108/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 2778.4234 - val_loss: 1463.4675\n",
            "Epoch 109/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 1030.5039 - val_loss: 2016.3636\n",
            "Epoch 110/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1533.4706 - val_loss: 2149.0908\n",
            "Epoch 111/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 6001.0853 - val_loss: 7587.5977\n",
            "Epoch 112/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 4491.5683 - val_loss: 1348.4973\n",
            "Epoch 113/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 1506.9425 - val_loss: 2148.4709\n",
            "Epoch 114/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 2861.6629 - val_loss: 2296.7183\n",
            "Epoch 115/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 2270.5530 - val_loss: 1439.2767\n",
            "Epoch 116/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 1112.7039 - val_loss: 1915.4144\n",
            "Epoch 117/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 1470.7742 - val_loss: 2145.3472\n",
            "Epoch 118/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 1404.2746 - val_loss: 1561.8652\n",
            "Epoch 119/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 1025.2556 - val_loss: 1385.2332\n",
            "Epoch 120/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 1238.3739 - val_loss: 1483.0034\n",
            "Epoch 121/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 1190.9673 - val_loss: 1450.7505\n",
            "Epoch 122/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 1023.1006 - val_loss: 1647.9512\n",
            "Epoch 123/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 1133.6325 - val_loss: 1679.4889\n",
            "Epoch 124/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 1148.5299 - val_loss: 1591.3582\n",
            "Epoch 125/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1020.0360 - val_loss: 1442.5852\n",
            "Epoch 126/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 1107.8039 - val_loss: 1447.1416\n",
            "Epoch 127/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 1133.1094 - val_loss: 1447.3734\n",
            "Epoch 128/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 1026.3557 - val_loss: 1520.8406\n",
            "Epoch 129/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 1048.8664 - val_loss: 1565.0328\n",
            "Epoch 130/2000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 1040.4613 - val_loss: 1486.6140\n",
            "Epoch 131/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 1021.4722 - val_loss: 1444.4681\n",
            "Epoch 132/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 1038.7702 - val_loss: 1442.1029\n",
            "Epoch 133/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 1032.3511 - val_loss: 1473.9193\n",
            "Epoch 134/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 1017.9548 - val_loss: 1502.0630\n",
            "Epoch 135/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 1020.1667 - val_loss: 1904.5826\n",
            "Epoch 136/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 1232.7067 - val_loss: 1440.8622\n",
            "Epoch 137/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 1125.4775 - val_loss: 1436.5262\n",
            "Epoch 138/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 1106.2094 - val_loss: 1439.5443\n",
            "Epoch 139/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 1030.8551 - val_loss: 1702.5504\n",
            "Epoch 140/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1150.1521 - val_loss: 1538.0476\n",
            "Epoch 141/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1015.4072 - val_loss: 1429.4023\n",
            "Epoch 142/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1043.7930 - val_loss: 1408.9436\n",
            "Epoch 143/2000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 1070.0087 - val_loss: 1399.1298\n",
            "Epoch 144/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 1014.5721 - val_loss: 1451.7106\n",
            "Epoch 145/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 1009.9738 - val_loss: 1512.3718\n",
            "Epoch 146/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 1017.1801 - val_loss: 1478.9133\n",
            "Epoch 147/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 993.4040 - val_loss: 1422.1138\n",
            "Epoch 148/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 1030.2582 - val_loss: 1413.1028\n",
            "Epoch 149/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 1042.9044 - val_loss: 1437.6619\n",
            "Epoch 150/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 1035.1015 - val_loss: 1492.1494\n",
            "Epoch 151/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 1015.0705 - val_loss: 1481.5924\n",
            "Epoch 152/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1012.4335 - val_loss: 1438.7554\n",
            "Epoch 153/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 1002.9388 - val_loss: 1427.9125\n",
            "Epoch 154/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 1000.7086 - val_loss: 1424.9241\n",
            "Epoch 155/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 983.1567 - val_loss: 1420.2123\n",
            "Epoch 156/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 1023.0972 - val_loss: 1401.6104\n",
            "Epoch 157/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1032.5785 - val_loss: 1415.3289\n",
            "Epoch 158/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 1029.6676 - val_loss: 1459.7450\n",
            "Epoch 159/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 1008.0186 - val_loss: 1460.8088\n",
            "Epoch 160/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 1024.6474 - val_loss: 1479.3400\n",
            "Epoch 161/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1010.3467 - val_loss: 1540.8441\n",
            "Epoch 162/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1035.9937 - val_loss: 1469.8558\n",
            "Epoch 163/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1002.3467 - val_loss: 1433.1188\n",
            "Epoch 164/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 1014.5769 - val_loss: 1415.1028\n",
            "Epoch 165/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 1048.0345 - val_loss: 1415.7205\n",
            "Epoch 166/2000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 1012.6031 - val_loss: 1416.6111\n",
            "Epoch 167/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 1008.0701 - val_loss: 1426.7614\n",
            "Epoch 168/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 1013.2555 - val_loss: 1464.9934\n",
            "Epoch 169/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 1004.4921 - val_loss: 1447.2924\n",
            "Epoch 170/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 997.9577 - val_loss: 1436.9424\n",
            "Epoch 171/2000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 996.6714 - val_loss: 1409.3528\n",
            "Epoch 172/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 998.9666 - val_loss: 1409.6648\n",
            "Epoch 173/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 993.9708 - val_loss: 1442.7068\n",
            "Epoch 174/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 993.0931 - val_loss: 1430.8063\n",
            "Epoch 175/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 981.8649 - val_loss: 1389.0638\n",
            "Epoch 176/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 1007.9533 - val_loss: 1366.4036\n",
            "Epoch 177/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 973.9022 - val_loss: 1410.9036\n",
            "Epoch 178/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1005.0683 - val_loss: 1425.6296\n",
            "Epoch 179/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 941.7638 - val_loss: 1219.3871\n",
            "Epoch 180/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 782.1826 - val_loss: 6249.8228\n",
            "Epoch 181/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 3519.4326 - val_loss: 1728.2942\n",
            "Epoch 182/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1727.1197 - val_loss: 1440.9631\n",
            "Epoch 183/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 1113.3301 - val_loss: 1734.8281\n",
            "Epoch 184/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 1293.6794 - val_loss: 1854.3607\n",
            "Epoch 185/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 1163.7468 - val_loss: 1445.2145\n",
            "Epoch 186/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 1043.6713 - val_loss: 1433.3932\n",
            "Epoch 187/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 1167.7303 - val_loss: 1410.0400\n",
            "Epoch 188/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 1097.3276 - val_loss: 1533.3438\n",
            "Epoch 189/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 1030.2163 - val_loss: 1514.8077\n",
            "Epoch 190/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 1014.4690 - val_loss: 1453.3300\n",
            "Epoch 191/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 1020.6655 - val_loss: 1412.4446\n",
            "Epoch 192/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 1023.4412 - val_loss: 1432.9091\n",
            "Epoch 193/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 1006.6509 - val_loss: 1504.2697\n",
            "Epoch 194/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 1018.1847 - val_loss: 1490.6399\n",
            "Epoch 195/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1008.6998 - val_loss: 1451.4784\n",
            "Epoch 196/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 992.6962 - val_loss: 1405.6204\n",
            "Epoch 197/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 1031.0843 - val_loss: 1405.2366\n",
            "Epoch 198/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1040.7211 - val_loss: 1475.9857\n",
            "Epoch 199/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 1003.4819 - val_loss: 1466.3372\n",
            "Epoch 200/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 999.5818 - val_loss: 1434.6975\n",
            "Epoch 201/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 996.5123 - val_loss: 1427.3749\n",
            "Epoch 202/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 994.2519 - val_loss: 1449.5547\n",
            "Epoch 203/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 1005.7550 - val_loss: 1466.9736\n",
            "Epoch 204/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 999.9912 - val_loss: 1419.3348\n",
            "Epoch 205/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 993.9536 - val_loss: 1418.7461\n",
            "Epoch 206/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 992.4265 - val_loss: 1414.0841\n",
            "Epoch 207/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 990.4509 - val_loss: 1408.7180\n",
            "Epoch 208/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 989.2428 - val_loss: 1418.1610\n",
            "Epoch 209/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 987.6251 - val_loss: 1438.5271\n",
            "Epoch 210/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 984.2021 - val_loss: 1437.9631\n",
            "Epoch 211/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 995.6083 - val_loss: 1420.0974\n",
            "Epoch 212/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 972.5429 - val_loss: 1379.2623\n",
            "Epoch 213/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 874.7343 - val_loss: 9501.5342\n",
            "Epoch 214/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 7693.4079 - val_loss: 1308.6339\n",
            "Epoch 215/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1185.4680 - val_loss: 2295.6375\n",
            "Epoch 216/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 3568.9744 - val_loss: 2247.2830\n",
            "Epoch 217/2000\n",
            "80/80 [==============================] - 0s 842us/step - loss: 1884.6240 - val_loss: 2247.9529\n",
            "Epoch 218/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 3483.9167 - val_loss: 9174.4092\n",
            "Epoch 219/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 6115.6458 - val_loss: 5731.8569\n",
            "Epoch 220/2000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 8026.4313 - val_loss: 2435.8840\n",
            "Epoch 221/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 2074.8863 - val_loss: 3291.9348\n",
            "Epoch 222/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 3617.1034 - val_loss: 3197.8247\n",
            "Epoch 223/2000\n",
            "80/80 [==============================] - 0s 870us/step - loss: 2161.6966 - val_loss: 1664.2920\n",
            "Epoch 224/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 2247.6374 - val_loss: 1901.8945\n",
            "Epoch 225/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 1604.4680 - val_loss: 1699.1873\n",
            "Epoch 226/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 1538.4354 - val_loss: 2501.7549\n",
            "Epoch 227/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 1656.3454 - val_loss: 1932.1777\n",
            "Epoch 228/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 1003.5898 - val_loss: 1457.8158\n",
            "Epoch 229/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 1557.8500 - val_loss: 1538.3127\n",
            "Epoch 230/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 1255.1919 - val_loss: 1533.6830\n",
            "Epoch 231/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 1307.1161 - val_loss: 1905.1970\n",
            "Epoch 232/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 1273.4713 - val_loss: 1395.3060\n",
            "Epoch 233/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1011.0859 - val_loss: 1330.8823\n",
            "Epoch 234/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 1059.7370 - val_loss: 1306.3037\n",
            "Epoch 235/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 935.8099 - val_loss: 1363.0928\n",
            "Epoch 236/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 967.0628 - val_loss: 1338.9000\n",
            "Epoch 237/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 905.8839 - val_loss: 1083.1997\n",
            "Epoch 238/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 782.8038 - val_loss: 374.6569\n",
            "Epoch 239/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 881.7790 - val_loss: 1505.1150\n",
            "Epoch 240/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 1041.5495 - val_loss: 1382.1320\n",
            "Epoch 241/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 1059.5313 - val_loss: 1463.0610\n",
            "Epoch 242/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 1065.0493 - val_loss: 1606.9388\n",
            "Epoch 243/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 1039.5828 - val_loss: 1410.8007\n",
            "Epoch 244/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 976.9977 - val_loss: 1380.5681\n",
            "Epoch 245/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 1041.4178 - val_loss: 1390.8406\n",
            "Epoch 246/2000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 976.3525 - val_loss: 1510.1790\n",
            "Epoch 247/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1031.5900 - val_loss: 1514.8345\n",
            "Epoch 248/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 1011.9891 - val_loss: 1406.9680\n",
            "Epoch 249/2000\n",
            "80/80 [==============================] - 0s 869us/step - loss: 995.5874 - val_loss: 1389.7844\n",
            "Epoch 250/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1002.4729 - val_loss: 1446.6753\n",
            "Epoch 251/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 984.4499 - val_loss: 1451.2213\n",
            "Epoch 252/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 1013.7096 - val_loss: 1424.0164\n",
            "Epoch 253/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 951.3328 - val_loss: 1360.4573\n",
            "Epoch 254/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1035.2617 - val_loss: 1344.8087\n",
            "Epoch 255/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 973.5345 - val_loss: 1329.0128\n",
            "Epoch 256/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 1033.2234 - val_loss: 1410.3833\n",
            "Epoch 257/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 973.3500 - val_loss: 1405.3298\n",
            "Epoch 258/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 981.9574 - val_loss: 1400.4746\n",
            "Epoch 259/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 992.7892 - val_loss: 1373.9414\n",
            "Epoch 260/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 991.3116 - val_loss: 1414.4678\n",
            "Epoch 261/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 974.7729 - val_loss: 1430.4734\n",
            "Epoch 262/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 979.8872 - val_loss: 1433.9954\n",
            "Epoch 263/2000\n",
            "80/80 [==============================] - 0s 870us/step - loss: 971.2146 - val_loss: 1390.3044\n",
            "Epoch 264/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 980.0034 - val_loss: 1374.7559\n",
            "Epoch 265/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 976.5320 - val_loss: 1417.9734\n",
            "Epoch 266/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 970.9159 - val_loss: 1438.7493\n",
            "Epoch 267/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 977.6710 - val_loss: 1407.0443\n",
            "Epoch 268/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 981.1623 - val_loss: 1374.7670\n",
            "Epoch 269/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 979.5591 - val_loss: 1406.9745\n",
            "Epoch 270/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 969.4028 - val_loss: 1400.0670\n",
            "Epoch 271/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 968.8652 - val_loss: 1416.9011\n",
            "Epoch 272/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 973.6503 - val_loss: 1397.6722\n",
            "Epoch 273/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1007.5733 - val_loss: 1397.6174\n",
            "Epoch 274/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 973.5701 - val_loss: 1347.7699\n",
            "Epoch 275/2000\n",
            "80/80 [==============================] - 0s 862us/step - loss: 1019.0006 - val_loss: 1366.7365\n",
            "Epoch 276/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 991.4127 - val_loss: 1457.8118\n",
            "Epoch 277/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 978.6709 - val_loss: 1376.7229\n",
            "Epoch 278/2000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 962.0501 - val_loss: 1355.1830\n",
            "Epoch 279/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 960.9431 - val_loss: 1351.3303\n",
            "Epoch 280/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 959.8762 - val_loss: 1370.1003\n",
            "Epoch 281/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 950.7572 - val_loss: 1370.6946\n",
            "Epoch 282/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 952.7426 - val_loss: 1352.2634\n",
            "Epoch 283/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 965.4115 - val_loss: 1369.7102\n",
            "Epoch 284/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 990.0268 - val_loss: 1394.0726\n",
            "Epoch 285/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 965.1346 - val_loss: 1243.8436\n",
            "Epoch 286/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 892.1829 - val_loss: 979.5631\n",
            "Epoch 287/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 7506.1210 - val_loss: 2303.5149\n",
            "Epoch 288/2000\n",
            "80/80 [==============================] - 0s 880us/step - loss: 3083.9582 - val_loss: 1730.0514\n",
            "Epoch 289/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 1455.9260 - val_loss: 2020.2806\n",
            "Epoch 290/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 1647.0023 - val_loss: 2063.1492\n",
            "Epoch 291/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 1331.3206 - val_loss: 1356.9993\n",
            "Epoch 292/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 1233.0944 - val_loss: 1418.8375\n",
            "Epoch 293/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 1138.9722 - val_loss: 1420.6594\n",
            "Epoch 294/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 1068.0709 - val_loss: 1633.9797\n",
            "Epoch 295/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 1031.6270 - val_loss: 1355.6514\n",
            "Epoch 296/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 996.2840 - val_loss: 1357.2404\n",
            "Epoch 297/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 1082.7630 - val_loss: 1358.9828\n",
            "Epoch 298/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 931.3475 - val_loss: 1547.2526\n",
            "Epoch 299/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1076.1703 - val_loss: 1485.6879\n",
            "Epoch 300/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 975.1608 - val_loss: 1323.8174\n",
            "Epoch 301/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 1028.3642 - val_loss: 1310.7966\n",
            "Epoch 302/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 941.5175 - val_loss: 1393.1919\n",
            "Epoch 303/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 969.0799 - val_loss: 1431.4060\n",
            "Epoch 304/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 975.7989 - val_loss: 1346.6870\n",
            "Epoch 305/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 973.6176 - val_loss: 1354.9203\n",
            "Epoch 306/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 954.0793 - val_loss: 1403.7800\n",
            "Epoch 307/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 959.5357 - val_loss: 1420.7843\n",
            "Epoch 308/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 963.7569 - val_loss: 1342.9500\n",
            "Epoch 309/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 949.5680 - val_loss: 1337.4934\n",
            "Epoch 310/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 942.2582 - val_loss: 1333.8470\n",
            "Epoch 311/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 940.6367 - val_loss: 1339.6816\n",
            "Epoch 312/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 934.1719 - val_loss: 1338.0658\n",
            "Epoch 313/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 933.9512 - val_loss: 1351.4758\n",
            "Epoch 314/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 934.5293 - val_loss: 1381.6832\n",
            "Epoch 315/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 949.3918 - val_loss: 1333.9303\n",
            "Epoch 316/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 940.8948 - val_loss: 1346.5524\n",
            "Epoch 317/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 957.4409 - val_loss: 1286.5349\n",
            "Epoch 318/2000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 1086.7863 - val_loss: 1431.4321\n",
            "Epoch 319/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 958.9660 - val_loss: 1315.1350\n",
            "Epoch 320/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 996.7320 - val_loss: 1322.9707\n",
            "Epoch 321/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 979.0246 - val_loss: 1431.3403\n",
            "Epoch 322/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 956.8661 - val_loss: 1341.1134\n",
            "Epoch 323/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 934.7222 - val_loss: 1305.5209\n",
            "Epoch 324/2000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 941.1776 - val_loss: 1370.5648\n",
            "Epoch 325/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 969.0262 - val_loss: 1412.8180\n",
            "Epoch 326/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 956.0513 - val_loss: 1345.1301\n",
            "Epoch 327/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 967.9711 - val_loss: 1324.8955\n",
            "Epoch 328/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 1001.2262 - val_loss: 1371.2897\n",
            "Epoch 329/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 944.6395 - val_loss: 1397.6443\n",
            "Epoch 330/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 949.6209 - val_loss: 1368.6669\n",
            "Epoch 331/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 957.0595 - val_loss: 1342.5072\n",
            "Epoch 332/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 981.1903 - val_loss: 1374.4929\n",
            "Epoch 333/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 961.9363 - val_loss: 1341.7656\n",
            "Epoch 334/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 966.4635 - val_loss: 1392.0691\n",
            "Epoch 335/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 944.0476 - val_loss: 1343.2794\n",
            "Epoch 336/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 946.8719 - val_loss: 1323.4884\n",
            "Epoch 337/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 953.1105 - val_loss: 1369.5474\n",
            "Epoch 338/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 942.2351 - val_loss: 1387.7152\n",
            "Epoch 339/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 1003.5105 - val_loss: 1366.5977\n",
            "Epoch 340/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 928.1950 - val_loss: 1314.5621\n",
            "Epoch 341/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 1038.5857 - val_loss: 1332.1245\n",
            "Epoch 342/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 904.7200 - val_loss: 1583.0862\n",
            "Epoch 343/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1089.3900 - val_loss: 1424.9813\n",
            "Epoch 344/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 1012.1069 - val_loss: 1330.3976\n",
            "Epoch 345/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1088.3073 - val_loss: 1320.0245\n",
            "Epoch 346/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 940.4701 - val_loss: 1447.7812\n",
            "Epoch 347/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 985.2783 - val_loss: 1368.5809\n",
            "Epoch 348/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 937.3284 - val_loss: 1353.2561\n",
            "Epoch 349/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 934.6488 - val_loss: 1347.5332\n",
            "Epoch 350/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 936.1371 - val_loss: 1331.8362\n",
            "Epoch 351/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 931.3729 - val_loss: 1323.2334\n",
            "Epoch 352/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 929.2626 - val_loss: 1358.9768\n",
            "Epoch 353/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 941.4146 - val_loss: 1351.3309\n",
            "Epoch 354/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 925.7611 - val_loss: 1296.5625\n",
            "Epoch 355/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 948.4925 - val_loss: 1312.1504\n",
            "Epoch 356/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 937.2828 - val_loss: 1355.5336\n",
            "Epoch 357/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 941.5214 - val_loss: 1332.6937\n",
            "Epoch 358/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 919.2265 - val_loss: 1286.4314\n",
            "Epoch 359/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 956.6585 - val_loss: 1313.5159\n",
            "Epoch 360/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 933.2879 - val_loss: 1375.7507\n",
            "Epoch 361/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 951.2579 - val_loss: 1358.1838\n",
            "Epoch 362/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 900.9844 - val_loss: 1277.0651\n",
            "Epoch 363/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 964.2383 - val_loss: 1277.9814\n",
            "Epoch 364/2000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 916.9816 - val_loss: 1377.2830\n",
            "Epoch 365/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 951.7137 - val_loss: 1357.0820\n",
            "Epoch 366/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 965.6137 - val_loss: 1262.6765\n",
            "Epoch 367/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 931.4217 - val_loss: 1309.4152\n",
            "Epoch 368/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 933.3078 - val_loss: 1354.8167\n",
            "Epoch 369/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 914.7673 - val_loss: 1258.2566\n",
            "Epoch 370/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 919.2253 - val_loss: 1245.1584\n",
            "Epoch 371/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 885.8048 - val_loss: 1331.3239\n",
            "Epoch 372/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 927.0844 - val_loss: 1336.1619\n",
            "Epoch 373/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 885.3772 - val_loss: 1177.4441\n",
            "Epoch 374/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 875.3976 - val_loss: 908.8719\n",
            "Epoch 375/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 654.3247 - val_loss: 1171.2122\n",
            "Epoch 376/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 868.5902 - val_loss: 1270.2880\n",
            "Epoch 377/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 946.6305 - val_loss: 1270.8510\n",
            "Epoch 378/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 902.5479 - val_loss: 1384.8845\n",
            "Epoch 379/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 935.2072 - val_loss: 1327.7168\n",
            "Epoch 380/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 913.0713 - val_loss: 1270.9313\n",
            "Epoch 381/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 927.2948 - val_loss: 1301.7048\n",
            "Epoch 382/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 912.4385 - val_loss: 1370.0305\n",
            "Epoch 383/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 920.4072 - val_loss: 1290.9008\n",
            "Epoch 384/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 909.9408 - val_loss: 1263.5378\n",
            "Epoch 385/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 919.2621 - val_loss: 1316.8783\n",
            "Epoch 386/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 903.1785 - val_loss: 1355.3943\n",
            "Epoch 387/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 923.3596 - val_loss: 1318.2415\n",
            "Epoch 388/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 884.7286 - val_loss: 1253.5811\n",
            "Epoch 389/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 952.3654 - val_loss: 1251.9521\n",
            "Epoch 390/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 946.6029 - val_loss: 1326.3650\n",
            "Epoch 391/2000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 905.7563 - val_loss: 1297.9340\n",
            "Epoch 392/2000\n",
            "80/80 [==============================] - 0s 870us/step - loss: 896.5656 - val_loss: 1277.6073\n",
            "Epoch 393/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 895.8234 - val_loss: 1257.1637\n",
            "Epoch 394/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 895.1037 - val_loss: 1294.4058\n",
            "Epoch 395/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 916.7781 - val_loss: 1306.3416\n",
            "Epoch 396/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 890.8362 - val_loss: 1262.8638\n",
            "Epoch 397/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 898.7430 - val_loss: 1240.9526\n",
            "Epoch 398/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 893.1925 - val_loss: 1291.7883\n",
            "Epoch 399/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 891.6321 - val_loss: 1279.9065\n",
            "Epoch 400/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 887.6401 - val_loss: 1248.6031\n",
            "Epoch 401/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 883.4511 - val_loss: 1237.2516\n",
            "Epoch 402/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 879.5966 - val_loss: 1221.5505\n",
            "Epoch 403/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 870.6298 - val_loss: 1269.1881\n",
            "Epoch 404/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 910.0727 - val_loss: 1278.4594\n",
            "Epoch 405/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 874.9408 - val_loss: 1181.6172\n",
            "Epoch 406/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 890.2427 - val_loss: 1214.1868\n",
            "Epoch 407/2000\n",
            "80/80 [==============================] - 0s 872us/step - loss: 901.5786 - val_loss: 1295.7922\n",
            "Epoch 408/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 865.8589 - val_loss: 1151.2153\n",
            "Epoch 409/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 864.2790 - val_loss: 1133.2573\n",
            "Epoch 410/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 804.1642 - val_loss: 1218.2626\n",
            "Epoch 411/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 898.1542 - val_loss: 1074.4790\n",
            "Epoch 412/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 758.3645 - val_loss: 841.4570\n",
            "Epoch 413/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 589.9125 - val_loss: 17897.8945\n",
            "Epoch 414/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 7958.5454 - val_loss: 2534.3662\n",
            "Epoch 415/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 2405.1044 - val_loss: 1508.6733\n",
            "Epoch 416/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 1518.5726 - val_loss: 2187.8030\n",
            "Epoch 417/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 1318.2191 - val_loss: 1349.6776\n",
            "Epoch 418/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 1433.9073 - val_loss: 1282.8295\n",
            "Epoch 419/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 917.4504 - val_loss: 1826.4844\n",
            "Epoch 420/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 1372.2509 - val_loss: 1400.6656\n",
            "Epoch 421/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 924.0198 - val_loss: 1385.5132\n",
            "Epoch 422/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 1252.4514 - val_loss: 1271.8999\n",
            "Epoch 423/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 904.9223 - val_loss: 1707.6497\n",
            "Epoch 424/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1167.3300 - val_loss: 1297.8424\n",
            "Epoch 425/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 969.3819 - val_loss: 1288.3387\n",
            "Epoch 426/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 1033.5592 - val_loss: 1324.9930\n",
            "Epoch 427/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 1042.8491 - val_loss: 1459.6659\n",
            "Epoch 428/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 995.5916 - val_loss: 1257.5443\n",
            "Epoch 429/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 1069.6590 - val_loss: 1245.6713\n",
            "Epoch 430/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 965.0887 - val_loss: 1448.4731\n",
            "Epoch 431/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 931.5707 - val_loss: 1240.2024\n",
            "Epoch 432/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 911.8871 - val_loss: 1225.0178\n",
            "Epoch 433/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 926.0120 - val_loss: 1292.7562\n",
            "Epoch 434/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 918.6058 - val_loss: 1328.6731\n",
            "Epoch 435/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 897.8934 - val_loss: 1250.7352\n",
            "Epoch 436/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 882.0585 - val_loss: 1227.9199\n",
            "Epoch 437/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 884.0119 - val_loss: 1265.7311\n",
            "Epoch 438/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 876.4835 - val_loss: 1241.0768\n",
            "Epoch 439/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 870.0951 - val_loss: 1231.6858\n",
            "Epoch 440/2000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 864.0861 - val_loss: 1234.3826\n",
            "Epoch 441/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 860.6411 - val_loss: 1232.6802\n",
            "Epoch 442/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 871.3629 - val_loss: 1200.5417\n",
            "Epoch 443/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 858.0944 - val_loss: 1167.3895\n",
            "Epoch 444/2000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 871.4817 - val_loss: 1210.1539\n",
            "Epoch 445/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 850.1266 - val_loss: 1178.0413\n",
            "Epoch 446/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 814.7108 - val_loss: 1181.7601\n",
            "Epoch 447/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 812.3826 - val_loss: 1096.2245\n",
            "Epoch 448/2000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 767.5834 - val_loss: 953.4760\n",
            "Epoch 449/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 692.1386 - val_loss: 472.1718\n",
            "Epoch 450/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 10815.9629 - val_loss: 883.8361\n",
            "Epoch 451/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 8609.9268 - val_loss: 8392.1973\n",
            "Epoch 452/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 6444.7845 - val_loss: 6112.3555\n",
            "Epoch 453/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 9067.9398 - val_loss: 3121.3430\n",
            "Epoch 454/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 2217.7501 - val_loss: 4014.4590\n",
            "Epoch 455/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 4786.4007 - val_loss: 3975.7644\n",
            "Epoch 456/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 2794.5444 - val_loss: 1472.0304\n",
            "Epoch 457/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 1957.2971 - val_loss: 1453.9436\n",
            "Epoch 458/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 1152.9373 - val_loss: 2080.8860\n",
            "Epoch 459/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 1521.9666 - val_loss: 1460.5948\n",
            "Epoch 460/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 978.9484 - val_loss: 1353.6075\n",
            "Epoch 461/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 1204.5334 - val_loss: 1269.8513\n",
            "Epoch 462/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 935.0736 - val_loss: 1462.0510\n",
            "Epoch 463/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 972.8383 - val_loss: 1263.9705\n",
            "Epoch 464/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 968.5113 - val_loss: 1252.5349\n",
            "Epoch 465/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 882.6638 - val_loss: 1455.7859\n",
            "Epoch 466/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 998.6168 - val_loss: 1361.2695\n",
            "Epoch 467/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 879.0511 - val_loss: 1250.7924\n",
            "Epoch 468/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1013.9246 - val_loss: 1262.7145\n",
            "Epoch 469/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 959.8348 - val_loss: 1466.3682\n",
            "Epoch 470/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 972.8623 - val_loss: 1247.2297\n",
            "Epoch 471/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 934.7241 - val_loss: 1260.2424\n",
            "Epoch 472/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 895.5197 - val_loss: 1334.4209\n",
            "Epoch 473/2000\n",
            "80/80 [==============================] - 0s 871us/step - loss: 908.7394 - val_loss: 1282.9783\n",
            "Epoch 474/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 899.4888 - val_loss: 1243.9802\n",
            "Epoch 475/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 898.4387 - val_loss: 1305.8389\n",
            "Epoch 476/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 894.4010 - val_loss: 1280.8448\n",
            "Epoch 477/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 1303.6204 - val_loss: 1287.1760\n",
            "Epoch 478/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 1101.5802 - val_loss: 1404.1072\n",
            "Epoch 479/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 1172.7186 - val_loss: 1507.1832\n",
            "Epoch 480/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 1020.5598 - val_loss: 1409.3495\n",
            "Epoch 481/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 916.3333 - val_loss: 1240.9974\n",
            "Epoch 482/2000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 968.4432 - val_loss: 1259.4185\n",
            "Epoch 483/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 930.5122 - val_loss: 1438.0042\n",
            "Epoch 484/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 920.4490 - val_loss: 1242.6171\n",
            "Epoch 485/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 966.8535 - val_loss: 1237.7156\n",
            "Epoch 486/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 896.4791 - val_loss: 1448.2375\n",
            "Epoch 487/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 974.0433 - val_loss: 1296.7786\n",
            "Epoch 488/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 919.7582 - val_loss: 1233.4498\n",
            "Epoch 489/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 903.5429 - val_loss: 1344.6692\n",
            "Epoch 490/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 984.2106 - val_loss: 1356.2947\n",
            "Epoch 491/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 981.1651 - val_loss: 1215.6589\n",
            "Epoch 492/2000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 1102.9546 - val_loss: 2003.4329\n",
            "Epoch 493/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 1049.0913 - val_loss: 1443.6545\n",
            "Epoch 494/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 1365.4512 - val_loss: 1371.0568\n",
            "Epoch 495/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 1018.0325 - val_loss: 1425.1144\n",
            "Epoch 496/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 872.9653 - val_loss: 1247.9084\n",
            "Epoch 497/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 1045.4202 - val_loss: 1272.3969\n",
            "Epoch 498/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 955.1725 - val_loss: 1448.7939\n",
            "Epoch 499/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 910.4633 - val_loss: 1227.8030\n",
            "Epoch 500/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 984.7985 - val_loss: 1244.1399\n",
            "Epoch 501/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 893.8003 - val_loss: 1429.7031\n",
            "Epoch 502/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 938.8883 - val_loss: 1226.6838\n",
            "Epoch 503/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 893.0439 - val_loss: 1221.3618\n",
            "Epoch 504/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 891.1238 - val_loss: 1282.8163\n",
            "Epoch 505/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 903.5309 - val_loss: 1262.8641\n",
            "Epoch 506/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 919.0611 - val_loss: 1206.6654\n",
            "Epoch 507/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 881.9813 - val_loss: 1306.6409\n",
            "Epoch 508/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 864.8994 - val_loss: 1821.8138\n",
            "Epoch 509/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1021.9467 - val_loss: 1253.2020\n",
            "Epoch 510/2000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 1226.9133 - val_loss: 1262.7888\n",
            "Epoch 511/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 1041.5199 - val_loss: 1611.9967\n",
            "Epoch 512/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1023.1930 - val_loss: 1230.2477\n",
            "Epoch 513/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 983.9050 - val_loss: 1235.2822\n",
            "Epoch 514/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 953.4519 - val_loss: 1453.6575\n",
            "Epoch 515/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 963.2635 - val_loss: 1262.3030\n",
            "Epoch 516/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 887.1945 - val_loss: 1230.0577\n",
            "Epoch 517/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 891.2137 - val_loss: 1295.0389\n",
            "Epoch 518/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 901.0348 - val_loss: 1258.8069\n",
            "Epoch 519/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 887.1710 - val_loss: 1260.1945\n",
            "Epoch 520/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 886.7536 - val_loss: 1267.5077\n",
            "Epoch 521/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 892.4107 - val_loss: 1236.5847\n",
            "Epoch 522/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 897.5216 - val_loss: 1291.7744\n",
            "Epoch 523/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 897.3362 - val_loss: 1246.0865\n",
            "Epoch 524/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 878.5058 - val_loss: 1285.5498\n",
            "Epoch 525/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 874.4091 - val_loss: 1258.4489\n",
            "Epoch 526/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 902.1953 - val_loss: 1241.0994\n",
            "Epoch 527/2000\n",
            "80/80 [==============================] - 0s 872us/step - loss: 869.7880 - val_loss: 1279.8535\n",
            "Epoch 528/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 870.9229 - val_loss: 1246.4286\n",
            "Epoch 529/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 878.9994 - val_loss: 1223.7032\n",
            "Epoch 530/2000\n",
            "80/80 [==============================] - 0s 872us/step - loss: 915.0277 - val_loss: 1224.0618\n",
            "Epoch 531/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 926.5426 - val_loss: 1377.6439\n",
            "Epoch 532/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 884.6510 - val_loss: 1214.1570\n",
            "Epoch 533/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 943.8862 - val_loss: 1214.5731\n",
            "Epoch 534/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 950.9906 - val_loss: 1393.3640\n",
            "Epoch 535/2000\n",
            "80/80 [==============================] - 0s 851us/step - loss: 919.0551 - val_loss: 1222.8015\n",
            "Epoch 536/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 927.5829 - val_loss: 1209.5690\n",
            "Epoch 537/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 912.4875 - val_loss: 1397.3337\n",
            "Epoch 538/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 943.9503 - val_loss: 1235.5508\n",
            "Epoch 539/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 865.0869 - val_loss: 1214.3481\n",
            "Epoch 540/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 933.1177 - val_loss: 1300.3221\n",
            "Epoch 541/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 960.4207 - val_loss: 1281.4631\n",
            "Epoch 542/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 958.3430 - val_loss: 1203.2786\n",
            "Epoch 543/2000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 901.1651 - val_loss: 1345.5027\n",
            "Epoch 544/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 903.7538 - val_loss: 1243.1736\n",
            "Epoch 545/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 879.5933 - val_loss: 1218.3898\n",
            "Epoch 546/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 890.8482 - val_loss: 1261.0033\n",
            "Epoch 547/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 931.3227 - val_loss: 1204.0907\n",
            "Epoch 548/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 854.4216 - val_loss: 1356.7533\n",
            "Epoch 549/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 917.0077 - val_loss: 1259.8143\n",
            "Epoch 550/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 858.6094 - val_loss: 1198.2654\n",
            "Epoch 551/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 887.6466 - val_loss: 1238.7205\n",
            "Epoch 552/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 859.0003 - val_loss: 1245.5723\n",
            "Epoch 553/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 945.3302 - val_loss: 1214.0920\n",
            "Epoch 554/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 854.1033 - val_loss: 1420.2686\n",
            "Epoch 555/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 968.7336 - val_loss: 1231.7800\n",
            "Epoch 556/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 854.6016 - val_loss: 1212.4733\n",
            "Epoch 557/2000\n",
            "80/80 [==============================] - 0s 858us/step - loss: 865.8591 - val_loss: 1215.4056\n",
            "Epoch 558/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 868.5482 - val_loss: 1268.2380\n",
            "Epoch 559/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 880.4220 - val_loss: 1213.1698\n",
            "Epoch 560/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 860.4849 - val_loss: 1258.8167\n",
            "Epoch 561/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 853.3434 - val_loss: 1191.5215\n",
            "Epoch 562/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 865.2495 - val_loss: 1219.8925\n",
            "Epoch 563/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 841.9210 - val_loss: 1292.1322\n",
            "Epoch 564/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 884.3207 - val_loss: 1205.0188\n",
            "Epoch 565/2000\n",
            "80/80 [==============================] - 0s 880us/step - loss: 854.7861 - val_loss: 1194.6311\n",
            "Epoch 566/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 882.6644 - val_loss: 1224.7932\n",
            "Epoch 567/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 857.7034 - val_loss: 1178.5553\n",
            "Epoch 568/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 861.6132 - val_loss: 1171.7917\n",
            "Epoch 569/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 1416.1210 - val_loss: 2319.3569\n",
            "Epoch 570/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 1576.5271 - val_loss: 2119.6653\n",
            "Epoch 571/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 1716.9830 - val_loss: 1380.9169\n",
            "Epoch 572/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 1361.9688 - val_loss: 1550.5623\n",
            "Epoch 573/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 869.2833 - val_loss: 1252.0836\n",
            "Epoch 574/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 1146.9206 - val_loss: 1185.1348\n",
            "Epoch 575/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 891.3799 - val_loss: 1856.2937\n",
            "Epoch 576/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 1175.4223 - val_loss: 1255.4033\n",
            "Epoch 577/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1216.6107 - val_loss: 1182.8772\n",
            "Epoch 578/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 940.1543 - val_loss: 1747.3391\n",
            "Epoch 579/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 1266.9214 - val_loss: 1176.3300\n",
            "Epoch 580/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 1020.9803 - val_loss: 1193.8575\n",
            "Epoch 581/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 1160.2146 - val_loss: 2445.9958\n",
            "Epoch 582/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 1386.4483 - val_loss: 1573.5276\n",
            "Epoch 583/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 1843.8875 - val_loss: 1170.7802\n",
            "Epoch 584/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 1082.4783 - val_loss: 1830.6816\n",
            "Epoch 585/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1163.8054 - val_loss: 1217.5156\n",
            "Epoch 586/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 1118.2218 - val_loss: 1167.9156\n",
            "Epoch 587/2000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 849.0642 - val_loss: 1551.3129\n",
            "Epoch 588/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 1101.2698 - val_loss: 1192.8279\n",
            "Epoch 589/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 965.0506 - val_loss: 1210.9246\n",
            "Epoch 590/2000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 974.6830 - val_loss: 1404.0894\n",
            "Epoch 591/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 936.8343 - val_loss: 1224.8486\n",
            "Epoch 592/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 817.5409 - val_loss: 1165.2122\n",
            "Epoch 593/2000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 893.1194 - val_loss: 1216.5902\n",
            "Epoch 594/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 997.4172 - val_loss: 1271.4174\n",
            "Epoch 595/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 896.3283 - val_loss: 1178.9639\n",
            "Epoch 596/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 934.2409 - val_loss: 1285.9851\n",
            "Epoch 597/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 897.5461 - val_loss: 1271.9828\n",
            "Epoch 598/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 868.2971 - val_loss: 1167.7181\n",
            "Epoch 599/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 844.6163 - val_loss: 1217.3009\n",
            "Epoch 600/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 836.4487 - val_loss: 1207.9634\n",
            "Epoch 601/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 848.5966 - val_loss: 1209.2927\n",
            "Epoch 602/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 836.5715 - val_loss: 1212.7006\n",
            "Epoch 603/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 839.9762 - val_loss: 1169.9814\n",
            "Epoch 604/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 854.4706 - val_loss: 1191.5859\n",
            "Epoch 605/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 836.1779 - val_loss: 1160.4172\n",
            "Epoch 606/2000\n",
            "80/80 [==============================] - 0s 872us/step - loss: 831.3999 - val_loss: 1242.4281\n",
            "Epoch 607/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 881.3736 - val_loss: 1191.6614\n",
            "Epoch 608/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 829.0325 - val_loss: 1222.9961\n",
            "Epoch 609/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 829.6433 - val_loss: 1184.0083\n",
            "Epoch 610/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 837.9753 - val_loss: 1164.5612\n",
            "Epoch 611/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 826.1675 - val_loss: 1210.7598\n",
            "Epoch 612/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 854.6534 - val_loss: 1221.3496\n",
            "Epoch 613/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 812.5488 - val_loss: 1145.3958\n",
            "Epoch 614/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 871.3292 - val_loss: 1154.6930\n",
            "Epoch 615/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 836.9718 - val_loss: 1242.7886\n",
            "Epoch 616/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 843.3925 - val_loss: 1160.1829\n",
            "Epoch 617/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 830.1530 - val_loss: 1177.1497\n",
            "Epoch 618/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 833.5296 - val_loss: 1199.4705\n",
            "Epoch 619/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 826.3960 - val_loss: 1161.9128\n",
            "Epoch 620/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 822.7312 - val_loss: 1198.4891\n",
            "Epoch 621/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 826.1224 - val_loss: 1224.4172\n",
            "Epoch 622/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 862.0569 - val_loss: 1141.4889\n",
            "Epoch 623/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 825.7651 - val_loss: 1234.9229\n",
            "Epoch 624/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 833.6950 - val_loss: 1186.9591\n",
            "Epoch 625/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 840.3267 - val_loss: 1153.9744\n",
            "Epoch 626/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 831.9369 - val_loss: 1221.7629\n",
            "Epoch 627/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 823.4051 - val_loss: 1156.0625\n",
            "Epoch 628/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 833.6162 - val_loss: 1146.5826\n",
            "Epoch 629/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 825.2718 - val_loss: 1241.6012\n",
            "Epoch 630/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 850.8295 - val_loss: 1147.1106\n",
            "Epoch 631/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 830.2412 - val_loss: 1162.0377\n",
            "Epoch 632/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 846.5262 - val_loss: 1158.4121\n",
            "Epoch 633/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 863.2133 - val_loss: 1236.0648\n",
            "Epoch 634/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 857.1874 - val_loss: 1163.6338\n",
            "Epoch 635/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 810.6407 - val_loss: 1208.1943\n",
            "Epoch 636/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 820.0059 - val_loss: 1168.6416\n",
            "Epoch 637/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 807.7100 - val_loss: 1133.0645\n",
            "Epoch 638/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 828.6510 - val_loss: 1164.3553\n",
            "Epoch 639/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 809.8065 - val_loss: 1140.3420\n",
            "Epoch 640/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 840.7438 - val_loss: 1148.3080\n",
            "Epoch 641/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 816.7210 - val_loss: 1243.2136\n",
            "Epoch 642/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 830.6979 - val_loss: 1121.1033\n",
            "Epoch 643/2000\n",
            "80/80 [==============================] - 0s 878us/step - loss: 868.8636 - val_loss: 1142.9683\n",
            "Epoch 644/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 905.8012 - val_loss: 1321.2356\n",
            "Epoch 645/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 857.7740 - val_loss: 1121.1052\n",
            "Epoch 646/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 829.6346 - val_loss: 1134.8981\n",
            "Epoch 647/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 795.0089 - val_loss: 1259.2255\n",
            "Epoch 648/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 835.0947 - val_loss: 1142.0203\n",
            "Epoch 649/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 805.4841 - val_loss: 1121.4297\n",
            "Epoch 650/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 861.8181 - val_loss: 1132.4861\n",
            "Epoch 651/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 843.7104 - val_loss: 1114.0345\n",
            "Epoch 652/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 823.5318 - val_loss: 1282.0125\n",
            "Epoch 653/2000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 872.8888 - val_loss: 1119.6013\n",
            "Epoch 654/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 809.2353 - val_loss: 1172.0901\n",
            "Epoch 655/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 846.7086 - val_loss: 1158.8064\n",
            "Epoch 656/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 803.4079 - val_loss: 1110.3967\n",
            "Epoch 657/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 844.3686 - val_loss: 1234.6768\n",
            "Epoch 658/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 872.2461 - val_loss: 1174.3043\n",
            "Epoch 659/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 797.1478 - val_loss: 1108.8988\n",
            "Epoch 660/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 850.1846 - val_loss: 1195.8184\n",
            "Epoch 661/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 864.3843 - val_loss: 1140.1718\n",
            "Epoch 662/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 907.0152 - val_loss: 1117.5027\n",
            "Epoch 663/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 866.1140 - val_loss: 1355.6735\n",
            "Epoch 664/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 907.9929 - val_loss: 1135.3558\n",
            "Epoch 665/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 800.9462 - val_loss: 1106.7722\n",
            "Epoch 666/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 808.1615 - val_loss: 1156.2384\n",
            "Epoch 667/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 799.6073 - val_loss: 1133.5983\n",
            "Epoch 668/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 813.0956 - val_loss: 1155.4988\n",
            "Epoch 669/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 801.7340 - val_loss: 1199.7268\n",
            "Epoch 670/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 807.6333 - val_loss: 1103.0305\n",
            "Epoch 671/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 800.8078 - val_loss: 1120.3333\n",
            "Epoch 672/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 791.1918 - val_loss: 1162.4690\n",
            "Epoch 673/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 800.1403 - val_loss: 1135.8834\n",
            "Epoch 674/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 792.0914 - val_loss: 1130.8782\n",
            "Epoch 675/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 828.2646 - val_loss: 1104.6594\n",
            "Epoch 676/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 795.7427 - val_loss: 1189.6592\n",
            "Epoch 677/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 790.1364 - val_loss: 1095.9998\n",
            "Epoch 678/2000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 806.1156 - val_loss: 1103.0916\n",
            "Epoch 679/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 800.4639 - val_loss: 1159.7644\n",
            "Epoch 680/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 805.7761 - val_loss: 1124.8822\n",
            "Epoch 681/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 766.9780 - val_loss: 1083.9109\n",
            "Epoch 682/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 857.5464 - val_loss: 1101.3588\n",
            "Epoch 683/2000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 779.4831 - val_loss: 1174.1605\n",
            "Epoch 684/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 798.9808 - val_loss: 1106.1934\n",
            "Epoch 685/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 862.6249 - val_loss: 1084.4324\n",
            "Epoch 686/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 776.8631 - val_loss: 1306.3823\n",
            "Epoch 687/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 843.2242 - val_loss: 1083.1823\n",
            "Epoch 688/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 844.1424 - val_loss: 1088.1199\n",
            "Epoch 689/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 915.4914 - val_loss: 1180.3069\n",
            "Epoch 690/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 904.0251 - val_loss: 1080.3522\n",
            "Epoch 691/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 934.9299 - val_loss: 1172.7438\n",
            "Epoch 692/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 818.9201 - val_loss: 1073.4359\n",
            "Epoch 693/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 803.6011 - val_loss: 1214.3827\n",
            "Epoch 694/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 946.8582 - val_loss: 1071.4891\n",
            "Epoch 695/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 941.9369 - val_loss: 1096.5524\n",
            "Epoch 696/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 905.6559 - val_loss: 1346.0452\n",
            "Epoch 697/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 890.7055 - val_loss: 1075.1204\n",
            "Epoch 698/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 817.5456 - val_loss: 1092.5740\n",
            "Epoch 699/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 922.3777 - val_loss: 1103.9396\n",
            "Epoch 700/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 846.2036 - val_loss: 1087.8220\n",
            "Epoch 701/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 858.3785 - val_loss: 1267.0796\n",
            "Epoch 702/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 912.7769 - val_loss: 1069.8727\n",
            "Epoch 703/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 824.7601 - val_loss: 1067.3483\n",
            "Epoch 704/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 766.8526 - val_loss: 1190.0219\n",
            "Epoch 705/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 789.4776 - val_loss: 1077.9628\n",
            "Epoch 706/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 809.7634 - val_loss: 1064.0968\n",
            "Epoch 707/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 801.3272 - val_loss: 1148.4756\n",
            "Epoch 708/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 752.0886 - val_loss: 1055.4094\n",
            "Epoch 709/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 851.5917 - val_loss: 1082.2191\n",
            "Epoch 710/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 863.0076 - val_loss: 1224.7808\n",
            "Epoch 711/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 788.6987 - val_loss: 1088.5262\n",
            "Epoch 712/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 891.7208 - val_loss: 1213.4725\n",
            "Epoch 713/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 862.4130 - val_loss: 1238.5789\n",
            "Epoch 714/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 793.9617 - val_loss: 1074.3934\n",
            "Epoch 715/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 856.9928 - val_loss: 1152.9474\n",
            "Epoch 716/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 836.8123 - val_loss: 1104.7004\n",
            "Epoch 717/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 812.6268 - val_loss: 1051.8715\n",
            "Epoch 718/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 803.3313 - val_loss: 1322.4893\n",
            "Epoch 719/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 960.7204 - val_loss: 1047.1160\n",
            "Epoch 720/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 1139.7104 - val_loss: 1091.6625\n",
            "Epoch 721/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 980.3960 - val_loss: 1428.5442\n",
            "Epoch 722/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 943.5422 - val_loss: 1078.3975\n",
            "Epoch 723/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 898.9372 - val_loss: 1148.8953\n",
            "Epoch 724/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 852.0889 - val_loss: 1202.0405\n",
            "Epoch 725/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 757.5007 - val_loss: 1053.1091\n",
            "Epoch 726/2000\n",
            "80/80 [==============================] - 0s 868us/step - loss: 851.3205 - val_loss: 1082.0812\n",
            "Epoch 727/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 744.4616 - val_loss: 1137.3779\n",
            "Epoch 728/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 785.0302 - val_loss: 1057.5974\n",
            "Epoch 729/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 747.2098 - val_loss: 1092.3556\n",
            "Epoch 730/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 805.8631 - val_loss: 1046.5979\n",
            "Epoch 731/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 793.6054 - val_loss: 1032.5054\n",
            "Epoch 732/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 760.5241 - val_loss: 1240.5665\n",
            "Epoch 733/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 821.0169 - val_loss: 1050.2146\n",
            "Epoch 734/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 797.6212 - val_loss: 1030.2488\n",
            "Epoch 735/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 761.6186 - val_loss: 1244.0300\n",
            "Epoch 736/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 861.0623 - val_loss: 1025.4338\n",
            "Epoch 737/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 864.5302 - val_loss: 1024.1033\n",
            "Epoch 738/2000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 781.8375 - val_loss: 1254.8154\n",
            "Epoch 739/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 814.4905 - val_loss: 1028.2120\n",
            "Epoch 740/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 755.7367 - val_loss: 1035.5935\n",
            "Epoch 741/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 732.0263 - val_loss: 1104.2892\n",
            "Epoch 742/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 744.0174 - val_loss: 1044.4241\n",
            "Epoch 743/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 739.5724 - val_loss: 1027.7346\n",
            "Epoch 744/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 735.2218 - val_loss: 1098.5779\n",
            "Epoch 745/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 789.6033 - val_loss: 1026.1904\n",
            "Epoch 746/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 868.4923 - val_loss: 1013.4291\n",
            "Epoch 747/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 794.7871 - val_loss: 1315.3162\n",
            "Epoch 748/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 834.8334 - val_loss: 1011.5583\n",
            "Epoch 749/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 813.3577 - val_loss: 1020.5508\n",
            "Epoch 750/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 731.1418 - val_loss: 1087.1862\n",
            "Epoch 751/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 743.5018 - val_loss: 1043.0900\n",
            "Epoch 752/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 721.6086 - val_loss: 1077.2673\n",
            "Epoch 753/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 761.3198 - val_loss: 1011.1600\n",
            "Epoch 754/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 900.5935 - val_loss: 1007.5212\n",
            "Epoch 755/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 747.7651 - val_loss: 1319.7880\n",
            "Epoch 756/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 906.6888 - val_loss: 1000.8687\n",
            "Epoch 757/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 867.7279 - val_loss: 999.8922\n",
            "Epoch 758/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 704.2831 - val_loss: 1495.1696\n",
            "Epoch 759/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 949.5302 - val_loss: 995.8590\n",
            "Epoch 760/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 868.4012 - val_loss: 998.5482\n",
            "Epoch 761/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 823.3478 - val_loss: 1129.5386\n",
            "Epoch 762/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 760.5661 - val_loss: 991.7888\n",
            "Epoch 763/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 776.2744 - val_loss: 1063.0094\n",
            "Epoch 764/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 716.2917 - val_loss: 1000.0648\n",
            "Epoch 765/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 719.2367 - val_loss: 1019.8117\n",
            "Epoch 766/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 775.1053 - val_loss: 1026.9114\n",
            "Epoch 767/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 689.4969 - val_loss: 982.8680\n",
            "Epoch 768/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 823.0268 - val_loss: 1044.1868\n",
            "Epoch 769/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 800.4020 - val_loss: 1135.2413\n",
            "Epoch 770/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 766.2448 - val_loss: 997.4606\n",
            "Epoch 771/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 770.2862 - val_loss: 1192.7917\n",
            "Epoch 772/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 821.7490 - val_loss: 1013.7832\n",
            "Epoch 773/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 709.8150 - val_loss: 968.5046\n",
            "Epoch 774/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 808.7749 - val_loss: 1012.6615\n",
            "Epoch 775/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 746.2932 - val_loss: 968.3866\n",
            "Epoch 776/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 685.8100 - val_loss: 1110.2236\n",
            "Epoch 777/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 736.0902 - val_loss: 975.7374\n",
            "Epoch 778/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 690.0886 - val_loss: 957.8744\n",
            "Epoch 779/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 700.7774 - val_loss: 993.3008\n",
            "Epoch 780/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 686.2556 - val_loss: 995.5288\n",
            "Epoch 781/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 698.1950 - val_loss: 963.7489\n",
            "Epoch 782/2000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 689.5344 - val_loss: 951.1674\n",
            "Epoch 783/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 696.6990 - val_loss: 971.1784\n",
            "Epoch 784/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 681.9862 - val_loss: 958.0015\n",
            "Epoch 785/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 684.6177 - val_loss: 973.9740\n",
            "Epoch 786/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 675.3443 - val_loss: 938.9700\n",
            "Epoch 787/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 718.6350 - val_loss: 1011.1196\n",
            "Epoch 788/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 695.4498 - val_loss: 1091.8044\n",
            "Epoch 789/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 697.0646 - val_loss: 910.3166\n",
            "Epoch 790/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 734.5380 - val_loss: 928.7241\n",
            "Epoch 791/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 752.1700 - val_loss: 953.8065\n",
            "Epoch 792/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 673.5943 - val_loss: 899.6119\n",
            "Epoch 793/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 673.1134 - val_loss: 991.1500\n",
            "Epoch 794/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 687.2513 - val_loss: 906.9053\n",
            "Epoch 795/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 668.8908 - val_loss: 895.7490\n",
            "Epoch 796/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 657.1727 - val_loss: 994.5734\n",
            "Epoch 797/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 710.9231 - val_loss: 874.3507\n",
            "Epoch 798/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 681.5229 - val_loss: 868.0890\n",
            "Epoch 799/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 676.5464 - val_loss: 933.7578\n",
            "Epoch 800/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 676.2133 - val_loss: 857.1527\n",
            "Epoch 801/2000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 674.8401 - val_loss: 888.8475\n",
            "Epoch 802/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 646.5935 - val_loss: 843.2902\n",
            "Epoch 803/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 683.7669 - val_loss: 849.3137\n",
            "Epoch 804/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 655.1854 - val_loss: 911.0392\n",
            "Epoch 805/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 658.9664 - val_loss: 793.0284\n",
            "Epoch 806/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 629.0343 - val_loss: 919.5936\n",
            "Epoch 807/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 654.3917 - val_loss: 809.5953\n",
            "Epoch 808/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 749.8476 - val_loss: 764.9300\n",
            "Epoch 809/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 626.8229 - val_loss: 1057.0226\n",
            "Epoch 810/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 718.3427 - val_loss: 743.1894\n",
            "Epoch 811/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 683.7154 - val_loss: 763.7169\n",
            "Epoch 812/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 580.0171 - val_loss: 930.8319\n",
            "Epoch 813/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 618.6210 - val_loss: 600.6639\n",
            "Epoch 814/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 479.8745 - val_loss: 903.8158\n",
            "Epoch 815/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 531.4999 - val_loss: 434.9897\n",
            "Epoch 816/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 550.5756 - val_loss: 694.3279\n",
            "Epoch 817/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 478.3904 - val_loss: 479.0308\n",
            "Epoch 818/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 520.2518 - val_loss: 595.9028\n",
            "Epoch 819/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 477.1259 - val_loss: 454.0328\n",
            "Epoch 820/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 418.6721 - val_loss: 519.5588\n",
            "Epoch 821/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 430.6734 - val_loss: 543.5357\n",
            "Epoch 822/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 414.3979 - val_loss: 398.0895\n",
            "Epoch 823/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 407.4118 - val_loss: 610.6835\n",
            "Epoch 824/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 426.7402 - val_loss: 379.8524\n",
            "Epoch 825/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 511.2850 - val_loss: 552.8506\n",
            "Epoch 826/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 601.5626 - val_loss: 601.5012\n",
            "Epoch 827/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 470.6058 - val_loss: 548.2433\n",
            "Epoch 828/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 475.6912 - val_loss: 535.9667\n",
            "Epoch 829/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 447.7255 - val_loss: 412.0739\n",
            "Epoch 830/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 456.9605 - val_loss: 528.7543\n",
            "Epoch 831/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 409.8726 - val_loss: 338.8654\n",
            "Epoch 832/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 375.3473 - val_loss: 514.1605\n",
            "Epoch 833/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 391.8041 - val_loss: 361.4180\n",
            "Epoch 834/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 437.0512 - val_loss: 434.8445\n",
            "Epoch 835/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 378.0904 - val_loss: 475.3535\n",
            "Epoch 836/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 351.4206 - val_loss: 354.2289\n",
            "Epoch 837/2000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 388.8082 - val_loss: 526.5872\n",
            "Epoch 838/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 383.1966 - val_loss: 435.6956\n",
            "Epoch 839/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 372.3654 - val_loss: 295.1119\n",
            "Epoch 840/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 468.5554 - val_loss: 306.7712\n",
            "Epoch 841/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 511.0735 - val_loss: 384.3565\n",
            "Epoch 842/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 455.6062 - val_loss: 690.3898\n",
            "Epoch 843/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 469.6735 - val_loss: 321.9955\n",
            "Epoch 844/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 410.5353 - val_loss: 607.0408\n",
            "Epoch 845/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 466.5628 - val_loss: 315.6732\n",
            "Epoch 846/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 418.6082 - val_loss: 347.6727\n",
            "Epoch 847/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 333.1381 - val_loss: 427.5418\n",
            "Epoch 848/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 370.9884 - val_loss: 196.6089\n",
            "Epoch 849/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 391.4930 - val_loss: 422.9832\n",
            "Epoch 850/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 367.4598 - val_loss: 358.8425\n",
            "Epoch 851/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 313.2219 - val_loss: 448.9465\n",
            "Epoch 852/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 331.7627 - val_loss: 258.0012\n",
            "Epoch 853/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 345.8959 - val_loss: 269.9742\n",
            "Epoch 854/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 310.3308 - val_loss: 407.4220\n",
            "Epoch 855/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 306.6207 - val_loss: 327.3248\n",
            "Epoch 856/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 299.6932 - val_loss: 350.8258\n",
            "Epoch 857/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 295.9938 - val_loss: 312.1615\n",
            "Epoch 858/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 336.1914 - val_loss: 150.2056\n",
            "Epoch 859/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 462.9415 - val_loss: 587.4659\n",
            "Epoch 860/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 598.9585 - val_loss: 616.3191\n",
            "Epoch 861/2000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 524.2602 - val_loss: 584.1126\n",
            "Epoch 862/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 515.7573 - val_loss: 630.0547\n",
            "Epoch 863/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 498.8143 - val_loss: 426.3192\n",
            "Epoch 864/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 482.9785 - val_loss: 314.8674\n",
            "Epoch 865/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 274.1023 - val_loss: 351.6791\n",
            "Epoch 866/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 394.9978 - val_loss: 183.5288\n",
            "Epoch 867/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 319.9658 - val_loss: 708.8580\n",
            "Epoch 868/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 446.0827 - val_loss: 327.4515\n",
            "Epoch 869/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 390.3657 - val_loss: 330.1165\n",
            "Epoch 870/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 374.3599 - val_loss: 163.3743\n",
            "Epoch 871/2000\n",
            "80/80 [==============================] - 0s 871us/step - loss: 306.5834 - val_loss: 385.2505\n",
            "Epoch 872/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 385.2555 - val_loss: 305.9063\n",
            "Epoch 873/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 389.3554 - val_loss: 159.1660\n",
            "Epoch 874/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 327.5504 - val_loss: 277.0439\n",
            "Epoch 875/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 363.5846 - val_loss: 648.2039\n",
            "Epoch 876/2000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 575.7594 - val_loss: 771.7354\n",
            "Epoch 877/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 648.6814 - val_loss: 796.9503\n",
            "Epoch 878/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 616.2168 - val_loss: 980.3641\n",
            "Epoch 879/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 633.3933 - val_loss: 808.1682\n",
            "Epoch 880/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 651.7737 - val_loss: 849.7117\n",
            "Epoch 881/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 672.1891 - val_loss: 812.1131\n",
            "Epoch 882/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 688.3686 - val_loss: 825.4758\n",
            "Epoch 883/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 598.2765 - val_loss: 994.7050\n",
            "Epoch 884/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 646.8820 - val_loss: 782.1365\n",
            "Epoch 885/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 592.3301 - val_loss: 902.9113\n",
            "Epoch 886/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 634.2973 - val_loss: 708.3296\n",
            "Epoch 887/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 771.9167 - val_loss: 673.8133\n",
            "Epoch 888/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 658.2470 - val_loss: 757.3488\n",
            "Epoch 889/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 533.6412 - val_loss: 459.1868\n",
            "Epoch 890/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 494.0582 - val_loss: 543.9351\n",
            "Epoch 891/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 311.5909 - val_loss: 996.7770\n",
            "Epoch 892/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 2271.7456 - val_loss: 1000.5078\n",
            "Epoch 893/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 1074.1606 - val_loss: 1775.2180\n",
            "Epoch 894/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 1200.5500 - val_loss: 1374.5250\n",
            "Epoch 895/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 1183.4853 - val_loss: 1654.4795\n",
            "Epoch 896/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1110.3818 - val_loss: 974.5032\n",
            "Epoch 897/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 1309.1157 - val_loss: 963.0621\n",
            "Epoch 898/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 867.5896 - val_loss: 1674.3311\n",
            "Epoch 899/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 1166.6456 - val_loss: 1277.3007\n",
            "Epoch 900/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1144.0200 - val_loss: 1334.2659\n",
            "Epoch 901/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 877.1501 - val_loss: 984.4588\n",
            "Epoch 902/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1011.3130 - val_loss: 971.9940\n",
            "Epoch 903/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 702.8083 - val_loss: 1551.3953\n",
            "Epoch 904/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 906.9804 - val_loss: 1057.3440\n",
            "Epoch 905/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 982.7872 - val_loss: 1060.2404\n",
            "Epoch 906/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 983.5557 - val_loss: 949.6345\n",
            "Epoch 907/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 893.6744 - val_loss: 976.2484\n",
            "Epoch 908/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 834.8083 - val_loss: 1172.0334\n",
            "Epoch 909/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 817.7491 - val_loss: 968.4756\n",
            "Epoch 910/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 754.1434 - val_loss: 1195.6006\n",
            "Epoch 911/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 806.7762 - val_loss: 939.2736\n",
            "Epoch 912/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 714.0858 - val_loss: 968.9763\n",
            "Epoch 913/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 673.4392 - val_loss: 1032.1029\n",
            "Epoch 914/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 698.9697 - val_loss: 937.4070\n",
            "Epoch 915/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 706.4380 - val_loss: 1057.5587\n",
            "Epoch 916/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 701.7356 - val_loss: 935.2262\n",
            "Epoch 917/2000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 720.2543 - val_loss: 1012.1144\n",
            "Epoch 918/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 736.5951 - val_loss: 952.4540\n",
            "Epoch 919/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 699.7232 - val_loss: 960.1985\n",
            "Epoch 920/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 664.4696 - val_loss: 1083.5439\n",
            "Epoch 921/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 735.3323 - val_loss: 937.2747\n",
            "Epoch 922/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 755.5782 - val_loss: 981.5750\n",
            "Epoch 923/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 677.7117 - val_loss: 965.3827\n",
            "Epoch 924/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 681.6028 - val_loss: 977.5389\n",
            "Epoch 925/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 673.4629 - val_loss: 948.8377\n",
            "Epoch 926/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 668.1651 - val_loss: 941.8335\n",
            "Epoch 927/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 684.8928 - val_loss: 946.5138\n",
            "Epoch 928/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 682.0372 - val_loss: 949.0674\n",
            "Epoch 929/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 805.4053 - val_loss: 937.3428\n",
            "Epoch 930/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 905.2154 - val_loss: 955.0820\n",
            "Epoch 931/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 777.4661 - val_loss: 1015.1104\n",
            "Epoch 932/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 747.5377 - val_loss: 918.1111\n",
            "Epoch 933/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 708.4511 - val_loss: 1120.2625\n",
            "Epoch 934/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 701.0732 - val_loss: 919.9573\n",
            "Epoch 935/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 701.4131 - val_loss: 1199.1133\n",
            "Epoch 936/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 908.6690 - val_loss: 918.9437\n",
            "Epoch 937/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 1128.0598 - val_loss: 946.4451\n",
            "Epoch 938/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 714.4050 - val_loss: 1338.5588\n",
            "Epoch 939/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 928.6909 - val_loss: 996.6019\n",
            "Epoch 940/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 756.0788 - val_loss: 1440.1353\n",
            "Epoch 941/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1016.4365 - val_loss: 911.5734\n",
            "Epoch 942/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 743.7555 - val_loss: 961.1878\n",
            "Epoch 943/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 693.1044 - val_loss: 901.9445\n",
            "Epoch 944/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 721.3806 - val_loss: 920.8629\n",
            "Epoch 945/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 646.1001 - val_loss: 1041.7246\n",
            "Epoch 946/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 712.7055 - val_loss: 910.5067\n",
            "Epoch 947/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 646.5169 - val_loss: 985.2115\n",
            "Epoch 948/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 669.5409 - val_loss: 919.5012\n",
            "Epoch 949/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 647.9588 - val_loss: 933.1232\n",
            "Epoch 950/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 673.2421 - val_loss: 892.6091\n",
            "Epoch 951/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 700.4151 - val_loss: 1002.7790\n",
            "Epoch 952/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 743.9459 - val_loss: 889.9908\n",
            "Epoch 953/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 880.2235 - val_loss: 900.8842\n",
            "Epoch 954/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 879.9261 - val_loss: 1007.5388\n",
            "Epoch 955/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 684.9859 - val_loss: 913.1937\n",
            "Epoch 956/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 715.4855 - val_loss: 1205.3401\n",
            "Epoch 957/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 798.4177 - val_loss: 918.4572\n",
            "Epoch 958/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 723.2134 - val_loss: 1150.6017\n",
            "Epoch 959/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 822.3596 - val_loss: 910.7714\n",
            "Epoch 960/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 886.3988 - val_loss: 1009.5439\n",
            "Epoch 961/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 724.0017 - val_loss: 1056.8517\n",
            "Epoch 962/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 658.1990 - val_loss: 920.2992\n",
            "Epoch 963/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 806.4305 - val_loss: 954.2648\n",
            "Epoch 964/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 715.4689 - val_loss: 875.6647\n",
            "Epoch 965/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 781.5047 - val_loss: 917.5822\n",
            "Epoch 966/2000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 751.2686 - val_loss: 931.8143\n",
            "Epoch 967/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 738.0829 - val_loss: 868.4336\n",
            "Epoch 968/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 702.4806 - val_loss: 1029.8945\n",
            "Epoch 969/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 667.9877 - val_loss: 866.5820\n",
            "Epoch 970/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 682.6546 - val_loss: 929.3877\n",
            "Epoch 971/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 632.9904 - val_loss: 873.5249\n",
            "Epoch 972/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 627.1012 - val_loss: 971.2360\n",
            "Epoch 973/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 758.6721 - val_loss: 863.6400\n",
            "Epoch 974/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 800.8929 - val_loss: 935.2361\n",
            "Epoch 975/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 702.5525 - val_loss: 921.3024\n",
            "Epoch 976/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 716.2317 - val_loss: 866.2072\n",
            "Epoch 977/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 825.9942 - val_loss: 923.1831\n",
            "Epoch 978/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 745.5538 - val_loss: 855.3309\n",
            "Epoch 979/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 592.2428 - val_loss: 1263.2333\n",
            "Epoch 980/2000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 882.5248 - val_loss: 901.0153\n",
            "Epoch 981/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 756.0752 - val_loss: 981.9469\n",
            "Epoch 982/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 688.0482 - val_loss: 852.3351\n",
            "Epoch 983/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 626.4930 - val_loss: 892.3361\n",
            "Epoch 984/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 638.8921 - val_loss: 864.6161\n",
            "Epoch 985/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 648.4489 - val_loss: 868.6522\n",
            "Epoch 986/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 612.8922 - val_loss: 981.3262\n",
            "Epoch 987/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 671.9543 - val_loss: 840.5104\n",
            "Epoch 988/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 748.6251 - val_loss: 903.3853\n",
            "Epoch 989/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 676.6096 - val_loss: 900.5740\n",
            "Epoch 990/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 679.1417 - val_loss: 849.1561\n",
            "Epoch 991/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 674.8581 - val_loss: 902.8845\n",
            "Epoch 992/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 610.4847 - val_loss: 835.7184\n",
            "Epoch 993/2000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 664.4603 - val_loss: 1010.4455\n",
            "Epoch 994/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 641.7528 - val_loss: 843.5861\n",
            "Epoch 995/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 690.1944 - val_loss: 1028.3865\n",
            "Epoch 996/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 721.3542 - val_loss: 828.4250\n",
            "Epoch 997/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 647.6712 - val_loss: 872.4539\n",
            "Epoch 998/2000\n",
            "80/80 [==============================] - 0s 870us/step - loss: 600.0917 - val_loss: 893.0903\n",
            "Epoch 999/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 603.0183 - val_loss: 828.6253\n",
            "Epoch 1000/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 626.5504 - val_loss: 832.6285\n",
            "Epoch 1001/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 664.2472 - val_loss: 876.6839\n",
            "Epoch 1002/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 615.7523 - val_loss: 875.6242\n",
            "Epoch 1003/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 624.6127 - val_loss: 818.9108\n",
            "Epoch 1004/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 656.0471 - val_loss: 877.9000\n",
            "Epoch 1005/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 621.4197 - val_loss: 829.1000\n",
            "Epoch 1006/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 596.6840 - val_loss: 924.6371\n",
            "Epoch 1007/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 615.5499 - val_loss: 823.7269\n",
            "Epoch 1008/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 678.0551 - val_loss: 882.7867\n",
            "Epoch 1009/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 629.0181 - val_loss: 847.4188\n",
            "Epoch 1010/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 610.9996 - val_loss: 828.3795\n",
            "Epoch 1011/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 605.7969 - val_loss: 854.3314\n",
            "Epoch 1012/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 619.7557 - val_loss: 811.2267\n",
            "Epoch 1013/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 617.7717 - val_loss: 867.1380\n",
            "Epoch 1014/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 579.3417 - val_loss: 802.1100\n",
            "Epoch 1015/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 610.7630 - val_loss: 856.7780\n",
            "Epoch 1016/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 579.7692 - val_loss: 803.4667\n",
            "Epoch 1017/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 592.9555 - val_loss: 884.1075\n",
            "Epoch 1018/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 639.3698 - val_loss: 796.3436\n",
            "Epoch 1019/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 643.9585 - val_loss: 816.5234\n",
            "Epoch 1020/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 597.9525 - val_loss: 868.6162\n",
            "Epoch 1021/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 622.6643 - val_loss: 792.2650\n",
            "Epoch 1022/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 615.6482 - val_loss: 927.7922\n",
            "Epoch 1023/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 597.7013 - val_loss: 818.4509\n",
            "Epoch 1024/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 693.5736 - val_loss: 944.0948\n",
            "Epoch 1025/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 617.5816 - val_loss: 792.6460\n",
            "Epoch 1026/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 620.1721 - val_loss: 825.9136\n",
            "Epoch 1027/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 597.0535 - val_loss: 791.8788\n",
            "Epoch 1028/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 669.1605 - val_loss: 805.4927\n",
            "Epoch 1029/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 595.7077 - val_loss: 895.3869\n",
            "Epoch 1030/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 586.8369 - val_loss: 779.6906\n",
            "Epoch 1031/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 599.0806 - val_loss: 963.2965\n",
            "Epoch 1032/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 648.6742 - val_loss: 778.3485\n",
            "Epoch 1033/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 610.8247 - val_loss: 849.9503\n",
            "Epoch 1034/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 576.0435 - val_loss: 773.5165\n",
            "Epoch 1035/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 635.7581 - val_loss: 829.8917\n",
            "Epoch 1036/2000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 614.4393 - val_loss: 804.6094\n",
            "Epoch 1037/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 637.0107 - val_loss: 783.3329\n",
            "Epoch 1038/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 548.4338 - val_loss: 940.6473\n",
            "Epoch 1039/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 646.2167 - val_loss: 772.8552\n",
            "Epoch 1040/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 627.7820 - val_loss: 826.8885\n",
            "Epoch 1041/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 573.2922 - val_loss: 807.9891\n",
            "Epoch 1042/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 571.5906 - val_loss: 783.7037\n",
            "Epoch 1043/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 549.8858 - val_loss: 883.8459\n",
            "Epoch 1044/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 613.4923 - val_loss: 774.0784\n",
            "Epoch 1045/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 558.5220 - val_loss: 811.2894\n",
            "Epoch 1046/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 576.7068 - val_loss: 809.2233\n",
            "Epoch 1047/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 562.2235 - val_loss: 806.2668\n",
            "Epoch 1048/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 553.3010 - val_loss: 752.1978\n",
            "Epoch 1049/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 604.2765 - val_loss: 894.9803\n",
            "Epoch 1050/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 683.3800 - val_loss: 757.1531\n",
            "Epoch 1051/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 560.5940 - val_loss: 763.0641\n",
            "Epoch 1052/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 546.1619 - val_loss: 780.3400\n",
            "Epoch 1053/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 549.5097 - val_loss: 767.9549\n",
            "Epoch 1054/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 623.5706 - val_loss: 751.9374\n",
            "Epoch 1055/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 740.4696 - val_loss: 815.3231\n",
            "Epoch 1056/2000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 833.9993 - val_loss: 739.7693\n",
            "Epoch 1057/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 946.6789 - val_loss: 753.9598\n",
            "Epoch 1058/2000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 776.5765 - val_loss: 862.6526\n",
            "Epoch 1059/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 511.4001 - val_loss: 765.0847\n",
            "Epoch 1060/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 619.7398 - val_loss: 930.5690\n",
            "Epoch 1061/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 614.9413 - val_loss: 732.2859\n",
            "Epoch 1062/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 564.7918 - val_loss: 811.7775\n",
            "Epoch 1063/2000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 546.2722 - val_loss: 729.9949\n",
            "Epoch 1064/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 564.4985 - val_loss: 851.2594\n",
            "Epoch 1065/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 609.6568 - val_loss: 733.7729\n",
            "Epoch 1066/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 572.1545 - val_loss: 724.7037\n",
            "Epoch 1067/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 563.1061 - val_loss: 732.4913\n",
            "Epoch 1068/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 547.8758 - val_loss: 781.1155\n",
            "Epoch 1069/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 541.9773 - val_loss: 735.9470\n",
            "Epoch 1070/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 688.8895 - val_loss: 829.8870\n",
            "Epoch 1071/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 636.8056 - val_loss: 801.7908\n",
            "Epoch 1072/2000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 546.9565 - val_loss: 714.6647\n",
            "Epoch 1073/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 537.9833 - val_loss: 803.9286\n",
            "Epoch 1074/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 544.4698 - val_loss: 712.6205\n",
            "Epoch 1075/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 537.7835 - val_loss: 750.0153\n",
            "Epoch 1076/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 517.0796 - val_loss: 710.5781\n",
            "Epoch 1077/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 527.9016 - val_loss: 852.5172\n",
            "Epoch 1078/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 571.8571 - val_loss: 709.0126\n",
            "Epoch 1079/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 574.2509 - val_loss: 732.0135\n",
            "Epoch 1080/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 509.2867 - val_loss: 769.8255\n",
            "Epoch 1081/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 522.4743 - val_loss: 708.1373\n",
            "Epoch 1082/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 528.2556 - val_loss: 792.9955\n",
            "Epoch 1083/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 576.6942 - val_loss: 699.8004\n",
            "Epoch 1084/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 634.3012 - val_loss: 784.6816\n",
            "Epoch 1085/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 573.8185 - val_loss: 727.5226\n",
            "Epoch 1086/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 553.4232 - val_loss: 700.7133\n",
            "Epoch 1087/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 559.1205 - val_loss: 725.7948\n",
            "Epoch 1088/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 530.5315 - val_loss: 699.2189\n",
            "Epoch 1089/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 528.3020 - val_loss: 755.2123\n",
            "Epoch 1090/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 530.0962 - val_loss: 713.4032\n",
            "Epoch 1091/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 626.1075 - val_loss: 734.1917\n",
            "Epoch 1092/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 859.5573 - val_loss: 865.4850\n",
            "Epoch 1093/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 687.9489 - val_loss: 711.6915\n",
            "Epoch 1094/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 744.5860 - val_loss: 680.1306\n",
            "Epoch 1095/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 594.3279 - val_loss: 1144.8491\n",
            "Epoch 1096/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 820.2858 - val_loss: 720.5383\n",
            "Epoch 1097/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 634.8761 - val_loss: 815.1569\n",
            "Epoch 1098/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 476.6171 - val_loss: 737.4919\n",
            "Epoch 1099/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 628.8938 - val_loss: 851.3573\n",
            "Epoch 1100/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 555.7222 - val_loss: 670.0698\n",
            "Epoch 1101/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 528.0861 - val_loss: 731.8866\n",
            "Epoch 1102/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 556.2807 - val_loss: 714.9982\n",
            "Epoch 1103/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 596.3172 - val_loss: 673.7062\n",
            "Epoch 1104/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 644.5298 - val_loss: 700.4624\n",
            "Epoch 1105/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 805.1661 - val_loss: 708.8679\n",
            "Epoch 1106/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 760.2289 - val_loss: 763.7278\n",
            "Epoch 1107/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 981.7212 - val_loss: 705.1843\n",
            "Epoch 1108/2000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 858.4011 - val_loss: 886.4446\n",
            "Epoch 1109/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 1119.2780 - val_loss: 762.4583\n",
            "Epoch 1110/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 1310.6790 - val_loss: 801.4264\n",
            "Epoch 1111/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 1295.9773 - val_loss: 652.3129\n",
            "Epoch 1112/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 1353.0020 - val_loss: 650.9850\n",
            "Epoch 1113/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 929.0489 - val_loss: 1189.0105\n",
            "Epoch 1114/2000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 665.6170 - val_loss: 829.0588\n",
            "Epoch 1115/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 755.6057 - val_loss: 1049.5270\n",
            "Epoch 1116/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 599.7727 - val_loss: 721.9716\n",
            "Epoch 1117/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 671.8785 - val_loss: 794.4911\n",
            "Epoch 1118/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 551.9423 - val_loss: 644.3314\n",
            "Epoch 1119/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 481.1875 - val_loss: 711.6126\n",
            "Epoch 1120/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 479.4033 - val_loss: 753.6432\n",
            "Epoch 1121/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 821.5414 - val_loss: 980.9580\n",
            "Epoch 1122/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 837.7373 - val_loss: 640.9481\n",
            "Epoch 1123/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 636.1737 - val_loss: 689.3859\n",
            "Epoch 1124/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 621.6087 - val_loss: 633.0844\n",
            "Epoch 1125/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 654.9835 - val_loss: 660.8366\n",
            "Epoch 1126/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 574.3156 - val_loss: 647.4038\n",
            "Epoch 1127/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 551.8708 - val_loss: 649.1256\n",
            "Epoch 1128/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 735.3127 - val_loss: 626.0922\n",
            "Epoch 1129/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 651.4994 - val_loss: 680.6740\n",
            "Epoch 1130/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 548.3428 - val_loss: 658.3835\n",
            "Epoch 1131/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 555.6020 - val_loss: 633.2539\n",
            "Epoch 1132/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 514.7128 - val_loss: 677.2808\n",
            "Epoch 1133/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 455.9088 - val_loss: 632.2193\n",
            "Epoch 1134/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 488.8985 - val_loss: 869.6276\n",
            "Epoch 1135/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 569.2517 - val_loss: 709.2955\n",
            "Epoch 1136/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 768.8456 - val_loss: 826.3746\n",
            "Epoch 1137/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 897.1652 - val_loss: 821.5910\n",
            "Epoch 1138/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 1183.3935 - val_loss: 1023.9019\n",
            "Epoch 1139/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 779.8060 - val_loss: 628.5698\n",
            "Epoch 1140/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 572.4473 - val_loss: 623.4664\n",
            "Epoch 1141/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 549.6265 - val_loss: 627.2732\n",
            "Epoch 1142/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 517.2010 - val_loss: 608.4321\n",
            "Epoch 1143/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 481.5693 - val_loss: 697.5087\n",
            "Epoch 1144/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 484.7290 - val_loss: 609.3525\n",
            "Epoch 1145/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 482.6614 - val_loss: 767.9543\n",
            "Epoch 1146/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 501.3180 - val_loss: 632.6606\n",
            "Epoch 1147/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 532.8861 - val_loss: 729.8221\n",
            "Epoch 1148/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 471.2737 - val_loss: 592.2952\n",
            "Epoch 1149/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 481.3952 - val_loss: 615.1625\n",
            "Epoch 1150/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 430.2961 - val_loss: 590.0692\n",
            "Epoch 1151/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 442.4149 - val_loss: 645.1880\n",
            "Epoch 1152/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 424.3059 - val_loss: 598.4399\n",
            "Epoch 1153/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 502.1221 - val_loss: 681.8609\n",
            "Epoch 1154/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 455.7672 - val_loss: 593.0101\n",
            "Epoch 1155/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 428.9068 - val_loss: 599.2604\n",
            "Epoch 1156/2000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 462.1804 - val_loss: 578.0883\n",
            "Epoch 1157/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 443.4104 - val_loss: 621.8011\n",
            "Epoch 1158/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 451.2252 - val_loss: 575.7248\n",
            "Epoch 1159/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 442.7434 - val_loss: 598.7469\n",
            "Epoch 1160/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 436.0569 - val_loss: 571.9172\n",
            "Epoch 1161/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 509.3712 - val_loss: 659.0266\n",
            "Epoch 1162/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 640.9945 - val_loss: 745.0920\n",
            "Epoch 1163/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 1181.2787 - val_loss: 849.4771\n",
            "Epoch 1164/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 890.1995 - val_loss: 578.2782\n",
            "Epoch 1165/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 687.9269 - val_loss: 743.9440\n",
            "Epoch 1166/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 592.8435 - val_loss: 567.2152\n",
            "Epoch 1167/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 550.4766 - val_loss: 585.4813\n",
            "Epoch 1168/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 655.5965 - val_loss: 611.5897\n",
            "Epoch 1169/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 914.1027 - val_loss: 661.5266\n",
            "Epoch 1170/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 697.4972 - val_loss: 618.6595\n",
            "Epoch 1171/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 609.6017 - val_loss: 555.9586\n",
            "Epoch 1172/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 496.1941 - val_loss: 862.2406\n",
            "Epoch 1173/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 617.8239 - val_loss: 608.7559\n",
            "Epoch 1174/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 512.8059 - val_loss: 1024.2423\n",
            "Epoch 1175/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 665.8933 - val_loss: 809.0121\n",
            "Epoch 1176/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 687.0331 - val_loss: 955.1341\n",
            "Epoch 1177/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 589.3833 - val_loss: 580.4686\n",
            "Epoch 1178/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 493.4002 - val_loss: 713.8096\n",
            "Epoch 1179/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 493.1697 - val_loss: 545.3214\n",
            "Epoch 1180/2000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 418.3309 - val_loss: 744.9152\n",
            "Epoch 1181/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 458.4259 - val_loss: 545.2078\n",
            "Epoch 1182/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 466.0461 - val_loss: 577.2959\n",
            "Epoch 1183/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 395.8835 - val_loss: 533.6608\n",
            "Epoch 1184/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 399.7721 - val_loss: 568.1577\n",
            "Epoch 1185/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 409.8135 - val_loss: 533.1333\n",
            "Epoch 1186/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 394.2725 - val_loss: 604.5675\n",
            "Epoch 1187/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 447.5337 - val_loss: 540.1505\n",
            "Epoch 1188/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 402.5455 - val_loss: 552.9845\n",
            "Epoch 1189/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 399.1665 - val_loss: 561.2067\n",
            "Epoch 1190/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 386.4311 - val_loss: 563.1059\n",
            "Epoch 1191/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 380.3954 - val_loss: 516.0367\n",
            "Epoch 1192/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 389.7611 - val_loss: 606.3417\n",
            "Epoch 1193/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 421.4562 - val_loss: 542.9123\n",
            "Epoch 1194/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 477.3279 - val_loss: 633.3248\n",
            "Epoch 1195/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 411.8616 - val_loss: 509.3978\n",
            "Epoch 1196/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 382.6226 - val_loss: 592.7459\n",
            "Epoch 1197/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 405.2779 - val_loss: 525.9735\n",
            "Epoch 1198/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 390.6417 - val_loss: 502.1855\n",
            "Epoch 1199/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 446.7383 - val_loss: 570.9660\n",
            "Epoch 1200/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 411.2555 - val_loss: 515.5053\n",
            "Epoch 1201/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 391.8801 - val_loss: 546.6490\n",
            "Epoch 1202/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 378.9259 - val_loss: 520.9623\n",
            "Epoch 1203/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 370.8005 - val_loss: 522.7708\n",
            "Epoch 1204/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 363.7358 - val_loss: 491.7974\n",
            "Epoch 1205/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 366.5033 - val_loss: 500.3480\n",
            "Epoch 1206/2000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 368.1058 - val_loss: 583.1963\n",
            "Epoch 1207/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 463.7570 - val_loss: 482.3517\n",
            "Epoch 1208/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 406.3695 - val_loss: 506.8423\n",
            "Epoch 1209/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 350.3918 - val_loss: 475.5847\n",
            "Epoch 1210/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 349.9336 - val_loss: 520.7137\n",
            "Epoch 1211/2000\n",
            "80/80 [==============================] - 0s 869us/step - loss: 379.2926 - val_loss: 502.8291\n",
            "Epoch 1212/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 350.0525 - val_loss: 560.9135\n",
            "Epoch 1213/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 397.7820 - val_loss: 478.5629\n",
            "Epoch 1214/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 386.8330 - val_loss: 560.6056\n",
            "Epoch 1215/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 380.2451 - val_loss: 480.5112\n",
            "Epoch 1216/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 377.1250 - val_loss: 469.2494\n",
            "Epoch 1217/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 397.2356 - val_loss: 514.9282\n",
            "Epoch 1218/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 362.6424 - val_loss: 493.7710\n",
            "Epoch 1219/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 344.8160 - val_loss: 503.4243\n",
            "Epoch 1220/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 347.6586 - val_loss: 463.2451\n",
            "Epoch 1221/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 346.4993 - val_loss: 496.4656\n",
            "Epoch 1222/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 341.1812 - val_loss: 469.0094\n",
            "Epoch 1223/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 341.0112 - val_loss: 460.8500\n",
            "Epoch 1224/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 348.6994 - val_loss: 533.3885\n",
            "Epoch 1225/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 398.7685 - val_loss: 516.9032\n",
            "Epoch 1226/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 625.4328 - val_loss: 800.2131\n",
            "Epoch 1227/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 679.6454 - val_loss: 527.1490\n",
            "Epoch 1228/2000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 613.6003 - val_loss: 670.9757\n",
            "Epoch 1229/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 520.0396 - val_loss: 449.6072\n",
            "Epoch 1230/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 425.1428 - val_loss: 562.1385\n",
            "Epoch 1231/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 569.1159 - val_loss: 612.2245\n",
            "Epoch 1232/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 794.4813 - val_loss: 876.9507\n",
            "Epoch 1233/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 650.2904 - val_loss: 464.0482\n",
            "Epoch 1234/2000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 507.0309 - val_loss: 586.9719\n",
            "Epoch 1235/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 409.5682 - val_loss: 420.5250\n",
            "Epoch 1236/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 437.4278 - val_loss: 575.6503\n",
            "Epoch 1237/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 409.8476 - val_loss: 375.7882\n",
            "Epoch 1238/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 288.8418 - val_loss: 343.1507\n",
            "Epoch 1239/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 223.9794 - val_loss: 774.5692\n",
            "Epoch 1240/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 741.5015 - val_loss: 456.2578\n",
            "Epoch 1241/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 421.4463 - val_loss: 617.1847\n",
            "Epoch 1242/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 394.4483 - val_loss: 451.8657\n",
            "Epoch 1243/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 335.7461 - val_loss: 757.8821\n",
            "Epoch 1244/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 473.4001 - val_loss: 475.3199\n",
            "Epoch 1245/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 381.4909 - val_loss: 743.9521\n",
            "Epoch 1246/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 503.2847 - val_loss: 629.3480\n",
            "Epoch 1247/2000\n",
            "80/80 [==============================] - 0s 880us/step - loss: 604.9220 - val_loss: 790.3384\n",
            "Epoch 1248/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 500.0763 - val_loss: 445.8573\n",
            "Epoch 1249/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 351.2486 - val_loss: 597.7128\n",
            "Epoch 1250/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 358.0058 - val_loss: 468.1422\n",
            "Epoch 1251/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 426.2769 - val_loss: 639.7540\n",
            "Epoch 1252/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 398.7396 - val_loss: 533.8705\n",
            "Epoch 1253/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 555.7274 - val_loss: 799.4572\n",
            "Epoch 1254/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 662.1605 - val_loss: 491.5611\n",
            "Epoch 1255/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 469.8892 - val_loss: 631.3880\n",
            "Epoch 1256/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 496.1695 - val_loss: 547.1910\n",
            "Epoch 1257/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 604.2961 - val_loss: 760.5903\n",
            "Epoch 1258/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 492.1511 - val_loss: 455.6242\n",
            "Epoch 1259/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 426.3939 - val_loss: 450.6242\n",
            "Epoch 1260/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 306.3853 - val_loss: 407.0910\n",
            "Epoch 1261/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 349.4744 - val_loss: 416.2865\n",
            "Epoch 1262/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 298.8221 - val_loss: 412.0654\n",
            "Epoch 1263/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 295.8755 - val_loss: 442.9072\n",
            "Epoch 1264/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 346.7189 - val_loss: 424.4056\n",
            "Epoch 1265/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 370.3746 - val_loss: 514.9006\n",
            "Epoch 1266/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 336.4707 - val_loss: 396.6883\n",
            "Epoch 1267/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 298.9880 - val_loss: 514.0200\n",
            "Epoch 1268/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 343.8817 - val_loss: 395.2713\n",
            "Epoch 1269/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 341.0812 - val_loss: 394.2791\n",
            "Epoch 1270/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 305.9767 - val_loss: 396.7250\n",
            "Epoch 1271/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 289.0663 - val_loss: 421.8934\n",
            "Epoch 1272/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 301.6319 - val_loss: 387.1659\n",
            "Epoch 1273/2000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 308.5264 - val_loss: 385.9581\n",
            "Epoch 1274/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 293.5143 - val_loss: 397.0062\n",
            "Epoch 1275/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 284.2273 - val_loss: 391.5343\n",
            "Epoch 1276/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 292.7602 - val_loss: 437.5864\n",
            "Epoch 1277/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 349.1536 - val_loss: 385.3520\n",
            "Epoch 1278/2000\n",
            "80/80 [==============================] - 0s 856us/step - loss: 335.2063 - val_loss: 462.5475\n",
            "Epoch 1279/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 310.4605 - val_loss: 383.5452\n",
            "Epoch 1280/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 320.4680 - val_loss: 373.6657\n",
            "Epoch 1281/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 279.9243 - val_loss: 427.8953\n",
            "Epoch 1282/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 279.4726 - val_loss: 369.8701\n",
            "Epoch 1283/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 297.3321 - val_loss: 436.2291\n",
            "Epoch 1284/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 303.2888 - val_loss: 380.9068\n",
            "Epoch 1285/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 314.9025 - val_loss: 367.8834\n",
            "Epoch 1286/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 336.5510 - val_loss: 461.4913\n",
            "Epoch 1287/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 312.9194 - val_loss: 361.0315\n",
            "Epoch 1288/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 280.4776 - val_loss: 390.8745\n",
            "Epoch 1289/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 275.6100 - val_loss: 357.1368\n",
            "Epoch 1290/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 279.9224 - val_loss: 393.1351\n",
            "Epoch 1291/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 306.1387 - val_loss: 397.8688\n",
            "Epoch 1292/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 270.0447 - val_loss: 368.8170\n",
            "Epoch 1293/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 266.6831 - val_loss: 358.5742\n",
            "Epoch 1294/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 284.2630 - val_loss: 348.6195\n",
            "Epoch 1295/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 275.0214 - val_loss: 380.3992\n",
            "Epoch 1296/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 259.7671 - val_loss: 356.3506\n",
            "Epoch 1297/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 271.2673 - val_loss: 365.0316\n",
            "Epoch 1298/2000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 357.7220 - val_loss: 576.6229\n",
            "Epoch 1299/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 451.1523 - val_loss: 523.8264\n",
            "Epoch 1300/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 512.2238 - val_loss: 846.5240\n",
            "Epoch 1301/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 549.2353 - val_loss: 391.2241\n",
            "Epoch 1302/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 268.6496 - val_loss: 1094.9060\n",
            "Epoch 1303/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 849.4591 - val_loss: 745.7709\n",
            "Epoch 1304/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 813.7875 - val_loss: 799.7214\n",
            "Epoch 1305/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 580.6474 - val_loss: 372.5834\n",
            "Epoch 1306/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 639.6469 - val_loss: 333.9290\n",
            "Epoch 1307/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 746.1073 - val_loss: 620.4664\n",
            "Epoch 1308/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 967.4624 - val_loss: 632.8084\n",
            "Epoch 1309/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 644.8223 - val_loss: 919.2689\n",
            "Epoch 1310/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 618.0716 - val_loss: 693.4791\n",
            "Epoch 1311/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 627.4450 - val_loss: 1155.7458\n",
            "Epoch 1312/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 879.0243 - val_loss: 691.5424\n",
            "Epoch 1313/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 706.1891 - val_loss: 367.1459\n",
            "Epoch 1314/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 307.2558 - val_loss: 334.4625\n",
            "Epoch 1315/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 328.7435 - val_loss: 329.6620\n",
            "Epoch 1316/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 468.4643 - val_loss: 467.0770\n",
            "Epoch 1317/2000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 702.4352 - val_loss: 651.0513\n",
            "Epoch 1318/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 618.2068 - val_loss: 830.6315\n",
            "Epoch 1319/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 503.2572 - val_loss: 379.6404\n",
            "Epoch 1320/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 328.6976 - val_loss: 569.1421\n",
            "Epoch 1321/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 535.2729 - val_loss: 325.9555\n",
            "Epoch 1322/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 355.9844 - val_loss: 372.2055\n",
            "Epoch 1323/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 289.5964 - val_loss: 303.9234\n",
            "Epoch 1324/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 273.7264 - val_loss: 316.1425\n",
            "Epoch 1325/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 233.5740 - val_loss: 299.4174\n",
            "Epoch 1326/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 243.9508 - val_loss: 317.4825\n",
            "Epoch 1327/2000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 224.8038 - val_loss: 298.3369\n",
            "Epoch 1328/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 222.6128 - val_loss: 337.5251\n",
            "Epoch 1329/2000\n",
            "80/80 [==============================] - 0s 868us/step - loss: 226.2323 - val_loss: 299.1136\n",
            "Epoch 1330/2000\n",
            "80/80 [==============================] - 0s 871us/step - loss: 219.7171 - val_loss: 335.2131\n",
            "Epoch 1331/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 238.1185 - val_loss: 300.4460\n",
            "Epoch 1332/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 255.1404 - val_loss: 423.3555\n",
            "Epoch 1333/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 301.1646 - val_loss: 380.1672\n",
            "Epoch 1334/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 456.3543 - val_loss: 497.5243\n",
            "Epoch 1335/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 401.7116 - val_loss: 350.6408\n",
            "Epoch 1336/2000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 294.2129 - val_loss: 586.8500\n",
            "Epoch 1337/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 347.6455 - val_loss: 398.7688\n",
            "Epoch 1338/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 378.0648 - val_loss: 447.9802\n",
            "Epoch 1339/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 308.9720 - val_loss: 355.5710\n",
            "Epoch 1340/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 301.9697 - val_loss: 593.6300\n",
            "Epoch 1341/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 379.9327 - val_loss: 333.1994\n",
            "Epoch 1342/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 242.3671 - val_loss: 811.3282\n",
            "Epoch 1343/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 680.5797 - val_loss: 293.2388\n",
            "Epoch 1344/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 420.5901 - val_loss: 274.3597\n",
            "Epoch 1345/2000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 371.4156 - val_loss: 304.8752\n",
            "Epoch 1346/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 322.4547 - val_loss: 266.4382\n",
            "Epoch 1347/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 303.0410 - val_loss: 431.3776\n",
            "Epoch 1348/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 438.0324 - val_loss: 454.1140\n",
            "Epoch 1349/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 406.0371 - val_loss: 707.2518\n",
            "Epoch 1350/2000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 434.9382 - val_loss: 409.4981\n",
            "Epoch 1351/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 362.5986 - val_loss: 631.9064\n",
            "Epoch 1352/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 443.2826 - val_loss: 442.1104\n",
            "Epoch 1353/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 356.8234 - val_loss: 655.5203\n",
            "Epoch 1354/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 413.6094 - val_loss: 346.5830\n",
            "Epoch 1355/2000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 314.3245 - val_loss: 334.3830\n",
            "Epoch 1356/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 268.3041 - val_loss: 256.0805\n",
            "Epoch 1357/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 197.6029 - val_loss: 300.4273\n",
            "Epoch 1358/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 262.1637 - val_loss: 276.7890\n",
            "Epoch 1359/2000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 230.8569 - val_loss: 248.4685\n",
            "Epoch 1360/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 196.5340 - val_loss: 271.8232\n",
            "Epoch 1361/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 201.0686 - val_loss: 244.1093\n",
            "Epoch 1362/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 196.0227 - val_loss: 254.6939\n",
            "Epoch 1363/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 184.7955 - val_loss: 242.6307\n",
            "Epoch 1364/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 183.9135 - val_loss: 265.8264\n",
            "Epoch 1365/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 188.5769 - val_loss: 263.7193\n",
            "Epoch 1366/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 200.3882 - val_loss: 237.6022\n",
            "Epoch 1367/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 183.5057 - val_loss: 255.3930\n",
            "Epoch 1368/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 175.7314 - val_loss: 234.5234\n",
            "Epoch 1369/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 190.5479 - val_loss: 269.8100\n",
            "Epoch 1370/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 188.2278 - val_loss: 247.3125\n",
            "Epoch 1371/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 213.8718 - val_loss: 269.3464\n",
            "Epoch 1372/2000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 294.3542 - val_loss: 488.7341\n",
            "Epoch 1373/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 324.5145 - val_loss: 518.9565\n",
            "Epoch 1374/2000\n",
            "80/80 [==============================] - 0s 874us/step - loss: 497.8898 - val_loss: 693.9755\n",
            "Epoch 1375/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 418.3324 - val_loss: 453.9736\n",
            "Epoch 1376/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 405.5512 - val_loss: 615.8539\n",
            "Epoch 1377/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 662.2942 - val_loss: 222.3216\n",
            "Epoch 1378/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 551.2249 - val_loss: 319.6131\n",
            "Epoch 1379/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 1014.7229 - val_loss: 1368.7838\n",
            "Epoch 1380/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 1244.4849 - val_loss: 1212.7269\n",
            "Epoch 1381/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 1047.3042 - val_loss: 1635.4312\n",
            "Epoch 1382/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 1002.9210 - val_loss: 941.1461\n",
            "Epoch 1383/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 1099.7673 - val_loss: 840.6649\n",
            "Epoch 1384/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 842.2362 - val_loss: 225.1759\n",
            "Epoch 1385/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 767.3632 - val_loss: 336.2049\n",
            "Epoch 1386/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 766.8569 - val_loss: 619.3890\n",
            "Epoch 1387/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 603.4239 - val_loss: 459.2096\n",
            "Epoch 1388/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 498.4544 - val_loss: 885.2585\n",
            "Epoch 1389/2000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 680.5143 - val_loss: 550.9684\n",
            "Epoch 1390/2000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 534.3270 - val_loss: 445.3583\n",
            "Epoch 1391/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 379.0868 - val_loss: 212.4342\n",
            "Epoch 1392/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 290.0141 - val_loss: 212.6449\n",
            "Epoch 1393/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 341.6270 - val_loss: 262.8281\n",
            "Epoch 1394/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 337.3510 - val_loss: 234.7304\n",
            "Epoch 1395/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 214.0527 - val_loss: 426.6163\n",
            "Epoch 1396/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 301.3468 - val_loss: 304.0302\n",
            "Epoch 1397/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 309.2806 - val_loss: 377.2758\n",
            "Epoch 1398/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 257.9347 - val_loss: 199.5967\n",
            "Epoch 1399/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 207.6223 - val_loss: 232.9059\n",
            "Epoch 1400/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 178.5647 - val_loss: 217.3310\n",
            "Epoch 1401/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 154.5252 - val_loss: 218.6310\n",
            "Epoch 1402/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 158.1595 - val_loss: 199.1453\n",
            "Epoch 1403/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 153.8479 - val_loss: 206.5502\n",
            "Epoch 1404/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 182.1849 - val_loss: 237.4325\n",
            "Epoch 1405/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 247.0033 - val_loss: 372.4579\n",
            "Epoch 1406/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 405.7027 - val_loss: 468.4166\n",
            "Epoch 1407/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 310.1265 - val_loss: 253.0339\n",
            "Epoch 1408/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 207.2337 - val_loss: 411.7823\n",
            "Epoch 1409/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 245.9972 - val_loss: 345.2435\n",
            "Epoch 1410/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 396.5629 - val_loss: 214.2417\n",
            "Epoch 1411/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 309.6249 - val_loss: 205.8942\n",
            "Epoch 1412/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 295.3033 - val_loss: 296.1218\n",
            "Epoch 1413/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 412.9226 - val_loss: 436.0486\n",
            "Epoch 1414/2000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 330.8174 - val_loss: 244.7109\n",
            "Epoch 1415/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 271.6051 - val_loss: 190.7991\n",
            "Epoch 1416/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 184.0612 - val_loss: 211.7031\n",
            "Epoch 1417/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 157.3950 - val_loss: 177.5708\n",
            "Epoch 1418/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 158.2420 - val_loss: 311.7959\n",
            "Epoch 1419/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 200.6861 - val_loss: 173.4527\n",
            "Epoch 1420/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 153.4209 - val_loss: 185.4040\n",
            "Epoch 1421/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 133.7453 - val_loss: 176.2838\n",
            "Epoch 1422/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 165.9986 - val_loss: 398.3903\n",
            "Epoch 1423/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 361.7623 - val_loss: 386.0476\n",
            "Epoch 1424/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 332.5570 - val_loss: 413.4141\n",
            "Epoch 1425/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 383.6188 - val_loss: 170.4323\n",
            "Epoch 1426/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 269.4086 - val_loss: 173.9481\n",
            "Epoch 1427/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 271.3323 - val_loss: 454.3497\n",
            "Epoch 1428/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 363.6171 - val_loss: 412.6028\n",
            "Epoch 1429/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 383.1894 - val_loss: 445.3244\n",
            "Epoch 1430/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 255.5806 - val_loss: 280.8144\n",
            "Epoch 1431/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 229.3775 - val_loss: 429.1026\n",
            "Epoch 1432/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 239.8048 - val_loss: 252.9608\n",
            "Epoch 1433/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 320.3219 - val_loss: 167.1086\n",
            "Epoch 1434/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 182.2027 - val_loss: 194.6060\n",
            "Epoch 1435/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 307.2733 - val_loss: 289.4028\n",
            "Epoch 1436/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 277.6446 - val_loss: 430.4464\n",
            "Epoch 1437/2000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 249.0970 - val_loss: 217.4977\n",
            "Epoch 1438/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 240.3555 - val_loss: 224.4507\n",
            "Epoch 1439/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 236.5911 - val_loss: 163.7240\n",
            "Epoch 1440/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 193.7105 - val_loss: 167.4756\n",
            "Epoch 1441/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 179.9968 - val_loss: 400.9680\n",
            "Epoch 1442/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 289.6272 - val_loss: 290.1783\n",
            "Epoch 1443/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 297.5315 - val_loss: 251.1505\n",
            "Epoch 1444/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 231.7238 - val_loss: 153.8021\n",
            "Epoch 1445/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 152.8421 - val_loss: 164.9342\n",
            "Epoch 1446/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 154.2621 - val_loss: 166.4128\n",
            "Epoch 1447/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 144.5717 - val_loss: 146.0284\n",
            "Epoch 1448/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 129.6285 - val_loss: 219.0170\n",
            "Epoch 1449/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 130.7042 - val_loss: 182.0063\n",
            "Epoch 1450/2000\n",
            "80/80 [==============================] - 0s 872us/step - loss: 176.4307 - val_loss: 211.9120\n",
            "Epoch 1451/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 132.9234 - val_loss: 138.9137\n",
            "Epoch 1452/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 160.2943 - val_loss: 206.2191\n",
            "Epoch 1453/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 306.1992 - val_loss: 599.3480\n",
            "Epoch 1454/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 386.4736 - val_loss: 260.0956\n",
            "Epoch 1455/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 250.4008 - val_loss: 317.0231\n",
            "Epoch 1456/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 168.4803 - val_loss: 250.7452\n",
            "Epoch 1457/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 287.3760 - val_loss: 311.3957\n",
            "Epoch 1458/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 249.6195 - val_loss: 146.4768\n",
            "Epoch 1459/2000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 227.2409 - val_loss: 181.3215\n",
            "Epoch 1460/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 312.0965 - val_loss: 294.9583\n",
            "Epoch 1461/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 197.4037 - val_loss: 174.6176\n",
            "Epoch 1462/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 167.6832 - val_loss: 261.6238\n",
            "Epoch 1463/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 151.9066 - val_loss: 134.0863\n",
            "Epoch 1464/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 135.4958 - val_loss: 147.5557\n",
            "Epoch 1465/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 168.1477 - val_loss: 176.9157\n",
            "Epoch 1466/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 162.0875 - val_loss: 147.9833\n",
            "Epoch 1467/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 141.4056 - val_loss: 198.6796\n",
            "Epoch 1468/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 115.7386 - val_loss: 136.1177\n",
            "Epoch 1469/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 109.8360 - val_loss: 238.7024\n",
            "Epoch 1470/2000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 188.0908 - val_loss: 126.5911\n",
            "Epoch 1471/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 236.6123 - val_loss: 252.3353\n",
            "Epoch 1472/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 566.4515 - val_loss: 509.7316\n",
            "Epoch 1473/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 433.8771 - val_loss: 306.5894\n",
            "Epoch 1474/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 354.2115 - val_loss: 129.6123\n",
            "Epoch 1475/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 304.3070 - val_loss: 250.0712\n",
            "Epoch 1476/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 312.2307 - val_loss: 173.2658\n",
            "Epoch 1477/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 181.2791 - val_loss: 319.7459\n",
            "Epoch 1478/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 212.4671 - val_loss: 260.5675\n",
            "Epoch 1479/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 240.1355 - val_loss: 302.0749\n",
            "Epoch 1480/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 209.5061 - val_loss: 118.8349\n",
            "Epoch 1481/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 143.0873 - val_loss: 139.0134\n",
            "Epoch 1482/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 174.9630 - val_loss: 122.8034\n",
            "Epoch 1483/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 134.8013 - val_loss: 116.5582\n",
            "Epoch 1484/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 140.0430 - val_loss: 245.8793\n",
            "Epoch 1485/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 141.3633 - val_loss: 133.8605\n",
            "Epoch 1486/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 137.5079 - val_loss: 127.1805\n",
            "Epoch 1487/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 118.7976 - val_loss: 152.1028\n",
            "Epoch 1488/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 130.5137 - val_loss: 125.0491\n",
            "Epoch 1489/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 131.7867 - val_loss: 247.1887\n",
            "Epoch 1490/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 145.7237 - val_loss: 135.8637\n",
            "Epoch 1491/2000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 216.5299 - val_loss: 124.8370\n",
            "Epoch 1492/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 273.7463 - val_loss: 375.1115\n",
            "Epoch 1493/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 274.0517 - val_loss: 226.6942\n",
            "Epoch 1494/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 206.4317 - val_loss: 238.9426\n",
            "Epoch 1495/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 223.1210 - val_loss: 99.9187\n",
            "Epoch 1496/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 132.4615 - val_loss: 101.8545\n",
            "Epoch 1497/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 80.9355 - val_loss: 103.1132\n",
            "Epoch 1498/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 86.4073 - val_loss: 96.2339\n",
            "Epoch 1499/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 78.5355 - val_loss: 99.9042\n",
            "Epoch 1500/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 76.4246 - val_loss: 128.8186\n",
            "Epoch 1501/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 79.4704 - val_loss: 97.5436\n",
            "Epoch 1502/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 86.1158 - val_loss: 150.7181\n",
            "Epoch 1503/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 99.2009 - val_loss: 101.6097\n",
            "Epoch 1504/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 95.3443 - val_loss: 105.0584\n",
            "Epoch 1505/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 72.8248 - val_loss: 99.3124\n",
            "Epoch 1506/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 97.7309 - val_loss: 91.2754\n",
            "Epoch 1507/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 86.5535 - val_loss: 88.5565\n",
            "Epoch 1508/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 98.9088 - val_loss: 157.1867\n",
            "Epoch 1509/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 103.5171 - val_loss: 148.4599\n",
            "Epoch 1510/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 145.7048 - val_loss: 234.8328\n",
            "Epoch 1511/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 220.7221 - val_loss: 86.4742\n",
            "Epoch 1512/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 197.8459 - val_loss: 178.3248\n",
            "Epoch 1513/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 275.9668 - val_loss: 305.7220\n",
            "Epoch 1514/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 223.2255 - val_loss: 87.9494\n",
            "Epoch 1515/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 105.3114 - val_loss: 94.4217\n",
            "Epoch 1516/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 68.4490 - val_loss: 91.6783\n",
            "Epoch 1517/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 86.6757 - val_loss: 80.8891\n",
            "Epoch 1518/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 71.3004 - val_loss: 96.6131\n",
            "Epoch 1519/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 67.6209 - val_loss: 79.4194\n",
            "Epoch 1520/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 72.9032 - val_loss: 108.6757\n",
            "Epoch 1521/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 73.5447 - val_loss: 85.8239\n",
            "Epoch 1522/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 95.2684 - val_loss: 81.0902\n",
            "Epoch 1523/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 77.8899 - val_loss: 80.4568\n",
            "Epoch 1524/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 73.9016 - val_loss: 124.1446\n",
            "Epoch 1525/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 76.6975 - val_loss: 111.1080\n",
            "Epoch 1526/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 112.2496 - val_loss: 229.7082\n",
            "Epoch 1527/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 138.3865 - val_loss: 119.5136\n",
            "Epoch 1528/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 146.4246 - val_loss: 74.5233\n",
            "Epoch 1529/2000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 72.3213 - val_loss: 111.8319\n",
            "Epoch 1530/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 82.3844 - val_loss: 99.7055\n",
            "Epoch 1531/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 101.7133 - val_loss: 131.8406\n",
            "Epoch 1532/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 81.6600 - val_loss: 71.1508\n",
            "Epoch 1533/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 73.3081 - val_loss: 80.7282\n",
            "Epoch 1534/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 57.3804 - val_loss: 78.3373\n",
            "Epoch 1535/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 54.6326 - val_loss: 75.6043\n",
            "Epoch 1536/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 52.9241 - val_loss: 68.8979\n",
            "Epoch 1537/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 59.8905 - val_loss: 75.4325\n",
            "Epoch 1538/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 76.3753 - val_loss: 97.2859\n",
            "Epoch 1539/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 79.9651 - val_loss: 96.0469\n",
            "Epoch 1540/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 74.2373 - val_loss: 65.7488\n",
            "Epoch 1541/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 49.3600 - val_loss: 127.5331\n",
            "Epoch 1542/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 64.8035 - val_loss: 89.8924\n",
            "Epoch 1543/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 85.4431 - val_loss: 147.0694\n",
            "Epoch 1544/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 94.1541 - val_loss: 67.2180\n",
            "Epoch 1545/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 94.9108 - val_loss: 62.5841\n",
            "Epoch 1546/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 101.3136 - val_loss: 173.9470\n",
            "Epoch 1547/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 123.3655 - val_loss: 113.7798\n",
            "Epoch 1548/2000\n",
            "80/80 [==============================] - 0s 866us/step - loss: 140.4451 - val_loss: 61.7101\n",
            "Epoch 1549/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 118.9024 - val_loss: 202.4004\n",
            "Epoch 1550/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 139.9207 - val_loss: 113.5946\n",
            "Epoch 1551/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 106.7489 - val_loss: 97.0069\n",
            "Epoch 1552/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 101.3761 - val_loss: 91.1974\n",
            "Epoch 1553/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 69.4218 - val_loss: 67.3471\n",
            "Epoch 1554/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 69.4187 - val_loss: 120.2465\n",
            "Epoch 1555/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 66.7358 - val_loss: 57.9555\n",
            "Epoch 1556/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 58.6969 - val_loss: 83.7997\n",
            "Epoch 1557/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 65.5629 - val_loss: 59.8543\n",
            "Epoch 1558/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 50.6518 - val_loss: 57.9589\n",
            "Epoch 1559/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 46.7491 - val_loss: 67.6217\n",
            "Epoch 1560/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 45.8888 - val_loss: 55.3669\n",
            "Epoch 1561/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 40.3731 - val_loss: 94.6937\n",
            "Epoch 1562/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 75.9353 - val_loss: 82.8164\n",
            "Epoch 1563/2000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 102.3571 - val_loss: 91.1818\n",
            "Epoch 1564/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 220.3251 - val_loss: 100.5416\n",
            "Epoch 1565/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 183.5790 - val_loss: 382.3559\n",
            "Epoch 1566/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 601.8036 - val_loss: 1855.1332\n",
            "Epoch 1567/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 1316.0062 - val_loss: 2386.3042\n",
            "Epoch 1568/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 2627.1066 - val_loss: 1581.8569\n",
            "Epoch 1569/2000\n",
            "80/80 [==============================] - 0s 863us/step - loss: 3846.1002 - val_loss: 137.0141\n",
            "Epoch 1570/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 3428.2925 - val_loss: 2827.8042\n",
            "Epoch 1571/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 6385.5906 - val_loss: 9720.6211\n",
            "Epoch 1572/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 6453.6847 - val_loss: 2513.8164\n",
            "Epoch 1573/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 1619.1962 - val_loss: 1586.0555\n",
            "Epoch 1574/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 1811.4598 - val_loss: 464.7355\n",
            "Epoch 1575/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1752.4228 - val_loss: 1955.8174\n",
            "Epoch 1576/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5257.9357 - val_loss: 5750.4180\n",
            "Epoch 1577/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 5233.4212 - val_loss: 67.7085\n",
            "Epoch 1578/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 152.1620 - val_loss: 57.8529\n",
            "Epoch 1579/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 132.2396 - val_loss: 130.1090\n",
            "Epoch 1580/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 211.3499 - val_loss: 226.6745\n",
            "Epoch 1581/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 353.9006 - val_loss: 306.3023\n",
            "Epoch 1582/2000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 1153.0785 - val_loss: 1018.4559\n",
            "Epoch 1583/2000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 788.0776 - val_loss: 95.4837\n",
            "Epoch 1584/2000\n",
            "80/80 [==============================] - 0s 871us/step - loss: 12311.6958 - val_loss: 27386.3066\n",
            "Epoch 1585/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 20850.7224 - val_loss: 3347.3235\n",
            "Epoch 1586/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 11569.0361 - val_loss: 261.9373\n",
            "Epoch 1587/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 7051.1862 - val_loss: 2559.3423\n",
            "Epoch 1588/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 3519.1895 - val_loss: 2684.2366\n",
            "Epoch 1589/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 2138.7316 - val_loss: 1325.0422\n",
            "Epoch 1590/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 1062.3616 - val_loss: 765.0831\n",
            "Epoch 1591/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 701.9440 - val_loss: 723.0172\n",
            "Epoch 1592/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 803.8124 - val_loss: 579.7404\n",
            "Epoch 1593/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 688.2038 - val_loss: 51.0747\n",
            "Epoch 1594/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 596.9656 - val_loss: 227.7315\n",
            "Epoch 1595/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 464.2167 - val_loss: 459.8929\n",
            "Epoch 1596/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 342.9808 - val_loss: 147.2594\n",
            "Epoch 1597/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 132.4741 - val_loss: 201.7627\n",
            "Epoch 1598/2000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 93.7686 - val_loss: 53.6533\n",
            "Epoch 1599/2000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 87.0121 - val_loss: 82.4458\n",
            "Epoch 1600/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 84.9283 - val_loss: 46.1513\n",
            "Epoch 1601/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 52.9429 - val_loss: 56.5781\n",
            "Epoch 1602/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 50.2980 - val_loss: 53.8182\n",
            "Epoch 1603/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 44.7181 - val_loss: 44.6160\n",
            "Epoch 1604/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 60.2175 - val_loss: 82.6170\n",
            "Epoch 1605/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 62.5433 - val_loss: 61.8087\n",
            "Epoch 1606/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 78.5156 - val_loss: 45.4363\n",
            "Epoch 1607/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 45.5748 - val_loss: 51.1138\n",
            "Epoch 1608/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 45.1032 - val_loss: 42.9377\n",
            "Epoch 1609/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 45.4840 - val_loss: 43.5130\n",
            "Epoch 1610/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 40.4101 - val_loss: 57.8184\n",
            "Epoch 1611/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 47.5519 - val_loss: 42.4312\n",
            "Epoch 1612/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 34.6087 - val_loss: 105.6455\n",
            "Epoch 1613/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 74.4346 - val_loss: 94.6416\n",
            "Epoch 1614/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 100.3949 - val_loss: 87.9060\n",
            "Epoch 1615/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 57.9523 - val_loss: 41.9876\n",
            "Epoch 1616/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 41.3736 - val_loss: 130.7414\n",
            "Epoch 1617/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 59.5868 - val_loss: 55.7999\n",
            "Epoch 1618/2000\n",
            "80/80 [==============================] - 0s 877us/step - loss: 76.5680 - val_loss: 39.9098\n",
            "Epoch 1619/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 72.4046 - val_loss: 118.5777\n",
            "Epoch 1620/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 89.9850 - val_loss: 77.4470\n",
            "Epoch 1621/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 79.3174 - val_loss: 175.2956\n",
            "Epoch 1622/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 110.6493 - val_loss: 61.3034\n",
            "Epoch 1623/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 62.1173 - val_loss: 89.1958\n",
            "Epoch 1624/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 54.2364 - val_loss: 40.3743\n",
            "Epoch 1625/2000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 46.0823 - val_loss: 40.1469\n",
            "Epoch 1626/2000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 33.7816 - val_loss: 41.0139\n",
            "Epoch 1627/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 30.4999 - val_loss: 44.5535\n",
            "Epoch 1628/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 32.1159 - val_loss: 39.8813\n",
            "Epoch 1629/2000\n",
            "80/80 [==============================] - 0s 861us/step - loss: 31.1488 - val_loss: 37.1594\n",
            "Epoch 1630/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 31.1306 - val_loss: 43.3088\n",
            "Epoch 1631/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 30.1148 - val_loss: 39.3117\n",
            "Epoch 1632/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 29.9979 - val_loss: 43.1798\n",
            "Epoch 1633/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 30.1945 - val_loss: 40.9196\n",
            "Epoch 1634/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 31.5372 - val_loss: 46.2008\n",
            "Epoch 1635/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 52.7111 - val_loss: 81.1920\n",
            "Epoch 1636/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 53.9961 - val_loss: 34.8400\n",
            "Epoch 1637/2000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 40.6872 - val_loss: 34.6346\n",
            "Epoch 1638/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 53.1631 - val_loss: 62.0661\n",
            "Epoch 1639/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 44.0747 - val_loss: 36.0203\n",
            "Epoch 1640/2000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 35.3307 - val_loss: 49.5372\n",
            "Epoch 1641/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 29.9575 - val_loss: 34.0168\n",
            "Epoch 1642/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 33.8449 - val_loss: 34.0923\n",
            "Epoch 1643/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 37.4188 - val_loss: 36.3972\n",
            "Epoch 1644/2000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 28.6295 - val_loss: 46.5175\n",
            "Epoch 1645/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 29.5753 - val_loss: 42.2684\n",
            "Epoch 1646/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 43.8543 - val_loss: 86.0770\n",
            "Epoch 1647/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 56.5111 - val_loss: 35.4851\n",
            "Epoch 1648/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 62.0275 - val_loss: 47.7351\n",
            "Epoch 1649/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 75.8625 - val_loss: 107.0350\n",
            "Epoch 1650/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 73.8643 - val_loss: 60.8460\n",
            "Epoch 1651/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 69.1439 - val_loss: 51.2715\n",
            "Epoch 1652/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 48.6221 - val_loss: 33.3846\n",
            "Epoch 1653/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 33.8454 - val_loss: 30.2169\n",
            "Epoch 1654/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 35.4421 - val_loss: 37.4686\n",
            "Epoch 1655/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 27.2333 - val_loss: 31.4092\n",
            "Epoch 1656/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 29.0645 - val_loss: 85.6720\n",
            "Epoch 1657/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 59.2644 - val_loss: 38.1631\n",
            "Epoch 1658/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 41.4400 - val_loss: 37.7149\n",
            "Epoch 1659/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 61.6753 - val_loss: 49.3397\n",
            "Epoch 1660/2000\n",
            "80/80 [==============================] - 0s 864us/step - loss: 70.2731 - val_loss: 85.5261\n",
            "Epoch 1661/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 107.3477 - val_loss: 57.6109\n",
            "Epoch 1662/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 42.1513 - val_loss: 28.7458\n",
            "Epoch 1663/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 44.4239 - val_loss: 33.2644\n",
            "Epoch 1664/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 39.0602 - val_loss: 73.2513\n",
            "Epoch 1665/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 43.6247 - val_loss: 40.3711\n",
            "Epoch 1666/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 54.7952 - val_loss: 27.8220\n",
            "Epoch 1667/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 59.0624 - val_loss: 65.5793\n",
            "Epoch 1668/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 46.6047 - val_loss: 39.4197\n",
            "Epoch 1669/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 47.5475 - val_loss: 52.9126\n",
            "Epoch 1670/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 27.5116 - val_loss: 26.7275\n",
            "Epoch 1671/2000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 24.7869 - val_loss: 42.4355\n",
            "Epoch 1672/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 33.6596 - val_loss: 28.0325\n",
            "Epoch 1673/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 34.5387 - val_loss: 47.3831\n",
            "Epoch 1674/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 65.1275 - val_loss: 133.9128\n",
            "Epoch 1675/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 81.1615 - val_loss: 67.6176\n",
            "Epoch 1676/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 85.3694 - val_loss: 30.5402\n",
            "Epoch 1677/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 46.0825 - val_loss: 56.4140\n",
            "Epoch 1678/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 49.2441 - val_loss: 28.2794\n",
            "Epoch 1679/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 29.1909 - val_loss: 47.1747\n",
            "Epoch 1680/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 31.8938 - val_loss: 23.9600\n",
            "Epoch 1681/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 25.5213 - val_loss: 30.6188\n",
            "Epoch 1682/2000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 25.6117 - val_loss: 28.7403\n",
            "Epoch 1683/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 23.4709 - val_loss: 24.1050\n",
            "Epoch 1684/2000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 26.7225 - val_loss: 46.6073\n",
            "Epoch 1685/2000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 28.1257 - val_loss: 22.9298\n",
            "Epoch 1686/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 28.9849 - val_loss: 25.1487\n",
            "Epoch 1687/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 49.6355 - val_loss: 119.4402\n",
            "Epoch 1688/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 67.7365 - val_loss: 51.8344\n",
            "Epoch 1689/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 55.3624 - val_loss: 77.7984\n",
            "Epoch 1690/2000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 49.4142 - val_loss: 26.6379\n",
            "Epoch 1691/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 40.2124 - val_loss: 25.8754\n",
            "Epoch 1692/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 30.1508 - val_loss: 25.9953\n",
            "Epoch 1693/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 18.4400 - val_loss: 31.9034\n",
            "Epoch 1694/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 26.0671 - val_loss: 30.2703\n",
            "Epoch 1695/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 22.1640 - val_loss: 23.0758\n",
            "Epoch 1696/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 25.9359 - val_loss: 58.0054\n",
            "Epoch 1697/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 52.7618 - val_loss: 24.2036\n",
            "Epoch 1698/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 29.0243 - val_loss: 20.7009\n",
            "Epoch 1699/2000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 24.2868 - val_loss: 24.4850\n",
            "Epoch 1700/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 18.5796 - val_loss: 23.1369\n",
            "Epoch 1701/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 28.6911 - val_loss: 22.3948\n",
            "Epoch 1702/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 29.4067 - val_loss: 19.8475\n",
            "Epoch 1703/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 35.0478 - val_loss: 58.8716\n",
            "Epoch 1704/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 40.0229 - val_loss: 25.0852\n",
            "Epoch 1705/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23.1486 - val_loss: 59.1506\n",
            "Epoch 1706/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 34.3096 - val_loss: 23.0871\n",
            "Epoch 1707/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 38.8654 - val_loss: 24.6698\n",
            "Epoch 1708/2000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 49.9297 - val_loss: 66.6926\n",
            "Epoch 1709/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 40.0398 - val_loss: 24.4031\n",
            "Epoch 1710/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 35.0350 - val_loss: 19.0286\n",
            "Epoch 1711/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 22.5386 - val_loss: 32.7249\n",
            "Epoch 1712/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 20.3256 - val_loss: 19.1784\n",
            "Epoch 1713/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 20.6229 - val_loss: 35.7450\n",
            "Epoch 1714/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 21.6122 - val_loss: 18.2544\n",
            "Epoch 1715/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 17.2195 - val_loss: 19.3876\n",
            "Epoch 1716/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 15.2612 - val_loss: 23.4781\n",
            "Epoch 1717/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 16.9346 - val_loss: 23.2354\n",
            "Epoch 1718/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 17.7327 - val_loss: 17.4415\n",
            "Epoch 1719/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 18.0704 - val_loss: 17.0095\n",
            "Epoch 1720/2000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 19.8163 - val_loss: 21.0884\n",
            "Epoch 1721/2000\n",
            "80/80 [==============================] - 0s 880us/step - loss: 14.6998 - val_loss: 19.8124\n",
            "Epoch 1722/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 19.1733 - val_loss: 16.7724\n",
            "Epoch 1723/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 14.8132 - val_loss: 53.2221\n",
            "Epoch 1724/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 35.0911 - val_loss: 20.2003\n",
            "Epoch 1725/2000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 24.4716 - val_loss: 35.3935\n",
            "Epoch 1726/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 20.5653 - val_loss: 19.7636\n",
            "Epoch 1727/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 21.8354 - val_loss: 19.3820\n",
            "Epoch 1728/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 27.2789 - val_loss: 20.0907\n",
            "Epoch 1729/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26.6104 - val_loss: 32.1105\n",
            "Epoch 1730/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 30.7076 - val_loss: 17.8816\n",
            "Epoch 1731/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 21.6995 - val_loss: 16.4102\n",
            "Epoch 1732/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 21.0157 - val_loss: 28.7433\n",
            "Epoch 1733/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 25.0016 - val_loss: 22.5563\n",
            "Epoch 1734/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 35.5855 - val_loss: 15.4871\n",
            "Epoch 1735/2000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 35.0834 - val_loss: 88.8051\n",
            "Epoch 1736/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 76.9075 - val_loss: 98.5372\n",
            "Epoch 1737/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 145.0745 - val_loss: 29.3815\n",
            "Epoch 1738/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 116.2689 - val_loss: 183.7950\n",
            "Epoch 1739/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 182.2015 - val_loss: 209.7493\n",
            "Epoch 1740/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 180.0518 - val_loss: 40.9462\n",
            "Epoch 1741/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 69.7721 - val_loss: 64.2968\n",
            "Epoch 1742/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 108.6096 - val_loss: 121.6232\n",
            "Epoch 1743/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 106.2096 - val_loss: 113.4937\n",
            "Epoch 1744/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 179.1147 - val_loss: 32.4424\n",
            "Epoch 1745/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 154.4380 - val_loss: 180.9430\n",
            "Epoch 1746/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 203.1218 - val_loss: 195.3923\n",
            "Epoch 1747/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 126.3642 - val_loss: 44.5669\n",
            "Epoch 1748/2000\n",
            "80/80 [==============================] - 0s 865us/step - loss: 95.7568 - val_loss: 14.7459\n",
            "Epoch 1749/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 49.0056 - val_loss: 73.0425\n",
            "Epoch 1750/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 48.8361 - val_loss: 26.0224\n",
            "Epoch 1751/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 32.8031 - val_loss: 12.8549\n",
            "Epoch 1752/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 30.6924 - val_loss: 58.8462\n",
            "Epoch 1753/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 41.6998 - val_loss: 40.6891\n",
            "Epoch 1754/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 52.4813 - val_loss: 17.1844\n",
            "Epoch 1755/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 28.1577 - val_loss: 42.1616\n",
            "Epoch 1756/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 37.3964 - val_loss: 25.6165\n",
            "Epoch 1757/2000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 36.6722 - val_loss: 14.6857\n",
            "Epoch 1758/2000\n",
            "80/80 [==============================] - 0s 875us/step - loss: 30.7959 - val_loss: 50.4314\n",
            "Epoch 1759/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 29.8742 - val_loss: 15.7580\n",
            "Epoch 1760/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 20.6126 - val_loss: 15.9073\n",
            "Epoch 1761/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 12.8964 - val_loss: 11.5928\n",
            "Epoch 1762/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 12.5436 - val_loss: 13.1935\n",
            "Epoch 1763/2000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 13.7022 - val_loss: 28.5801\n",
            "Epoch 1764/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 16.9288 - val_loss: 21.8744\n",
            "Epoch 1765/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 21.3887 - val_loss: 56.2669\n",
            "Epoch 1766/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 39.4387 - val_loss: 11.0315\n",
            "Epoch 1767/2000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 24.0143 - val_loss: 11.6945\n",
            "Epoch 1768/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 26.7627 - val_loss: 44.4419\n",
            "Epoch 1769/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 30.1259 - val_loss: 11.1405\n",
            "Epoch 1770/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 19.7894 - val_loss: 10.7148\n",
            "Epoch 1771/2000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 13.3906 - val_loss: 32.9574\n",
            "Epoch 1772/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17.5627 - val_loss: 10.4241\n",
            "Epoch 1773/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 18.8558 - val_loss: 29.0824\n",
            "Epoch 1774/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 63.5114 - val_loss: 54.4921\n",
            "Epoch 1775/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 60.6356 - val_loss: 20.3941\n",
            "Epoch 1776/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 27.8447 - val_loss: 21.5563\n",
            "Epoch 1777/2000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 27.4439 - val_loss: 63.7695\n",
            "Epoch 1778/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 47.1708 - val_loss: 10.2831\n",
            "Epoch 1779/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 21.9140 - val_loss: 22.4876\n",
            "Epoch 1780/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 35.3832 - val_loss: 30.5198\n",
            "Epoch 1781/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 17.0542 - val_loss: 11.6348\n",
            "Epoch 1782/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 14.2916 - val_loss: 12.8377\n",
            "Epoch 1783/2000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 24.6353 - val_loss: 10.6044\n",
            "Epoch 1784/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 11.1541 - val_loss: 13.0548\n",
            "Epoch 1785/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 9.4211 - val_loss: 17.2012\n",
            "Epoch 1786/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 11.0722 - val_loss: 9.2845\n",
            "Epoch 1787/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 10.7054 - val_loss: 13.3385\n",
            "Epoch 1788/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 10.8146 - val_loss: 17.9209\n",
            "Epoch 1789/2000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 12.5131 - val_loss: 13.9761\n",
            "Epoch 1790/2000\n",
            "80/80 [==============================] - 0s 876us/step - loss: 16.5156 - val_loss: 25.0462\n",
            "Epoch 1791/2000\n",
            "80/80 [==============================] - 0s 860us/step - loss: 14.2284 - val_loss: 11.6981\n",
            "Epoch 1792/2000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 11.2358 - val_loss: 9.0714\n",
            "Epoch 1793/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 13.0629 - val_loss: 8.7681\n",
            "Epoch 1794/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 10.3114 - val_loss: 24.7435\n",
            "Epoch 1795/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 13.3658 - val_loss: 8.3686\n",
            "Epoch 1796/2000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 13.5721 - val_loss: 8.5642\n",
            "Epoch 1797/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 15.3559 - val_loss: 23.7395\n",
            "Epoch 1798/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 14.2988 - val_loss: 8.1287\n",
            "Epoch 1799/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 7.9807 - val_loss: 12.3791\n",
            "Epoch 1800/2000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 13.3712 - val_loss: 21.2402\n",
            "Epoch 1801/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 16.1173 - val_loss: 18.8217\n",
            "Epoch 1802/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 21.9349 - val_loss: 13.8622\n",
            "Epoch 1803/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 17.9428 - val_loss: 18.8392\n",
            "Epoch 1804/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 14.7013 - val_loss: 8.4162\n",
            "Epoch 1805/2000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 10.2443 - val_loss: 13.2811\n",
            "Epoch 1806/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 8.3666 - val_loss: 8.5621\n",
            "Epoch 1807/2000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 6.7295 - val_loss: 20.0445\n",
            "Epoch 1808/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 11.6049 - val_loss: 7.9131\n",
            "Epoch 1809/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 13.5125 - val_loss: 12.3652\n",
            "Epoch 1810/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 17.8272 - val_loss: 14.4689\n",
            "Epoch 1811/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15.4715 - val_loss: 14.1781\n",
            "Epoch 1812/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 13.9829 - val_loss: 11.9328\n",
            "Epoch 1813/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 14.5331 - val_loss: 21.7756\n",
            "Epoch 1814/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 19.3979 - val_loss: 13.0516\n",
            "Epoch 1815/2000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 14.3406 - val_loss: 10.3845\n",
            "Epoch 1816/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 10.6203 - val_loss: 47.1323\n",
            "Epoch 1817/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 22.8755 - val_loss: 9.5872\n",
            "Epoch 1818/2000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 25.7638 - val_loss: 11.8898\n",
            "Epoch 1819/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 25.1838 - val_loss: 49.2108\n",
            "Epoch 1820/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 31.0834 - val_loss: 13.2339\n",
            "Epoch 1821/2000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 20.3672 - val_loss: 9.6575\n",
            "Epoch 1822/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9.7498 - val_loss: 26.6066\n",
            "Epoch 1823/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 12.3541 - val_loss: 9.8996\n",
            "Epoch 1824/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 16.6434 - val_loss: 9.4114\n",
            "Epoch 1825/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 15.9107 - val_loss: 22.5462\n",
            "Epoch 1826/2000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 17.4609 - val_loss: 11.7042\n",
            "Epoch 1827/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 14.3211 - val_loss: 7.5984\n",
            "Epoch 1828/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 8.7141 - val_loss: 22.1419\n",
            "Epoch 1829/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 14.6500 - val_loss: 7.9852\n",
            "Epoch 1830/2000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 7.6499 - val_loss: 6.1525\n",
            "Epoch 1831/2000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 6.9283 - val_loss: 12.4183\n",
            "Epoch 1832/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 13.2613 - val_loss: 18.1957\n",
            "Epoch 1833/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 11.9582 - val_loss: 9.6290\n",
            "Epoch 1834/2000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 16.1602 - val_loss: 5.9081\n",
            "Epoch 1835/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 13.0158 - val_loss: 29.5953\n",
            "Epoch 1836/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 13.8665 - val_loss: 5.9328\n",
            "Epoch 1837/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 9.7326 - val_loss: 6.6660\n",
            "Epoch 1838/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 9.6658 - val_loss: 31.6149\n",
            "Epoch 1839/2000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 18.1633 - val_loss: 5.7946\n",
            "Epoch 1840/2000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 18.8233 - val_loss: 18.9455\n",
            "Epoch 1841/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 29.6590 - val_loss: 9.0891\n",
            "Epoch 1842/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 6.5507 - val_loss: 10.5136\n",
            "Epoch 1843/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 6.5503 - val_loss: 6.5414\n",
            "Epoch 1844/2000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 6.9929 - val_loss: 5.5220\n",
            "Epoch 1845/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.5070 - val_loss: 5.5069\n",
            "Epoch 1846/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 9.6910 - val_loss: 6.7494\n",
            "Epoch 1847/2000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 7.0487 - val_loss: 10.6161\n",
            "Epoch 1848/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9.5838 - val_loss: 21.1686\n",
            "Epoch 1849/2000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 11.2825 - val_loss: 5.1691\n",
            "Epoch 1850/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 8.3239 - val_loss: 5.1321\n",
            "Epoch 1851/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 7.5425 - val_loss: 5.1121\n",
            "Epoch 1852/2000\n",
            "80/80 [==============================] - 0s 878us/step - loss: 6.7961 - val_loss: 21.7397\n",
            "Epoch 1853/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 14.9328 - val_loss: 9.1746\n",
            "Epoch 1854/2000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 12.9761 - val_loss: 10.9492\n",
            "Epoch 1855/2000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 14.0046 - val_loss: 6.5534\n",
            "Epoch 1856/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 6.3321 - val_loss: 15.1558\n",
            "Epoch 1857/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 11.0106 - val_loss: 9.8556\n",
            "Epoch 1858/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 6.0465 - val_loss: 7.5491\n",
            "Epoch 1859/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 11.8581 - val_loss: 4.8902\n",
            "Epoch 1860/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 9.4507 - val_loss: 5.2944\n",
            "Epoch 1861/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 4.9402 - val_loss: 4.7234\n",
            "Epoch 1862/2000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 5.1923 - val_loss: 7.9021\n",
            "Epoch 1863/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 5.2944 - val_loss: 6.2103\n",
            "Epoch 1864/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 4.9528 - val_loss: 11.4165\n",
            "Epoch 1865/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 7.9205 - val_loss: 4.6003\n",
            "Epoch 1866/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4.8134 - val_loss: 9.7477\n",
            "Epoch 1867/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 7.7698 - val_loss: 12.9302\n",
            "Epoch 1868/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 7.0454 - val_loss: 4.5262\n",
            "Epoch 1869/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 6.4816 - val_loss: 5.5537\n",
            "Epoch 1870/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4.7145 - val_loss: 4.3926\n",
            "Epoch 1871/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 5.5506 - val_loss: 10.1468\n",
            "Epoch 1872/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 6.3650 - val_loss: 4.9836\n",
            "Epoch 1873/2000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 4.5574 - val_loss: 4.6145\n",
            "Epoch 1874/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 6.1221 - val_loss: 20.3169\n",
            "Epoch 1875/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 12.2208 - val_loss: 5.9815\n",
            "Epoch 1876/2000\n",
            "80/80 [==============================] - 0s 884us/step - loss: 4.2175 - val_loss: 5.7227\n",
            "Epoch 1877/2000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 4.1153 - val_loss: 6.1470\n",
            "Epoch 1878/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 4.5162 - val_loss: 4.5282\n",
            "Epoch 1879/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 4.6470 - val_loss: 4.2344\n",
            "Epoch 1880/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 5.9441 - val_loss: 5.0924\n",
            "Epoch 1881/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 6.5605 - val_loss: 15.8671\n",
            "Epoch 1882/2000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 9.2279 - val_loss: 7.1640\n",
            "Epoch 1883/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 6.3108 - val_loss: 11.7635\n",
            "Epoch 1884/2000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 19.1831 - val_loss: 6.9951\n",
            "Epoch 1885/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 13.1638 - val_loss: 38.2117\n",
            "Epoch 1886/2000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 21.1278 - val_loss: 8.5773\n",
            "Epoch 1887/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 15.9029 - val_loss: 11.4661\n",
            "Epoch 1888/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 14.9551 - val_loss: 22.1703\n",
            "Epoch 1889/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 31.4204 - val_loss: 55.0150\n",
            "Epoch 1890/2000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 44.7946 - val_loss: 4.9880\n",
            "Epoch 1891/2000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 17.9967 - val_loss: 18.9452\n",
            "Epoch 1892/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 25.6213 - val_loss: 7.2869\n",
            "Epoch 1893/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 21.8055 - val_loss: 43.5508\n",
            "Epoch 1894/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 27.5508 - val_loss: 4.2645\n",
            "Epoch 1895/2000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 17.7119 - val_loss: 28.9448\n",
            "Epoch 1896/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 31.5497 - val_loss: 4.2040\n",
            "Epoch 1897/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 17.2803 - val_loss: 26.8919\n",
            "Epoch 1898/2000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 15.2379 - val_loss: 8.1869\n",
            "Epoch 1899/2000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 9.9333 - val_loss: 4.5611\n",
            "Epoch 1900/2000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 9.4751 - val_loss: 12.6144\n",
            "Epoch 1901/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 14.7940 - val_loss: 20.8300\n",
            "Epoch 1902/2000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 10.2869 - val_loss: 8.2465\n",
            "Epoch 1903/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 5.0727 - val_loss: 6.2551\n",
            "Epoch 1904/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 3.4682 - val_loss: 3.9800\n",
            "Epoch 1905/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 7.2574 - val_loss: 9.1852\n",
            "Epoch 1906/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 19.0433 - val_loss: 6.4100\n",
            "Epoch 1907/2000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 4.4877 - val_loss: 9.2918\n",
            "Epoch 1908/2000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 5.4163 - val_loss: 5.6237\n",
            "Epoch 1909/2000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 4.1474 - val_loss: 3.6038\n",
            "Epoch 1910/2000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 6.6776 - val_loss: 6.2933\n",
            "Epoch 1911/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 8.9241 - val_loss: 24.6120\n",
            "Epoch 1912/2000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 18.5510 - val_loss: 12.5277\n",
            "Epoch 1913/2000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 10.0922 - val_loss: 5.2535\n",
            "Epoch 1914/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 9.9231 - val_loss: 4.3229\n",
            "Epoch 1915/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.9609 - val_loss: 6.3074\n",
            "Epoch 1916/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 4.8542 - val_loss: 4.7744\n",
            "Epoch 1917/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 3.1374 - val_loss: 3.5737\n",
            "Epoch 1918/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 3.8123 - val_loss: 4.3193\n",
            "Epoch 1919/2000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 4.6113 - val_loss: 4.1035\n",
            "Epoch 1920/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 3.1320 - val_loss: 3.9305\n",
            "Epoch 1921/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 3.0857 - val_loss: 4.5607\n",
            "Epoch 1922/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 3.0645 - val_loss: 3.3517\n",
            "Epoch 1923/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 3.4606 - val_loss: 3.7181\n",
            "Epoch 1924/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 3.7648 - val_loss: 7.5941\n",
            "Epoch 1925/2000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 6.0158 - val_loss: 3.5811\n",
            "Epoch 1926/2000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 8.4759 - val_loss: 6.7822\n",
            "Epoch 1927/2000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 7.6311 - val_loss: 6.0541\n",
            "Epoch 1928/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.8885 - val_loss: 16.4157\n",
            "Epoch 1929/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 8.9872 - val_loss: 3.7416\n",
            "Epoch 1930/2000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 7.7462 - val_loss: 5.9136\n",
            "Epoch 1931/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 7.8141 - val_loss: 3.9177\n",
            "Epoch 1932/2000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 4.0048 - val_loss: 13.7795\n",
            "Epoch 1933/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 5.6975 - val_loss: 3.6204\n",
            "Epoch 1934/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.7070 - val_loss: 17.9276\n",
            "Epoch 1935/2000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 19.7441 - val_loss: 14.0209\n",
            "Epoch 1936/2000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 8.4105 - val_loss: 10.0839\n",
            "Epoch 1937/2000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 4.5691 - val_loss: 3.1590\n",
            "Epoch 1938/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 3.8347 - val_loss: 5.3098\n",
            "Epoch 1939/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.4673 - val_loss: 3.0096\n",
            "Epoch 1940/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 4.7363 - val_loss: 3.0795\n",
            "Epoch 1941/2000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 3.3345 - val_loss: 10.0544\n",
            "Epoch 1942/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 5.0559 - val_loss: 4.0301\n",
            "Epoch 1943/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 4.2264 - val_loss: 8.3335\n",
            "Epoch 1944/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 16.7782 - val_loss: 13.2125\n",
            "Epoch 1945/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23.3331 - val_loss: 5.9813\n",
            "Epoch 1946/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 9.3132 - val_loss: 12.0069\n",
            "Epoch 1947/2000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 5.2107 - val_loss: 3.1432\n",
            "Epoch 1948/2000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 4.0196 - val_loss: 4.3798\n",
            "Epoch 1949/2000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 7.3398 - val_loss: 3.2185\n",
            "Epoch 1950/2000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 6.3824 - val_loss: 12.7833\n",
            "Epoch 1951/2000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 11.1064 - val_loss: 12.0615\n",
            "Epoch 1952/2000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 14.0174 - val_loss: 10.7192\n",
            "Epoch 1953/2000\n",
            "80/80 [==============================] - 0s 882us/step - loss: 18.7996 - val_loss: 3.4489\n",
            "Epoch 1954/2000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 10.0833 - val_loss: 14.6160\n",
            "Epoch 1955/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 14.3557 - val_loss: 19.5692\n",
            "Epoch 1956/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 21.2476 - val_loss: 2.7814\n",
            "Epoch 1957/2000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 14.6466 - val_loss: 10.3433\n",
            "Epoch 1958/2000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 15.4102 - val_loss: 5.5692\n",
            "Epoch 1959/2000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 11.9148 - val_loss: 4.0279\n",
            "Epoch 1960/2000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 7.3602 - val_loss: 13.1329\n",
            "Epoch 1961/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 9.4167 - val_loss: 17.4421\n",
            "Epoch 1962/2000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 10.4478 - val_loss: 2.9413\n",
            "Epoch 1963/2000\n",
            "80/80 [==============================] - 0s 883us/step - loss: 9.2222 - val_loss: 12.8278\n",
            "Epoch 1964/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 18.4080 - val_loss: 10.0664\n",
            "Epoch 1965/2000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 20.0977 - val_loss: 16.3461\n",
            "Epoch 1966/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 13.6815 - val_loss: 23.2738\n",
            "Epoch 1967/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 10.8946 - val_loss: 2.7631\n",
            "Epoch 1968/2000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 4.9794 - val_loss: 2.9887\n",
            "Epoch 1969/2000\n",
            "80/80 [==============================] - 0s 873us/step - loss: 2.5282 - val_loss: 5.5315\n",
            "Epoch 1970/2000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 3.6095 - val_loss: 11.2176\n",
            "Epoch 1971/2000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 6.1026 - val_loss: 5.1119\n",
            "Epoch 1972/2000\n",
            "80/80 [==============================] - 0s 853us/step - loss: 3.5413 - val_loss: 9.7859\n",
            "Epoch 1973/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 12.8931 - val_loss: 2.9910\n",
            "Epoch 1974/2000\n",
            "80/80 [==============================] - 0s 872us/step - loss: 6.1127 - val_loss: 28.7811\n",
            "Epoch 1975/2000\n",
            "80/80 [==============================] - 0s 897us/step - loss: 21.7391 - val_loss: 13.2034\n",
            "Epoch 1976/2000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 13.3387 - val_loss: 17.6840\n",
            "Epoch 1977/2000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 17.1533 - val_loss: 3.2038\n",
            "Epoch 1978/2000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 11.6246 - val_loss: 6.7104\n",
            "Epoch 1979/2000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 4.4808 - val_loss: 12.4142\n",
            "Epoch 1980/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 9.1002 - val_loss: 7.0408\n",
            "Epoch 1981/2000\n",
            "80/80 [==============================] - 0s 901us/step - loss: 3.6884 - val_loss: 2.7125\n",
            "Epoch 1982/2000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 3.2910 - val_loss: 2.8592\n",
            "Epoch 1983/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.3108 - val_loss: 12.4542\n",
            "Epoch 1984/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.0457 - val_loss: 11.6893\n",
            "Epoch 1985/2000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 5.5216 - val_loss: 4.2818\n",
            "Epoch 1986/2000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 5.1625 - val_loss: 3.0188\n",
            "Epoch 1987/2000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 3.1416 - val_loss: 3.3997\n",
            "Epoch 1988/2000\n",
            "80/80 [==============================] - 0s 885us/step - loss: 2.5239 - val_loss: 3.2063\n",
            "Epoch 1989/2000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 4.9939 - val_loss: 2.7052\n",
            "Epoch 1990/2000\n",
            "80/80 [==============================] - 0s 878us/step - loss: 3.3904 - val_loss: 15.3964\n",
            "Epoch 1991/2000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 11.7640 - val_loss: 17.7463\n",
            "Epoch 1992/2000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 28.8861 - val_loss: 6.2942\n",
            "Epoch 1993/2000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 37.0637 - val_loss: 40.7789\n",
            "Epoch 1994/2000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 43.6727 - val_loss: 4.6136\n",
            "Epoch 1995/2000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 8.0804 - val_loss: 15.9090\n",
            "Epoch 1996/2000\n",
            "80/80 [==============================] - 0s 865us/step - loss: 9.5598 - val_loss: 5.7486\n",
            "Epoch 1997/2000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 3.6580 - val_loss: 3.0996\n",
            "Epoch 1998/2000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 3.8011 - val_loss: 2.8802\n",
            "Epoch 1999/2000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 2.5130 - val_loss: 6.1549\n",
            "Epoch 2000/2000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 4.1565 - val_loss: 12.8588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCh_x4H_kzmc",
        "colab_type": "code",
        "outputId": "fa349e0e-7136-49a1-e1fc-f4611b29a7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "source": [
        "results = model.predict(x_test)\n",
        "results.shape\n",
        "y_test.shape\n",
        "plt.scatter(range(20),results,c='r')\n",
        "plt.scatter(range(20),y_test,c='g')\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEDCAYAAAAyZm/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGcFJREFUeJzt3XuQXGd95vHv07rYWHZsyRq8Xt3N\nagEn4EumbLM4YLIgyy5ikcsmUtggiCktKZwNm72UWapsSvyxIVTCLovBaMOUIQUy4eKNUiWwtZjg\nJCDQyBG2JV80CIOlUqzBMrZjG8uj+e0f523paKbPTM9M3/TO86nq6u73nNP9m9OjZ47e8/Z7FBGY\nmdnsUet2AWZm1lkOfjOzWcbBb2Y2yzj4zcxmGQe/mdks4+A3M5tlejb4JQ1IOiLpoSbW/bikPen2\nmKSfdaJGM7PTkXp1HL+kNwH/DHw+In5pCtv9IXBZRPx+24ozMzuN9ewRf0TcBxwtt0l6laRvSNot\n6e8kvabBphuArR0p0szsNDS32wVM0RbgfRGxX9KVwKeAX60vlLQCWAXc26X6zMx63mkT/JLOBv4N\n8GVJ9eYzxqy2HvhKRBzvZG1mZqeT0yb4KbqlfhYRl06wznrg/R2qx8zstNSzffxjRcSzwI8k/TsA\nFS6pL0/9/QuB73apRDOz00LPBr+krRQh/mpJByXdCLwTuFHSD4C9wLrSJuuBO6NXhymZmfWInh3O\naWZm7dGzR/xmZtYePXlyd/HixbFy5cpul2FmdtrYvXv3TyOir5l1ezL4V65cyeDgYLfLMDM7bUj6\ncbPrTtrVI2mZpG9J2idpr6Q/arCOJH1C0pCkByRdXlq2UdL+dNvY/I9hZmbt0MwR/wjwnyPifknn\nALsl7YiIfaV1rgNWp9uVwKeBKyUtAm4F+oFI226LiKdb+lOYmVnTJj3ij4jDEXF/evwc8DCwZMxq\n6ygmU4uI2AmcJ+lC4FpgR0QcTWG/A1jb0p/AzMymZEqjeiStBC4Dvjdm0RLgidLzg6mtqr3Ra2+S\nNChpcHh4eCplmZnZFDQd/GmunK8CH0jfom2piNgSEf0R0d/X19SJaTMzm4amgl/SPIrQ/0JEfK3B\nKoeAZaXnS1NbVbuZmXVJM6N6BHwWeDgi/rxitW3Au9LonquAZyLiMHA3sEbSQkkLgTWpzczMuqSZ\nUT1vBH4PeFDSntT234HlABFxO7AduB4YAl4A3pOWHZX0EWBX2m5zRJxycZVWGR0N/tc397P6grN5\n++v/ZTvewswsC5MGf0T8PaBJ1gkqpkOOiAFgYFrVTcFzPx/hi9//CUefP8bis8/gqovOb/dbmpmd\nlrKZq+fcs+bx5f/wBo6PBt969Ei3yzEz61nZBD/AysULuKhvAT/+6QvdLsXMrGdlFfwAKxadxY+P\nOvjNzKrkF/znL+AnTz2PrzNgZtZYdsG/dOEreP7YcZ59caTbpZiZ9aTsgv+cM4uBSs+99HKXKzEz\n603ZBf+CM4rgf+HY8S5XYmbWm/IL/vlF8D//krt6zMwayS74588tfqSXj/vkrplZI9kF/7w59eAf\n7XIlZma9KcPgL2aXOObgNzNrKMPgT0f8Iw5+M7NG8g1+9/GbmTWUYfAXXT3u4zczayzD4PfJXTOz\niWQX/B7OaWY2sUkvxCJpAHg7cCQifqnB8v8KvLP0eq8F+tLVtx4HngOOAyMR0d+qwqv4iN/MbGLN\nHPHfAaytWhgRH4uISyPiUuCDwLfHXF7xLWl520Mf3MdvZjaZSYM/Iu4Dmr1O7gZg64wqmqH6Eb/H\n8ZuZNdayPn5JZ1H8z+CrpeYA7pG0W9KmVr3XRE6O43cfv5lZI5P28U/BrwH/MKab5+qIOCTplcAO\nSY+k/0GMk/4wbAJYvnz5tIuYUxM1uavHzKxKK0f1rGdMN09EHEr3R4C7gCuqNo6ILRHRHxH9fX19\nMypkTk0c9xW4zMwaaknwSzoXeDPw16W2BZLOqT8G1gAPteL9mqgH576ZWWPNDOfcClwDLJZ0ELgV\nmAcQEben1X4duCcini9tegFwl6T6+3wxIr7RutInqLmorRNvZWZ22pk0+CNiQxPr3EEx7LPcdgC4\nZLqFzURNwrFvZtZYdt/cBZBgdNTRb2bWSJbB7yN+M7NqWQa/BKPu4zczayjP4AeP6jEzq5Bl8Ndq\n8qgeM7MKWQa/AJ/bNTNrLMvgL07uOvnNzBrJMviLk7vdrsLMrDdlGvyessHMrEqewY+nbDAzq5Jl\n8Nd8xG9mVinT4PcXuMzMqmQZ/JJ8ctfMrEKmwY+Hc5qZVcg3+J37ZmYNZRn8xcldJ7+ZWSNZBr+n\nbDAzqzZp8EsakHREUsPr5Uq6RtIzkvak2y2lZWslPSppSNLNrSx8Ip6P38ysWjNH/HcAaydZ5+8i\n4tJ02wwgaQ5wG3AdcDGwQdLFMym2aR7OaWZWadLgj4j7gKPTeO0rgKGIOBARx4A7gXXTeJ0pqxXD\neszMrIFW9fG/QdIPJH1d0i+mtiXAE6V1Dqa2hiRtkjQoaXB4eHhGxfgLXGZm1VoR/PcDKyLiEuB/\nA/93Oi8SEVsioj8i+vv6+mZUkJCD38yswoyDPyKejYh/To+3A/MkLQYOActKqy5NbW3ncfxmZtVm\nHPyS/oUkpcdXpNd8CtgFrJa0StJ8YD2wbabv12RNHs5pZlZh7mQrSNoKXAMslnQQuBWYBxARtwO/\nBfyBpBHgRWB9FN+eGpF0E3A3MAcYiIi9bfkpxqgJfHbXzKyxSYM/IjZMsvyTwCcrlm0Htk+vtOnz\nFbjMzKpl+c1dT9lgZlYty+D3lA1mZtXyDH55OKeZWZUsg784uWtmZo1kGfw+4jczq5Zl8Nf8BS4z\ns0pZBr+nbDAzq5Zn8PuI38yskoPfzGyWyTL4iytwOfnNzBrJMvg9ZYOZWbUsg7/m4ZxmZpWyDH5J\n7uM3M6uQZ/CDJ2kzM6uQZfDXfK11M7NKWQa/p2wwM6s2afBLGpB0RNJDFcvfKekBSQ9K+o6kS0rL\nHk/teyQNtrLwiXjKBjOzas0c8d8BrJ1g+Y+AN0fE64CPAFvGLH9LRFwaEf3TK3E6fM1dM7MqzVx6\n8T5JKydY/p3S053A0pmXNTPFEb+T38yskVb38d8IfL30PIB7JO2WtGmiDSVtkjQoaXB4eHhGRdQ8\nnNPMrNKkR/zNkvQWiuC/utR8dUQckvRKYIekRyLivkbbR8QWUjdRf3//jGK7+Oauk9/MrJGWHPFL\nej3wF8C6iHiq3h4Rh9L9EeAu4IpWvN9kirl6zMyskRkHv6TlwNeA34uIx0rtCySdU38MrAEajgxq\nOR/xm5lVmrSrR9JW4BpgsaSDwK3APICIuB24BTgf+JQkgJE0gucC4K7UNhf4YkR8ow0/wzg1+Rtc\nZmZVmhnVs2GS5e8F3tug/QBwyfgt2k/4iN/MrEqW39z1lA1mZtWyDH5P2WBmVi3T4PeUDWZmVbIM\nfn+By8ysWpbB75O7ZmbVsgx+H/GbmVXLMvg9ZYOZWbVMg99TNpiZVck0+D0ts5lZlSyD31fgMjOr\nlmXwC3+By8ysSpbB7ykbzMyqZRn8khj1RXfNzBrKNPjdx29mViXL4PcVuMzMqmUZ/J6ywcysWlPB\nL2lA0hFJDS+dqMInJA1JekDS5aVlGyXtT7eNrSp8IrWap2wwM6vS7BH/HcDaCZZfB6xOt03ApwEk\nLaK4VOOVFBdav1XSwukW2ywf8ZuZVWsq+CPiPuDoBKusAz4fhZ3AeZIuBK4FdkTE0Yh4GtjBxH9A\nWsJTNpiZVWtVH/8S4InS84Opraq9rTxlg5lZtZ45uStpk6RBSYPDw8Mzei1P2WBmVq1VwX8IWFZ6\nvjS1VbWPExFbIqI/Ivr7+vpmVEzN19w1M6vUquDfBrwrje65CngmIg4DdwNrJC1MJ3XXpLa2Kk7u\ntvtdzMxOT3ObWUnSVuAaYLGkgxQjdeYBRMTtwHbgemAIeAF4T1p2VNJHgF3ppTZHxEQniVtCEun9\nTzw2M7NCU8EfERsmWR7A+yuWDQADUy9t+upZH3HysZmZFXrm5G4r1epH/F2uw8ysF2UZ/PWDfJ/g\nNTMbL8vgr9XqffxdLsTMrAdlGfx1PuI3Mxsvy+Cv+YyumVmlTIO/uPcRv5nZeFkGv04Ef3frMDPr\nRVkGf630BS4zMztVlsFf5yN+M7Pxsgz+Eyd3HfxmZuNkGfzyyV0zs0pZBr+nbDAzq5Zl8PuI38ys\nWqbB7ykbzMyqZBn8tRPTMjv5zczGyjL4lebn9HBOM7Pxsgz+E0f8Pr1rZjZOU8Evaa2kRyUNSbq5\nwfKPS9qTbo9J+llp2fHSsm2tLL663uLeR/xmZuNNeulFSXOA24C3AQeBXZK2RcS++joR8Z9K6/8h\ncFnpJV6MiEtbV/Lk5CkbzMwqNXPEfwUwFBEHIuIYcCewboL1NwBbW1HcdNUnZXbum5mN10zwLwGe\nKD0/mNrGkbQCWAXcW2o+U9KgpJ2S3lH1JpI2pfUGh4eHmyirWv0LXB7Hb2Y2XqtP7q4HvhIRx0tt\nKyKiH/hd4H9KelWjDSNiS0T0R0R/X1/fjIqo1eqvOaOXMTPLUjPBfwhYVnq+NLU1sp4x3TwRcSjd\nHwD+llP7/9vi5HBOJ7+Z2VjNBP8uYLWkVZLmU4T7uNE5kl4DLAS+W2pbKOmM9Hgx8EZg39htW82T\nc5qZVZt0VE9EjEi6CbgbmAMMRMReSZuBwYio/xFYD9wZpw6leS3wGUmjFH9k/qQ8GqhdPKrHzKza\npMEPEBHbge1j2m4Z8/zDDbb7DvC6GdQ3LSenbOj0O5uZ9b4sv7nrKRvMzKplGfyessHMrFqWwV/v\n4x8d7XIhZmY9KNPgL+49nNPMbLwsg//ExdbNzGycLIO/Hvs+4jczGy/L4PeUDWZm1bIMfnmSNjOz\nSnkGf7r3OH4zs/GyDP6TJ3ed/GZmY2UZ/L70oplZtSyDv3ZikrYuF2Jm1oOyDH4P5zQzq5Zn8HtU\nj5lZpSyDv+Zzu2ZmlbIM/pNH/F0uxMysBzUV/JLWSnpU0pCkmxssf7ekYUl70u29pWUbJe1Pt42t\nLL6Kp2U2M6s26RW4JM0BbgPeBhwEdkna1uASil+KiJvGbLsIuBXop+h42Z22fbol1VfWXNz7iN/M\nbLxmjvivAIYi4kBEHAPuBNY1+frXAjsi4mgK+x3A2umV2jxfc9fMrFozwb8EeKL0/GBqG+s3JT0g\n6SuSlk1xWyRtkjQoaXB4eLiJsqp5HL+ZWbVWndz9G2BlRLye4qj+c1N9gYjYEhH9EdHf19c3o2I8\njt/MrFozwX8IWFZ6vjS1nRART0XES+npXwC/3Oy27eAjfjOzas0E/y5gtaRVkuYD64Ft5RUkXVh6\negPwcHp8N7BG0kJJC4E1qa2tfOlFM7Nqk47qiYgRSTdRBPYcYCAi9kraDAxGxDbgP0q6ARgBjgLv\nTtselfQRij8eAJsj4mgbfo5T6MRwTjMzG2vS4AeIiO3A9jFtt5QefxD4YMW2A8DADGqcsppH9ZiZ\nVcr0m7vFvcfxm5mNl2Xw++SumVm1LIPfwznNzKrlGfz1I/4u12Fm1osyDf7i3id3zczGyzL43cdv\nZlYt0+Av7t3Hb2Y2XpbBL3whFjOzKnkGv/v4zcwqZR783a3DzKwXZRn8J07uekCnmdk4WQe/+/jN\nzMbLMvg9LbOZWbWsg9+5b2Y2Xp7Bj6dlNjOrkmXw13whFjOzSk0Fv6S1kh6VNCTp5gbL/1jSPkkP\nSPqmpBWlZccl7Um3bWO3bYcTJ3d9dtfMbJxJr8AlaQ5wG/A24CCwS9K2iNhXWu0fgf6IeEHSHwB/\nCvxOWvZiRFza4ronqbm4d+6bmY3XzBH/FcBQRByIiGPAncC68goR8a2IeCE93QksbW2ZU+Npmc3M\nqjUT/EuAJ0rPD6a2KjcCXy89P1PSoKSdkt4xjRqnzFM2mJlVa+pi682S9O+BfuDNpeYVEXFI0kXA\nvZIejIgfNth2E7AJYPny5TOqw9Mym5lVa+aI/xCwrPR8aWo7haS3Ah8CboiIl+rtEXEo3R8A/ha4\nrNGbRMSWiOiPiP6+vr6mf4BGfOlFM7NqzQT/LmC1pFWS5gPrgVNG50i6DPgMRegfKbUvlHRGerwY\neCNQPincFjX38ZuZVZq0qyciRiTdBNwNzAEGImKvpM3AYERsAz4GnA18OZ1Y/UlE3AC8FviMpFGK\nPzJ/MmY0UFt4ygYzs2pN9fFHxHZg+5i2W0qP31qx3XeA182kwOnwlA1mZtUy/eaup2wwM6uSZfCf\nPLnb1TLMzHpSlsHv4ZxmZtWyDH6f3DUzq5Zp8LuP38ysSpbBD8XUzI59M7Pxsg1+Se7qMTNrINvg\nr8knd83MGsk2+IU8nNPMrIF8g18Q7uU3Mxsn2+B/aWSUz3z7QLfLMDPrOdkGv5mZNebgNzObZRz8\nZmazTPbBP3J8tNslmJn1lOyD/+cjDn4zs7L8g//l490uwcyspzQV/JLWSnpU0pCkmxssP0PSl9Ly\n70laWVr2wdT+qKRrW1d6cxz8ZmanmjT4Jc0BbgOuAy4GNki6eMxqNwJPR8S/Aj4OfDRtezHFxdl/\nEVgLfCq9Xtv96mteCcDmv9nHked+zjF3+ZiZAc1dc/cKYCgiDgBIuhNYB5Qvmr4O+HB6/BXgkyrm\nRl4H3BkRLwE/kjSUXu+7rSm/2jWv7uPeR45wz74nuWffk0iw6vwFp6zz0sgo8+boxDTO0zWzrVvz\nIi+9PMr8ubXpvUxLfgBrtZl+LDP9ve6EsVOnP/PiCKMRnPeKedRqvV9/qy06az5/9b43tP19mgn+\nJcATpecHgSur1omIEUnPAOen9p1jtl3S6E0kbQI2ASxfvryZ2if02/3LePjwcxwbGeX+nzzNskVn\nce4r5p2yzssjo8ybblgmrZgUohXXDQhOXnms0+9trTfjT+V0+lhLv7YRMauvoHfOmc1E8sx15l2a\nEBFbgC0A/f39M/7Iz5w3h//xG6+bcV1mZrlp5uTuIWBZ6fnS1NZwHUlzgXOBp5rc1szMOqiZ4N8F\nrJa0StJ8ipO128assw3YmB7/FnBvFH0I24D1adTPKmA18P3WlG5mZtMxaVdP6rO/CbgbmAMMRMRe\nSZuBwYjYBnwW+Mt08vYoxR8H0np/RXEieAR4f0R4fKWZWRepF0/u9ff3x+DgYLfLMDM7bUjaHRH9\nzayb/Td3zczsVA5+M7NZxsFvZjbLOPjNzGaZnjy5K2kY+PE0N18M/LSF5bSK65oa1zU1rmtqcqxr\nRUT0NbNiTwb/TEgabPbMdie5rqlxXVPjuqZmttflrh4zs1nGwW9mNsvkGPxbul1ABdc1Na5ralzX\n1MzqurLr4zczs4nleMRvZmYTcPCbmc0y2QT/ZBeEb/N7L5P0LUn7JO2V9Eep/cOSDknak27Xl7bp\nyEXoJT0u6cH0/oOpbZGkHZL2p/uFqV2SPpHqekDS5W2q6dWlfbJH0rOSPtCt/SVpQNIRSQ+V2qa8\njyRtTOvvl7Sx0Xu1oK6PSXokvfddks5L7SslvVjad7eXtvnl9DswlGqf0TUNK+qa8mfX6n+zFXV9\nqVTT45L2pPaO7K8JsqG7v18RcdrfKKaL/iFwETAf+AFwcQff/0Lg8vT4HOAxigvTfxj4Lw3WvzjV\neAawKtU+p021PQ4sHtP2p8DN6fHNwEfT4+uBr1NcDO8q4Hsd+uz+CVjRrf0FvAm4HHhouvsIWAQc\nSPcL0+OFbahrDTA3Pf5oqa6V5fXGvM73U61KtV/Xhrqm9Nm1499so7rGLP8z4JZO7q8JsqGrv1+5\nHPGfuCB8RBwD6heE74iIOBwR96fHzwEPU3Ft4eTERegj4kdA/SL0nbIO+Fx6/DngHaX2z0dhJ3Ce\npAvbXMu/BX4YERN9U7ut+ysi7qO4jsTY95zKProW2BERRyPiaWAHsLbVdUXEPRExkp7upLiqXaVU\n2y9ExM4oEuTzpZ+lZXVNoOqza/m/2YnqSkftvw1sneg1Wr2/JsiGrv5+5RL8jS4IP1Hwto2klcBl\nwPdS003pv2wD9f/O0dl6A7hH0m4VF7QHuCAiDqfH/wRc0IW66tZz6j/Gbu+vuqnuo27U+PsUR4d1\nqyT9o6RvS/qV1LYk1dKJuqby2XV6f/0K8GRE7C+1dXR/jcmGrv5+5RL8PUHS2cBXgQ9ExLPAp4FX\nAZcChyn+q9lpV0fE5cB1wPslvam8MB3VdGVMr4pLed4AfDk19cL+Gqeb+6iKpA9RXNXuC6npMLA8\nIi4D/hj4oqRf6GBJPfnZlWzg1AOMju6vBtlwQjd+v3IJ/q5f1F3SPIoP9gsR8TWAiHgyIo5HxCjw\nfzjZPdGxeiPiULo/AtyVaniy3oWT7o90uq7kOuD+iHgy1dj1/VUy1X3UsRolvRt4O/DOFBqkrpSn\n0uPdFP3n/zrVUO4Oaktd0/jsOrm/5gK/AXypVG/H9lejbKDLv1+5BH8zF4Rvm9R/+Fng4Yj481J7\nuX/814H6aIOOXIRe0gJJ59QfU5wYfCi9f31UwEbgr0t1vSuNLLgKeKb039F2OOUorNv7a4yp7qO7\ngTWSFqZujjWpraUkrQX+G3BDRLxQau+TNCc9vohiHx1ItT0r6ar0e/qu0s/Syrqm+tl18t/sW4FH\nIuJEF06n9ldVNtDt36/pnhXutRvF2fDHKP5yf6jD7301xX/VHgD2pNv1wF8CD6b2bcCFpW0+lGp9\nlBmOspigrosoRkv8ANhb3y/A+cA3gf3A/wMWpXYBt6W6HgT627jPFgBPAeeW2rqyvyj++BwGXqbo\nO71xOvuIos99KN3e06a6hij6euu/Z7endX8zfcZ7gPuBXyu9Tj9FEP8Q+CTpG/strmvKn12r/802\nqiu13wG8b8y6HdlfVGdDV3+/PGWDmdksk0tXj5mZNcnBb2Y2yzj4zcxmGQe/mdks4+A3M5tlHPxm\nZrOMg9/MbJb5/xXcczYNBbGTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGUO1yHzll4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}