{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15MI424.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwkDYZlKKd7M",
        "colab_type": "code",
        "outputId": "21185aad-de90-4e34-b59c-4ec81a32199d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import math\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8P7R0wBK4G2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=[]\n",
        "Y1=[]\n",
        "for i in range(100):\n",
        "  X.append(i)\n",
        "  Y1.append(math.sin(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0V0FVCBLyMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y2=[]\n",
        "for i in range(100):\n",
        "  Y2.append(i*3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGoORbOENY7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "inppt = np.column_stack((Y1, Y2))\n",
        "tstt = [Y1*Y2 for Y1,Y2 in zip(Y1,Y2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP5mFr44NgR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inppt = np.array(inppt, dtype=\"float32\")\n",
        "trgt = np.array(tstt, dtype=\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNHZUjYoPX71",
        "colab_type": "code",
        "outputId": "481d9960-a679-49c0-a8dc-48efc35bc5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "inppt= np.array(inppt).reshape(100, 2,1)\n",
        "inppt.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmrwEw6DkEWc",
        "colab_type": "code",
        "outputId": "a7d0a0b6-a3dc-432a-da36-7eb74d46dfd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trgt.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medTU78GkGQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(inppt,trgt,test_size=0.2,random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C8wYedGkNQP",
        "colab_type": "code",
        "outputId": "da7e7b1f-73bb-456c-b2df-9064e6707a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(2,1)))\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(25, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 2, 200)            161600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 2, 100)            120400    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 2, 50)             30200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 25)                7600      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                520       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 320,541\n",
            "Trainable params: 320,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhEyK56qkdZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d91d59b7-0ddc-4cd8-e872-b485129768a8"
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs=1000,validation_data=(x_test,y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "80/80 [==============================] - 3s 39ms/step - loss: 16519.3809 - val_loss: 8973.3076\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16518.9074 - val_loss: 8975.3389\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 16518.1430 - val_loss: 8978.7295\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 16517.5090 - val_loss: 8981.4277\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16515.9820 - val_loss: 8985.4316\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16514.0914 - val_loss: 8994.2490\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16508.7246 - val_loss: 9006.0801\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16503.7629 - val_loss: 9028.9043\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16496.2844 - val_loss: 9068.7139\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 16475.4414 - val_loss: 9112.4688\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 16456.5289 - val_loss: 9177.5957\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 16424.6676 - val_loss: 9246.4824\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 16390.2340 - val_loss: 9392.1982\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 16316.8828 - val_loss: 9490.5000\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 16223.8352 - val_loss: 9692.1113\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 16058.7473 - val_loss: 9631.0068\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 15758.4305 - val_loss: 9551.7344\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 15377.0000 - val_loss: 9571.9785\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 14556.4633 - val_loss: 11390.5498\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 14566.8133 - val_loss: 10683.3721\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 13064.8438 - val_loss: 8243.9775\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11530.7881 - val_loss: 10412.9180\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 9882.5039 - val_loss: 7397.5781\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8382.8130 - val_loss: 6862.9766\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7400.7170 - val_loss: 6800.0054\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 7001.6319 - val_loss: 6263.2114\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6703.6523 - val_loss: 5786.7769\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5868.2159 - val_loss: 5036.3696\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 4848.4782 - val_loss: 4230.2715\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 3741.3140 - val_loss: 2965.9810\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 2464.9612 - val_loss: 1920.8386\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 1266.1719 - val_loss: 1118.4364\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 627.8447 - val_loss: 535.7465\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 478.9146 - val_loss: 381.9019\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 291.7606 - val_loss: 474.3251\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 321.8564 - val_loss: 495.0014\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 320.6881 - val_loss: 386.2579\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 271.9361 - val_loss: 791.5895\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 497.1858 - val_loss: 269.2027\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 215.5516 - val_loss: 319.9283\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 0s 890us/step - loss: 220.2097 - val_loss: 194.0175\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 174.0208 - val_loss: 121.1019\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 213.9946 - val_loss: 597.0966\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 666.4341 - val_loss: 1029.7861\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 852.5690 - val_loss: 1201.7582\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 0s 879us/step - loss: 641.8414 - val_loss: 359.4884\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 546.3258 - val_loss: 502.9336\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 366.6895 - val_loss: 330.0269\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 388.4726 - val_loss: 336.0109\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 240.9815 - val_loss: 369.8451\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 182.0764 - val_loss: 278.3687\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 173.2126 - val_loss: 211.9173\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 247.4432 - val_loss: 354.2151\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 158.4373 - val_loss: 242.8540\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 152.9027 - val_loss: 308.5925\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 90.3002 - val_loss: 74.7648\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 52.4880 - val_loss: 56.9292\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 43.6617 - val_loss: 71.6529\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 39.5472 - val_loss: 64.0511\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 36.3244 - val_loss: 66.6412\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 51.3976 - val_loss: 61.5008\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 82.2267 - val_loss: 124.7792\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 72.6800 - val_loss: 143.4734\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 64.3623 - val_loss: 157.2825\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 58.4847 - val_loss: 141.3990\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 80.1252 - val_loss: 83.4821\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 73.5918 - val_loss: 86.6789\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 68.6979 - val_loss: 141.6473\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 36.9183 - val_loss: 92.6297\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 43.7616 - val_loss: 123.2271\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 41.4137 - val_loss: 48.2313\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 35.9912 - val_loss: 111.7549\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 33.8607 - val_loss: 37.6822\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 20.6182 - val_loss: 66.7903\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 20.9357 - val_loss: 60.6741\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 29.7372 - val_loss: 45.9741\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 19.5676 - val_loss: 69.5075\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 30.4575 - val_loss: 39.1054\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 16.2386 - val_loss: 42.3471\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 24.8133 - val_loss: 25.1987\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 16.7496 - val_loss: 47.1678\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14.7887 - val_loss: 36.2770\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9.2381 - val_loss: 39.7446\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.8711 - val_loss: 68.2725\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 12.3600 - val_loss: 34.6723\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14.1357 - val_loss: 53.1077\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17.0732 - val_loss: 39.1566\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.0610 - val_loss: 30.2101\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7.4542 - val_loss: 44.7914\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.0054 - val_loss: 30.3022\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 8.0636 - val_loss: 39.2676\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 4.3636 - val_loss: 26.1211\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 10.4344 - val_loss: 24.1660\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 5.4390 - val_loss: 21.4645\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 4.4615 - val_loss: 23.3185\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4.2476 - val_loss: 23.4766\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.9878 - val_loss: 29.2693\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.8905 - val_loss: 28.1111\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.1748 - val_loss: 31.4923\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6286 - val_loss: 32.9669\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6607 - val_loss: 32.7893\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.7589 - val_loss: 28.3694\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6674 - val_loss: 23.6841\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.0038 - val_loss: 32.4647\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.9306 - val_loss: 28.2555\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 2.1331 - val_loss: 25.2542\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 2.3939 - val_loss: 33.7555\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 2.5844 - val_loss: 26.6892\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 2.5235 - val_loss: 28.1498\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 2.5461 - val_loss: 31.4263\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 2.3260 - val_loss: 25.8511\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 2.7111 - val_loss: 34.9841\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 3.5056 - val_loss: 25.7605\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 2.1619 - val_loss: 28.3785\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 2.7375 - val_loss: 33.0652\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 5.9131 - val_loss: 23.1969\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 18.4107 - val_loss: 72.4655\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 21.5312 - val_loss: 41.1368\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 36.9164 - val_loss: 41.3078\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 25.1152 - val_loss: 28.0100\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 17.3167 - val_loss: 33.4351\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 20.1436 - val_loss: 56.7520\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 8.7778 - val_loss: 44.8711\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 25.0491 - val_loss: 72.5725\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 27.8390 - val_loss: 36.8034\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 30.9862 - val_loss: 88.7636\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 51.1077 - val_loss: 36.9658\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 22.3870 - val_loss: 59.8439\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 37.3046 - val_loss: 65.2855\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 23.0443 - val_loss: 111.3949\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 36.1581 - val_loss: 59.9994\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 48.0026 - val_loss: 28.9984\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 47.5058 - val_loss: 49.2659\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 44.1726 - val_loss: 49.0011\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 38.7048 - val_loss: 82.9884\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 20.4244 - val_loss: 66.6522\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 39.3890 - val_loss: 62.2412\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 54.4467 - val_loss: 180.0761\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 38.6043 - val_loss: 69.3484\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 43.2902 - val_loss: 111.8129\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 75.0083 - val_loss: 134.6387\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 98.0577 - val_loss: 206.3844\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 236.3781 - val_loss: 179.5722\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 95.5094 - val_loss: 108.0880\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 96.6203 - val_loss: 101.8590\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 51.6913 - val_loss: 80.9949\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 51.3334 - val_loss: 153.3979\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 30.6673 - val_loss: 83.5577\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 39.1102 - val_loss: 107.3596\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 40.6284 - val_loss: 76.5842\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 30.4127 - val_loss: 50.9731\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 34.3098 - val_loss: 31.1546\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 18.8756 - val_loss: 46.3191\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15.4981 - val_loss: 31.1497\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 17.1084 - val_loss: 24.8937\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 18.1915 - val_loss: 55.8207\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 6.6774 - val_loss: 23.8875\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.2140 - val_loss: 30.7080\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7.4803 - val_loss: 36.9327\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11.9549 - val_loss: 28.3565\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 13.5235 - val_loss: 72.1230\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 22.8069 - val_loss: 30.6033\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 15.1846 - val_loss: 43.7564\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 19.0379 - val_loss: 69.1444\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 22.4656 - val_loss: 60.8268\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 26.3623 - val_loss: 94.4329\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 19.6346 - val_loss: 56.4141\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 34.6727 - val_loss: 62.5170\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 23.6724 - val_loss: 53.0478\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 23.7896 - val_loss: 66.6168\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 23.5345 - val_loss: 78.6465\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 21.0420 - val_loss: 55.3845\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 25.5484 - val_loss: 55.7065\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 14.9118 - val_loss: 46.6850\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 22.0572 - val_loss: 65.6259\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 25.3995 - val_loss: 41.9647\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 27.5052 - val_loss: 43.8423\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 27.4734 - val_loss: 80.8763\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 22.5229 - val_loss: 51.6914\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 20.1410 - val_loss: 42.1862\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 22.0668 - val_loss: 40.9033\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 19.0735 - val_loss: 61.6157\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15.3209 - val_loss: 39.1440\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9.5874 - val_loss: 26.6615\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.3090 - val_loss: 32.0282\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 9.3517 - val_loss: 44.5105\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 11.1899 - val_loss: 40.2432\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 12.5653 - val_loss: 49.8470\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 14.0159 - val_loss: 34.1070\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 10.5382 - val_loss: 32.3799\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 10.4548 - val_loss: 45.0949\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 9.3225 - val_loss: 30.3695\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.1095 - val_loss: 30.3332\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 6.6874 - val_loss: 27.9441\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 5.4562 - val_loss: 27.8843\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 5.9975 - val_loss: 33.7127\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 3.4000 - val_loss: 28.3254\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 3.3144 - val_loss: 30.6948\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 2.5410 - val_loss: 30.9272\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4.0945 - val_loss: 30.1694\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.0955 - val_loss: 38.8527\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 7.2890 - val_loss: 37.5976\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 10.3599 - val_loss: 32.0637\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 5.5521 - val_loss: 38.7003\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 6.3815 - val_loss: 36.1755\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 7.4841 - val_loss: 50.3414\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 5.7459 - val_loss: 31.7478\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.5324 - val_loss: 32.2976\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11.7362 - val_loss: 44.9024\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17.3461 - val_loss: 41.9560\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 19.7068 - val_loss: 60.1992\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 16.9422 - val_loss: 37.8362\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 21.5027 - val_loss: 59.5339\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 51.8819 - val_loss: 114.5255\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 33.5216 - val_loss: 162.5014\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 67.1823 - val_loss: 147.1878\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 85.7166 - val_loss: 128.7384\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 96.8286 - val_loss: 238.6866\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 124.1519 - val_loss: 83.5650\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 33.4319 - val_loss: 81.3136\n",
            "Epoch 221/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 43.2210 - val_loss: 172.7873\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 48.0906 - val_loss: 125.7237\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 73.2170 - val_loss: 62.4479\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 35.0800 - val_loss: 67.2433\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 26.4875 - val_loss: 105.9639\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 38.7874 - val_loss: 63.3209\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 16.8785 - val_loss: 54.5637\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 13.5623 - val_loss: 44.1789\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 8.2315 - val_loss: 41.6440\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 8.3022 - val_loss: 38.4410\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 4.8282 - val_loss: 32.3060\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 4.5929 - val_loss: 28.2081\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4.3271 - val_loss: 24.8007\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.4965 - val_loss: 20.3365\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6274 - val_loss: 26.1892\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 3.4486 - val_loss: 23.3132\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 4.8233 - val_loss: 36.3751\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 2.9922 - val_loss: 28.4749\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 2.2081 - val_loss: 31.5638\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.5832 - val_loss: 23.6095\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 4.9087 - val_loss: 25.9751\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 2.9661 - val_loss: 27.5949\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 1.8563 - val_loss: 33.5500\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 3.4558 - val_loss: 33.2484\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 8.5831 - val_loss: 28.9124\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 6.0453 - val_loss: 45.4644\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 12.7423 - val_loss: 43.9243\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 32.7870 - val_loss: 55.3148\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14.3105 - val_loss: 50.7525\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 35.7689 - val_loss: 64.0010\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 61.1104 - val_loss: 189.5584\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 90.3696 - val_loss: 159.7253\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 102.8108 - val_loss: 173.2645\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 62.6877 - val_loss: 90.6819\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 50.1767 - val_loss: 85.2445\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 64.7752 - val_loss: 99.7027\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 79.6538 - val_loss: 76.1866\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 40.7086 - val_loss: 58.8923\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 48.8272 - val_loss: 43.6969\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 25.2488 - val_loss: 70.6279\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 13.3368 - val_loss: 42.2143\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 12.8620 - val_loss: 42.0316\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 15.4939 - val_loss: 39.1923\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 14.6480 - val_loss: 28.3815\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 10.3662 - val_loss: 51.0409\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 9.4674 - val_loss: 46.4349\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 5.4351 - val_loss: 43.2907\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 5.9987 - val_loss: 29.4943\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 4.9459 - val_loss: 36.6799\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 5.3513 - val_loss: 18.7684\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7.2640 - val_loss: 36.7104\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 8.1907 - val_loss: 41.0597\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 8.0975 - val_loss: 23.2713\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 3.4141 - val_loss: 71.3158\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 9.9689 - val_loss: 21.0425\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 14.4644 - val_loss: 25.4363\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 21.0872 - val_loss: 52.6575\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 13.7654 - val_loss: 34.0132\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 19.2157 - val_loss: 97.7325\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 22.5099 - val_loss: 46.5385\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 23.5131 - val_loss: 25.3907\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 14.5877 - val_loss: 70.2034\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 21.6680 - val_loss: 24.0172\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 19.7453 - val_loss: 29.5203\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 8.4245 - val_loss: 32.3592\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 15.9821 - val_loss: 37.4565\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 17.2038 - val_loss: 63.9126\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 11.6405 - val_loss: 43.4130\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 21.0765 - val_loss: 68.6972\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 12.9983 - val_loss: 37.8866\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 13.7872 - val_loss: 29.8562\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 9.7991 - val_loss: 80.3898\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 22.4530 - val_loss: 48.3597\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 33.4969 - val_loss: 77.4689\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 25.5515 - val_loss: 42.1768\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.1362 - val_loss: 53.4984\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.7211 - val_loss: 45.0856\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4.2422 - val_loss: 36.6211\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 3.6703 - val_loss: 64.2533\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 6.4641 - val_loss: 38.2902\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 5.0548 - val_loss: 41.6939\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 6.7955 - val_loss: 34.1309\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 4.4726 - val_loss: 30.2043\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 4.8732 - val_loss: 37.6316\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.7375 - val_loss: 31.8918\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 5.6255 - val_loss: 55.8979\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 5.6004 - val_loss: 45.5011\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 9.0851 - val_loss: 58.8173\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 6.8608 - val_loss: 35.0500\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 4.5442 - val_loss: 41.9259\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 7.5616 - val_loss: 59.2246\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 0s 887us/step - loss: 6.6766 - val_loss: 43.8248\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 14.3424 - val_loss: 56.5201\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 7.1039 - val_loss: 42.8564\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 5.7306 - val_loss: 43.6875\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 6.6806 - val_loss: 36.0518\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 3.8701 - val_loss: 40.9956\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 5.7145 - val_loss: 41.5109\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 3.1280 - val_loss: 31.2104\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 3.0070 - val_loss: 52.0970\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 5.9617 - val_loss: 36.1869\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 3.6138 - val_loss: 41.8908\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 5.6553 - val_loss: 54.7795\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 4.5095 - val_loss: 38.5486\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 2.2053 - val_loss: 41.5199\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 1.9756 - val_loss: 40.7179\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 2.7202 - val_loss: 41.9553\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 1.1985 - val_loss: 39.0922\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1.0068 - val_loss: 42.9515\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 1.5115 - val_loss: 33.4836\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.9189 - val_loss: 35.7316\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 2.2048 - val_loss: 43.2749\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 0.8697 - val_loss: 39.8007\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 1.6056 - val_loss: 47.5334\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 1.9035 - val_loss: 43.1708\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 0.8935 - val_loss: 35.5883\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1.3344 - val_loss: 40.9808\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 0.6424 - val_loss: 34.4373\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 0.8540 - val_loss: 37.6995\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 0.6128 - val_loss: 40.7193\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 0.6316 - val_loss: 35.8463\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.7058 - val_loss: 39.4354\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 0.3628 - val_loss: 38.6114\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 0.3812 - val_loss: 35.1364\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.8350 - val_loss: 44.2428\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 1.1971 - val_loss: 35.4807\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 0.7197 - val_loss: 38.1968\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 1.3367 - val_loss: 48.9772\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 2.4347 - val_loss: 35.5772\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 2.4659 - val_loss: 36.9730\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 0s 888us/step - loss: 1.2284 - val_loss: 41.2590\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 1.2027 - val_loss: 30.2845\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.8121 - val_loss: 40.8087\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 0.8977 - val_loss: 34.2739\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 0.6583 - val_loss: 34.8521\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 0.5126 - val_loss: 38.5354\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.3597 - val_loss: 33.8890\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 0.4169 - val_loss: 34.9154\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 0.4723 - val_loss: 42.4767\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 0.9702 - val_loss: 31.7830\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7998 - val_loss: 37.3061\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 0.9405 - val_loss: 39.9625\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.9850 - val_loss: 32.6842\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 0.8293 - val_loss: 37.5603\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.8526 - val_loss: 37.1400\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.9575 - val_loss: 28.1166\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.8138 - val_loss: 56.7083\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.0766 - val_loss: 35.5218\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 3.6778 - val_loss: 32.9196\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.1674 - val_loss: 48.5042\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 3.4188 - val_loss: 36.8278\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6163 - val_loss: 55.5944\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.1086 - val_loss: 40.6325\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.9814 - val_loss: 28.4009\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6491 - val_loss: 84.8679\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 10.7896 - val_loss: 33.1209\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14.9408 - val_loss: 50.6110\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 13.2918 - val_loss: 98.2464\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23.5048 - val_loss: 51.3386\n",
            "Epoch 380/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 10.4632 - val_loss: 47.8009\n",
            "Epoch 381/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 10.5247 - val_loss: 54.0482\n",
            "Epoch 382/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 13.7487 - val_loss: 25.3562\n",
            "Epoch 383/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 14.6601 - val_loss: 16.0418\n",
            "Epoch 384/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 12.4026 - val_loss: 60.5847\n",
            "Epoch 385/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 12.8325 - val_loss: 28.5196\n",
            "Epoch 386/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 9.4379 - val_loss: 35.2011\n",
            "Epoch 387/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 10.8323 - val_loss: 40.5933\n",
            "Epoch 388/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.1478 - val_loss: 37.3004\n",
            "Epoch 389/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 9.9728 - val_loss: 33.3188\n",
            "Epoch 390/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 7.7303 - val_loss: 31.7404\n",
            "Epoch 391/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 10.0085 - val_loss: 37.4344\n",
            "Epoch 392/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 4.0960 - val_loss: 41.8237\n",
            "Epoch 393/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 4.2514 - val_loss: 52.7948\n",
            "Epoch 394/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 7.2455 - val_loss: 37.7913\n",
            "Epoch 395/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16.8959 - val_loss: 31.8642\n",
            "Epoch 396/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 13.1639 - val_loss: 91.2148\n",
            "Epoch 397/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 42.3504 - val_loss: 43.9012\n",
            "Epoch 398/1000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 31.4203 - val_loss: 37.4494\n",
            "Epoch 399/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 29.5710 - val_loss: 70.7105\n",
            "Epoch 400/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 34.4408 - val_loss: 105.8089\n",
            "Epoch 401/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 47.4597 - val_loss: 106.5461\n",
            "Epoch 402/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 50.8521 - val_loss: 54.7168\n",
            "Epoch 403/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 35.1695 - val_loss: 59.7283\n",
            "Epoch 404/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 39.9387 - val_loss: 129.8684\n",
            "Epoch 405/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 64.1996 - val_loss: 208.7695\n",
            "Epoch 406/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 104.8968 - val_loss: 87.5322\n",
            "Epoch 407/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 78.0686 - val_loss: 66.2739\n",
            "Epoch 408/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 54.5515 - val_loss: 76.7622\n",
            "Epoch 409/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 32.9230 - val_loss: 104.8934\n",
            "Epoch 410/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 56.0600 - val_loss: 81.5515\n",
            "Epoch 411/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 90.4520 - val_loss: 39.3180\n",
            "Epoch 412/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 67.3159 - val_loss: 77.8815\n",
            "Epoch 413/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 45.6588 - val_loss: 52.2664\n",
            "Epoch 414/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 45.1417 - val_loss: 89.4144\n",
            "Epoch 415/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 33.4724 - val_loss: 42.2653\n",
            "Epoch 416/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 51.0113 - val_loss: 90.3344\n",
            "Epoch 417/1000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 82.0858 - val_loss: 33.4859\n",
            "Epoch 418/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 67.1141 - val_loss: 40.9367\n",
            "Epoch 419/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 76.6414 - val_loss: 121.7174\n",
            "Epoch 420/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 45.7116 - val_loss: 69.2564\n",
            "Epoch 421/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 49.3736 - val_loss: 76.7672\n",
            "Epoch 422/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 42.6248 - val_loss: 97.7961\n",
            "Epoch 423/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 32.7775 - val_loss: 109.4348\n",
            "Epoch 424/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 53.0178 - val_loss: 26.6210\n",
            "Epoch 425/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 25.6389 - val_loss: 54.6907\n",
            "Epoch 426/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 59.2351 - val_loss: 109.4266\n",
            "Epoch 427/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 56.5625 - val_loss: 97.1068\n",
            "Epoch 428/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 54.2140 - val_loss: 99.8904\n",
            "Epoch 429/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 49.7945 - val_loss: 92.6079\n",
            "Epoch 430/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 68.6548 - val_loss: 96.8611\n",
            "Epoch 431/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 49.7596 - val_loss: 182.4940\n",
            "Epoch 432/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 78.7957 - val_loss: 217.8917\n",
            "Epoch 433/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 118.5321 - val_loss: 82.2325\n",
            "Epoch 434/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 167.6825 - val_loss: 55.5476\n",
            "Epoch 435/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 181.2996 - val_loss: 82.5552\n",
            "Epoch 436/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 186.5648 - val_loss: 100.4331\n",
            "Epoch 437/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 129.7115 - val_loss: 103.3482\n",
            "Epoch 438/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 113.0991 - val_loss: 79.7633\n",
            "Epoch 439/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 47.1860 - val_loss: 84.7153\n",
            "Epoch 440/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 27.1994 - val_loss: 65.1023\n",
            "Epoch 441/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 25.8722 - val_loss: 56.5916\n",
            "Epoch 442/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 24.2593 - val_loss: 47.0904\n",
            "Epoch 443/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 27.5388 - val_loss: 48.3144\n",
            "Epoch 444/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 30.5402 - val_loss: 62.1334\n",
            "Epoch 445/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 42.0612 - val_loss: 91.3586\n",
            "Epoch 446/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 40.9600 - val_loss: 94.9259\n",
            "Epoch 447/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 30.0778 - val_loss: 87.7294\n",
            "Epoch 448/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 30.7083 - val_loss: 51.7659\n",
            "Epoch 449/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 22.5530 - val_loss: 41.5530\n",
            "Epoch 450/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 26.9814 - val_loss: 48.1460\n",
            "Epoch 451/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23.0979 - val_loss: 37.7520\n",
            "Epoch 452/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 26.2573 - val_loss: 53.9817\n",
            "Epoch 453/1000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 27.7796 - val_loss: 37.7779\n",
            "Epoch 454/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 20.5733 - val_loss: 39.0396\n",
            "Epoch 455/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14.3072 - val_loss: 38.1768\n",
            "Epoch 456/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 11.3828 - val_loss: 48.2396\n",
            "Epoch 457/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11.4066 - val_loss: 30.1297\n",
            "Epoch 458/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 12.1511 - val_loss: 32.2432\n",
            "Epoch 459/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.5857 - val_loss: 24.8776\n",
            "Epoch 460/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 6.9015 - val_loss: 20.4962\n",
            "Epoch 461/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 8.5648 - val_loss: 18.7493\n",
            "Epoch 462/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 2.8615 - val_loss: 27.3581\n",
            "Epoch 463/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 10.2343 - val_loss: 16.3590\n",
            "Epoch 464/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 9.4921 - val_loss: 15.6501\n",
            "Epoch 465/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 6.6359 - val_loss: 31.8968\n",
            "Epoch 466/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 5.1551 - val_loss: 23.3594\n",
            "Epoch 467/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 9.6662 - val_loss: 59.5708\n",
            "Epoch 468/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.9882 - val_loss: 28.5213\n",
            "Epoch 469/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5.7229 - val_loss: 30.6270\n",
            "Epoch 470/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.7594 - val_loss: 20.3093\n",
            "Epoch 471/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 10.1868 - val_loss: 37.7482\n",
            "Epoch 472/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 6.9203 - val_loss: 14.8714\n",
            "Epoch 473/1000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 8.6214 - val_loss: 17.6813\n",
            "Epoch 474/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 3.3537 - val_loss: 27.2834\n",
            "Epoch 475/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 2.3972 - val_loss: 17.0488\n",
            "Epoch 476/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 3.9653 - val_loss: 31.4486\n",
            "Epoch 477/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 3.6736 - val_loss: 19.0940\n",
            "Epoch 478/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 1.4652 - val_loss: 23.0661\n",
            "Epoch 479/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 1.8997 - val_loss: 18.0629\n",
            "Epoch 480/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 2.0495 - val_loss: 25.4596\n",
            "Epoch 481/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 1.6334 - val_loss: 17.0041\n",
            "Epoch 482/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 1.6981 - val_loss: 24.5314\n",
            "Epoch 483/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.6566 - val_loss: 19.8134\n",
            "Epoch 484/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 2.3872 - val_loss: 16.3549\n",
            "Epoch 485/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 1.9115 - val_loss: 32.0689\n",
            "Epoch 486/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 2.2841 - val_loss: 17.5615\n",
            "Epoch 487/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 4.4414 - val_loss: 27.9061\n",
            "Epoch 488/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 2.4912 - val_loss: 21.0247\n",
            "Epoch 489/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1.4368 - val_loss: 19.0386\n",
            "Epoch 490/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 1.7661 - val_loss: 32.1613\n",
            "Epoch 491/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 3.6503 - val_loss: 19.6730\n",
            "Epoch 492/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 4.3095 - val_loss: 27.1720\n",
            "Epoch 493/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.5037 - val_loss: 23.6187\n",
            "Epoch 494/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 9.4103 - val_loss: 32.5076\n",
            "Epoch 495/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 9.6929 - val_loss: 22.3211\n",
            "Epoch 496/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 6.2480 - val_loss: 50.4067\n",
            "Epoch 497/1000\n",
            "80/80 [==============================] - 0s 889us/step - loss: 5.9898 - val_loss: 25.9669\n",
            "Epoch 498/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 5.9667 - val_loss: 35.0693\n",
            "Epoch 499/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 5.6757 - val_loss: 23.2411\n",
            "Epoch 500/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 3.6064 - val_loss: 26.1487\n",
            "Epoch 501/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 4.2138 - val_loss: 23.9351\n",
            "Epoch 502/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 4.6818 - val_loss: 23.9690\n",
            "Epoch 503/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.6328 - val_loss: 26.1800\n",
            "Epoch 504/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 11.7882 - val_loss: 24.2754\n",
            "Epoch 505/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 16.7498 - val_loss: 59.0379\n",
            "Epoch 506/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 23.0259 - val_loss: 17.5798\n",
            "Epoch 507/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 9.3351 - val_loss: 40.5090\n",
            "Epoch 508/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 6.9886 - val_loss: 17.7843\n",
            "Epoch 509/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 5.3240 - val_loss: 37.1451\n",
            "Epoch 510/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 7.8939 - val_loss: 20.4868\n",
            "Epoch 511/1000\n",
            "80/80 [==============================] - 0s 881us/step - loss: 5.7878 - val_loss: 16.2910\n",
            "Epoch 512/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 6.2961 - val_loss: 25.1942\n",
            "Epoch 513/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 6.3847 - val_loss: 17.3015\n",
            "Epoch 514/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 11.5150 - val_loss: 51.6048\n",
            "Epoch 515/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 13.0562 - val_loss: 37.1606\n",
            "Epoch 516/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 12.3763 - val_loss: 32.5428\n",
            "Epoch 517/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 7.1475 - val_loss: 24.7543\n",
            "Epoch 518/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 7.0261 - val_loss: 26.7058\n",
            "Epoch 519/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 4.8703 - val_loss: 26.0695\n",
            "Epoch 520/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 5.3679 - val_loss: 29.2826\n",
            "Epoch 521/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 5.1704 - val_loss: 28.2298\n",
            "Epoch 522/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 5.8127 - val_loss: 20.4927\n",
            "Epoch 523/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 5.0044 - val_loss: 21.4590\n",
            "Epoch 524/1000\n",
            "80/80 [==============================] - 0s 903us/step - loss: 2.9928 - val_loss: 23.6210\n",
            "Epoch 525/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 3.2390 - val_loss: 27.2779\n",
            "Epoch 526/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 2.8103 - val_loss: 22.2471\n",
            "Epoch 527/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 4.5422 - val_loss: 21.3566\n",
            "Epoch 528/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 3.7631 - val_loss: 28.3671\n",
            "Epoch 529/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 1.6876 - val_loss: 20.2173\n",
            "Epoch 530/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.5993 - val_loss: 20.8681\n",
            "Epoch 531/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.8635 - val_loss: 24.4293\n",
            "Epoch 532/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.1148 - val_loss: 18.4196\n",
            "Epoch 533/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.1105 - val_loss: 24.5622\n",
            "Epoch 534/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 1.7516 - val_loss: 16.6949\n",
            "Epoch 535/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.6160 - val_loss: 21.1297\n",
            "Epoch 536/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 1.3257 - val_loss: 20.4206\n",
            "Epoch 537/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 1.5029 - val_loss: 22.4334\n",
            "Epoch 538/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 0.7598 - val_loss: 20.0024\n",
            "Epoch 539/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 0.9103 - val_loss: 22.7257\n",
            "Epoch 540/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 0.6527 - val_loss: 19.5308\n",
            "Epoch 541/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 20.8255\n",
            "Epoch 542/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 0.7438 - val_loss: 20.6696\n",
            "Epoch 543/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 0.6074 - val_loss: 19.4426\n",
            "Epoch 544/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 0.6355 - val_loss: 20.0177\n",
            "Epoch 545/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 0.8173 - val_loss: 22.1954\n",
            "Epoch 546/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 0.9103 - val_loss: 19.4520\n",
            "Epoch 547/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 0.5558 - val_loss: 20.3877\n",
            "Epoch 548/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 0.5782 - val_loss: 21.5376\n",
            "Epoch 549/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 0.8761 - val_loss: 18.1266\n",
            "Epoch 550/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 1.0005 - val_loss: 29.9812\n",
            "Epoch 551/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.0142 - val_loss: 21.5985\n",
            "Epoch 552/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 2.7302 - val_loss: 31.8525\n",
            "Epoch 553/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 4.2411 - val_loss: 21.0483\n",
            "Epoch 554/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 2.5367 - val_loss: 21.5120\n",
            "Epoch 555/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 1.6126 - val_loss: 23.2486\n",
            "Epoch 556/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 2.1082 - val_loss: 18.5233\n",
            "Epoch 557/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 1.8874 - val_loss: 28.1630\n",
            "Epoch 558/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 3.7309 - val_loss: 20.0031\n",
            "Epoch 559/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.7119 - val_loss: 32.9923\n",
            "Epoch 560/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.8329 - val_loss: 20.7486\n",
            "Epoch 561/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 2.5139 - val_loss: 22.3386\n",
            "Epoch 562/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 2.5172 - val_loss: 20.3907\n",
            "Epoch 563/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.4303 - val_loss: 16.3596\n",
            "Epoch 564/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 0.7497 - val_loss: 21.0856\n",
            "Epoch 565/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 0.6945 - val_loss: 19.0765\n",
            "Epoch 566/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 0.8455 - val_loss: 18.5968\n",
            "Epoch 567/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.3048 - val_loss: 27.6337\n",
            "Epoch 568/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.7250 - val_loss: 17.7670\n",
            "Epoch 569/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.4873 - val_loss: 23.9278\n",
            "Epoch 570/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.4788 - val_loss: 18.7642\n",
            "Epoch 571/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.5821 - val_loss: 19.3665\n",
            "Epoch 572/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.0744 - val_loss: 21.9802\n",
            "Epoch 573/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 1.3132 - val_loss: 15.5624\n",
            "Epoch 574/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 2.4797 - val_loss: 27.8986\n",
            "Epoch 575/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 2.6321 - val_loss: 18.6981\n",
            "Epoch 576/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 1.7436 - val_loss: 21.3708\n",
            "Epoch 577/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 1.5144 - val_loss: 24.0252\n",
            "Epoch 578/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1.4656 - val_loss: 19.3807\n",
            "Epoch 579/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 1.3793 - val_loss: 22.8654\n",
            "Epoch 580/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 1.1009 - val_loss: 17.4903\n",
            "Epoch 581/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.4656 - val_loss: 23.2207\n",
            "Epoch 582/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.3658 - val_loss: 17.5934\n",
            "Epoch 583/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.9005 - val_loss: 31.5473\n",
            "Epoch 584/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.1939 - val_loss: 17.8249\n",
            "Epoch 585/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 3.1103 - val_loss: 19.0532\n",
            "Epoch 586/1000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 2.6998 - val_loss: 21.0980\n",
            "Epoch 587/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 2.7329 - val_loss: 18.5412\n",
            "Epoch 588/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 2.9314 - val_loss: 25.4269\n",
            "Epoch 589/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 2.3585 - val_loss: 20.6079\n",
            "Epoch 590/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 3.0222 - val_loss: 21.1495\n",
            "Epoch 591/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 2.4510 - val_loss: 17.2155\n",
            "Epoch 592/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 2.6565 - val_loss: 19.3649\n",
            "Epoch 593/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 2.3692 - val_loss: 16.7625\n",
            "Epoch 594/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 4.2046 - val_loss: 17.4294\n",
            "Epoch 595/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 2.2105 - val_loss: 15.2231\n",
            "Epoch 596/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 2.0334 - val_loss: 16.0382\n",
            "Epoch 597/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 0.9983 - val_loss: 13.6298\n",
            "Epoch 598/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 0.6628 - val_loss: 13.4941\n",
            "Epoch 599/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 0.4937 - val_loss: 14.6078\n",
            "Epoch 600/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 0.3699 - val_loss: 14.8255\n",
            "Epoch 601/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 0.4776 - val_loss: 14.0435\n",
            "Epoch 602/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 0.5313 - val_loss: 14.0109\n",
            "Epoch 603/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 0.4005 - val_loss: 13.9680\n",
            "Epoch 604/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 0.3061 - val_loss: 15.4777\n",
            "Epoch 605/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 0.2970 - val_loss: 13.3725\n",
            "Epoch 606/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 0.5073 - val_loss: 16.8853\n",
            "Epoch 607/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 0.6662 - val_loss: 14.6778\n",
            "Epoch 608/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 0.3952 - val_loss: 13.3407\n",
            "Epoch 609/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 0.4101 - val_loss: 14.0285\n",
            "Epoch 610/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 0.3018 - val_loss: 14.0230\n",
            "Epoch 611/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 0.3376 - val_loss: 15.2632\n",
            "Epoch 612/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 0.2339 - val_loss: 14.3724\n",
            "Epoch 613/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 0.2090 - val_loss: 14.1914\n",
            "Epoch 614/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 0.2168 - val_loss: 15.4567\n",
            "Epoch 615/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 0.1746 - val_loss: 14.0429\n",
            "Epoch 616/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 0.2194 - val_loss: 15.7502\n",
            "Epoch 617/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 0.1788 - val_loss: 14.9216\n",
            "Epoch 618/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 0.2069 - val_loss: 14.9760\n",
            "Epoch 619/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 0.1837 - val_loss: 16.9438\n",
            "Epoch 620/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 0.4080 - val_loss: 14.3051\n",
            "Epoch 621/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 0.3814 - val_loss: 14.4877\n",
            "Epoch 622/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 0.3852 - val_loss: 15.7513\n",
            "Epoch 623/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 0.4503 - val_loss: 19.4638\n",
            "Epoch 624/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 0.9415 - val_loss: 12.5814\n",
            "Epoch 625/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 1.1443 - val_loss: 18.3871\n",
            "Epoch 626/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7906 - val_loss: 14.3931\n",
            "Epoch 627/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7803 - val_loss: 14.2825\n",
            "Epoch 628/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 17.3508\n",
            "Epoch 629/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3397 - val_loss: 15.4933\n",
            "Epoch 630/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 16.8486\n",
            "Epoch 631/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2759 - val_loss: 15.5279\n",
            "Epoch 632/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2184 - val_loss: 16.7799\n",
            "Epoch 633/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2610 - val_loss: 15.4013\n",
            "Epoch 634/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3102 - val_loss: 16.9703\n",
            "Epoch 635/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 0.3392 - val_loss: 14.8846\n",
            "Epoch 636/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 0.2522 - val_loss: 17.2501\n",
            "Epoch 637/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 0.1963 - val_loss: 16.0074\n",
            "Epoch 638/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 0.1908 - val_loss: 16.6513\n",
            "Epoch 639/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1803 - val_loss: 16.6769\n",
            "Epoch 640/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1651 - val_loss: 15.7421\n",
            "Epoch 641/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 0.1198 - val_loss: 16.2954\n",
            "Epoch 642/1000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 0.1343 - val_loss: 15.5438\n",
            "Epoch 643/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 0.1320 - val_loss: 15.9912\n",
            "Epoch 644/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 0.1578 - val_loss: 15.8220\n",
            "Epoch 645/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 0.1551 - val_loss: 15.7827\n",
            "Epoch 646/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 0.2963 - val_loss: 16.0332\n",
            "Epoch 647/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 0.2702 - val_loss: 18.3568\n",
            "Epoch 648/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 0.3736 - val_loss: 14.9935\n",
            "Epoch 649/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 0.3661 - val_loss: 14.6121\n",
            "Epoch 650/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2177 - val_loss: 17.8980\n",
            "Epoch 651/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 0.1657 - val_loss: 13.9068\n",
            "Epoch 652/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 0.3025 - val_loss: 16.6188\n",
            "Epoch 653/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 0.2210 - val_loss: 14.6575\n",
            "Epoch 654/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 0.3468 - val_loss: 15.6082\n",
            "Epoch 655/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.3505 - val_loss: 17.0716\n",
            "Epoch 656/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 0.3431 - val_loss: 15.1671\n",
            "Epoch 657/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 0.1824 - val_loss: 15.1226\n",
            "Epoch 658/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 0.1875 - val_loss: 15.3620\n",
            "Epoch 659/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2505 - val_loss: 17.2128\n",
            "Epoch 660/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 0.1830 - val_loss: 14.3642\n",
            "Epoch 661/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 0.3315 - val_loss: 18.6238\n",
            "Epoch 662/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 0.3331 - val_loss: 14.1486\n",
            "Epoch 663/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 0.4450 - val_loss: 15.6246\n",
            "Epoch 664/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 0.4498 - val_loss: 16.1931\n",
            "Epoch 665/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 0.3541 - val_loss: 16.4830\n",
            "Epoch 666/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 0.7005 - val_loss: 12.5674\n",
            "Epoch 667/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 1.1987 - val_loss: 17.9411\n",
            "Epoch 668/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.4212 - val_loss: 19.5279\n",
            "Epoch 669/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1.6188 - val_loss: 17.0978\n",
            "Epoch 670/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 1.9205 - val_loss: 14.2392\n",
            "Epoch 671/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 1.8428 - val_loss: 24.0526\n",
            "Epoch 672/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.6920 - val_loss: 20.9144\n",
            "Epoch 673/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.8580 - val_loss: 20.1010\n",
            "Epoch 674/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 2.0070 - val_loss: 16.2713\n",
            "Epoch 675/1000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 0.9386 - val_loss: 18.6506\n",
            "Epoch 676/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 0.6725 - val_loss: 14.6339\n",
            "Epoch 677/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 2.6778 - val_loss: 19.4930\n",
            "Epoch 678/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 3.5367 - val_loss: 36.6806\n",
            "Epoch 679/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 10.8306 - val_loss: 32.0338\n",
            "Epoch 680/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 21.2212 - val_loss: 30.8538\n",
            "Epoch 681/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 14.2125 - val_loss: 34.6155\n",
            "Epoch 682/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 14.7467 - val_loss: 16.4924\n",
            "Epoch 683/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 6.6831 - val_loss: 16.3535\n",
            "Epoch 684/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 5.8375 - val_loss: 28.3240\n",
            "Epoch 685/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 5.7961 - val_loss: 18.1068\n",
            "Epoch 686/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 5.6404 - val_loss: 30.3397\n",
            "Epoch 687/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 4.1757 - val_loss: 25.2801\n",
            "Epoch 688/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.9238 - val_loss: 20.0395\n",
            "Epoch 689/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.7533 - val_loss: 28.7877\n",
            "Epoch 690/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.8170 - val_loss: 26.4946\n",
            "Epoch 691/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 1.4154 - val_loss: 20.7024\n",
            "Epoch 692/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 1.7291 - val_loss: 28.7049\n",
            "Epoch 693/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 1.4421 - val_loss: 20.5560\n",
            "Epoch 694/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 2.1244 - val_loss: 15.8663\n",
            "Epoch 695/1000\n",
            "80/80 [==============================] - 0s 895us/step - loss: 2.6095 - val_loss: 39.8638\n",
            "Epoch 696/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 4.6801 - val_loss: 24.7153\n",
            "Epoch 697/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 4.4660 - val_loss: 20.1997\n",
            "Epoch 698/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.8299 - val_loss: 32.4679\n",
            "Epoch 699/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6.7598 - val_loss: 19.9867\n",
            "Epoch 700/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 12.6297 - val_loss: 35.2821\n",
            "Epoch 701/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16.2200 - val_loss: 61.9061\n",
            "Epoch 702/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 10.1396 - val_loss: 31.8148\n",
            "Epoch 703/1000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 6.6059 - val_loss: 34.5496\n",
            "Epoch 704/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8.5469 - val_loss: 51.2550\n",
            "Epoch 705/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11.8125 - val_loss: 24.1696\n",
            "Epoch 706/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 15.1606 - val_loss: 43.4136\n",
            "Epoch 707/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 44.4844 - val_loss: 201.6747\n",
            "Epoch 708/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 162.0973 - val_loss: 115.5610\n",
            "Epoch 709/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 86.4912 - val_loss: 68.1619\n",
            "Epoch 710/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 59.2721 - val_loss: 151.4718\n",
            "Epoch 711/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 64.6936 - val_loss: 133.4864\n",
            "Epoch 712/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 89.6749 - val_loss: 107.6680\n",
            "Epoch 713/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 82.1094 - val_loss: 97.7932\n",
            "Epoch 714/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 87.2117 - val_loss: 121.1046\n",
            "Epoch 715/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 127.3776 - val_loss: 93.2494\n",
            "Epoch 716/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 178.2130 - val_loss: 91.5551\n",
            "Epoch 717/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 156.7234 - val_loss: 337.9680\n",
            "Epoch 718/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 177.8120 - val_loss: 169.9746\n",
            "Epoch 719/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 170.7266 - val_loss: 129.8246\n",
            "Epoch 720/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 92.4855 - val_loss: 115.7024\n",
            "Epoch 721/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 84.0107 - val_loss: 170.6550\n",
            "Epoch 722/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 93.7482 - val_loss: 100.3175\n",
            "Epoch 723/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 105.5053 - val_loss: 115.6737\n",
            "Epoch 724/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 89.1578 - val_loss: 37.0231\n",
            "Epoch 725/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 74.8468 - val_loss: 48.8343\n",
            "Epoch 726/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 53.1671 - val_loss: 76.9964\n",
            "Epoch 727/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 35.0964 - val_loss: 61.7415\n",
            "Epoch 728/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 29.2377 - val_loss: 76.9439\n",
            "Epoch 729/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 31.7224 - val_loss: 59.5759\n",
            "Epoch 730/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 28.8526 - val_loss: 65.7068\n",
            "Epoch 731/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 36.6356 - val_loss: 50.9759\n",
            "Epoch 732/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26.5426 - val_loss: 42.8730\n",
            "Epoch 733/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 22.9207 - val_loss: 35.7962\n",
            "Epoch 734/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23.4300 - val_loss: 31.1560\n",
            "Epoch 735/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 22.2300 - val_loss: 53.5355\n",
            "Epoch 736/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 25.9568 - val_loss: 47.4021\n",
            "Epoch 737/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 26.4311 - val_loss: 64.7378\n",
            "Epoch 738/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 23.3879 - val_loss: 60.9047\n",
            "Epoch 739/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 17.5036 - val_loss: 104.0926\n",
            "Epoch 740/1000\n",
            "80/80 [==============================] - 0s 896us/step - loss: 19.6866 - val_loss: 36.7477\n",
            "Epoch 741/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 15.4524 - val_loss: 32.7723\n",
            "Epoch 742/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 7.7873 - val_loss: 43.4160\n",
            "Epoch 743/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 9.7406 - val_loss: 25.6518\n",
            "Epoch 744/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 9.2793 - val_loss: 19.9990\n",
            "Epoch 745/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 5.2893 - val_loss: 34.9912\n",
            "Epoch 746/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 5.7352 - val_loss: 34.7833\n",
            "Epoch 747/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 5.3934 - val_loss: 37.1995\n",
            "Epoch 748/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 3.5292 - val_loss: 26.6567\n",
            "Epoch 749/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 2.0698 - val_loss: 16.8555\n",
            "Epoch 750/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 2.2464 - val_loss: 25.9173\n",
            "Epoch 751/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 1.5804 - val_loss: 18.1251\n",
            "Epoch 752/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7228 - val_loss: 25.9472\n",
            "Epoch 753/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.3353 - val_loss: 22.5644\n",
            "Epoch 754/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7712 - val_loss: 19.9254\n",
            "Epoch 755/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 0.6819 - val_loss: 23.3246\n",
            "Epoch 756/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 0.8836 - val_loss: 24.1833\n",
            "Epoch 757/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 1.8300 - val_loss: 15.5758\n",
            "Epoch 758/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 1.8056 - val_loss: 31.9178\n",
            "Epoch 759/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 1.6913 - val_loss: 20.6240\n",
            "Epoch 760/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 1.1480 - val_loss: 28.1581\n",
            "Epoch 761/1000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 1.0207 - val_loss: 23.8660\n",
            "Epoch 762/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 0.6379 - val_loss: 21.5179\n",
            "Epoch 763/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 0.7838 - val_loss: 20.3932\n",
            "Epoch 764/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 0.7131 - val_loss: 23.3754\n",
            "Epoch 765/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 0.6378 - val_loss: 17.2014\n",
            "Epoch 766/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.9177 - val_loss: 26.9817\n",
            "Epoch 767/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 0.6352 - val_loss: 22.0577\n",
            "Epoch 768/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 0.6334 - val_loss: 24.1196\n",
            "Epoch 769/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 0.7252 - val_loss: 22.1030\n",
            "Epoch 770/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4965 - val_loss: 15.5926\n",
            "Epoch 771/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 0.7471 - val_loss: 21.7191\n",
            "Epoch 772/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 0.3971 - val_loss: 17.7948\n",
            "Epoch 773/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.4451 - val_loss: 19.6244\n",
            "Epoch 774/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 23.8635\n",
            "Epoch 775/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7777 - val_loss: 19.2560\n",
            "Epoch 776/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4174 - val_loss: 22.0597\n",
            "Epoch 777/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4593 - val_loss: 20.4474\n",
            "Epoch 778/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 19.9154\n",
            "Epoch 779/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 0.3388 - val_loss: 22.2217\n",
            "Epoch 780/1000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 0.3512 - val_loss: 20.4405\n",
            "Epoch 781/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 0.4317 - val_loss: 17.2992\n",
            "Epoch 782/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 0.9348 - val_loss: 25.3620\n",
            "Epoch 783/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 1.0086 - val_loss: 18.3411\n",
            "Epoch 784/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1.1913 - val_loss: 23.1162\n",
            "Epoch 785/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 0.5666 - val_loss: 22.8779\n",
            "Epoch 786/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 0.5370 - val_loss: 18.9781\n",
            "Epoch 787/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 0.4947 - val_loss: 25.4296\n",
            "Epoch 788/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 0.5254 - val_loss: 17.9635\n",
            "Epoch 789/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 0.5005 - val_loss: 27.0197\n",
            "Epoch 790/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5223 - val_loss: 17.3890\n",
            "Epoch 791/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 0.6790 - val_loss: 20.9401\n",
            "Epoch 792/1000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 0.6004 - val_loss: 22.1615\n",
            "Epoch 793/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 0.5027 - val_loss: 19.2842\n",
            "Epoch 794/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 0.5012 - val_loss: 24.6467\n",
            "Epoch 795/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 0.6540 - val_loss: 17.8432\n",
            "Epoch 796/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 0.7589 - val_loss: 22.1428\n",
            "Epoch 797/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 0.3277 - val_loss: 24.2868\n",
            "Epoch 798/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 0.5870 - val_loss: 17.4461\n",
            "Epoch 799/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7571 - val_loss: 24.2235\n",
            "Epoch 800/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.2752 - val_loss: 22.3912\n",
            "Epoch 801/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 0.7458 - val_loss: 20.5928\n",
            "Epoch 802/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 0.7523 - val_loss: 28.2935\n",
            "Epoch 803/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 0.8796 - val_loss: 17.8793\n",
            "Epoch 804/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 0.9825 - val_loss: 32.3896\n",
            "Epoch 805/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 2.8476 - val_loss: 21.5392\n",
            "Epoch 806/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 2.7225 - val_loss: 15.5563\n",
            "Epoch 807/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 4.8050 - val_loss: 40.2884\n",
            "Epoch 808/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 4.1504 - val_loss: 18.0914\n",
            "Epoch 809/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.9604 - val_loss: 24.6662\n",
            "Epoch 810/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.7549 - val_loss: 22.8987\n",
            "Epoch 811/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.5072 - val_loss: 16.6829\n",
            "Epoch 812/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 1.6631 - val_loss: 31.7284\n",
            "Epoch 813/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 2.6077 - val_loss: 17.9722\n",
            "Epoch 814/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 2.9147 - val_loss: 26.5656\n",
            "Epoch 815/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 1.4868 - val_loss: 28.1109\n",
            "Epoch 816/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 0.8166 - val_loss: 32.6443\n",
            "Epoch 817/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 0.8661 - val_loss: 26.9445\n",
            "Epoch 818/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 0.6013 - val_loss: 22.2461\n",
            "Epoch 819/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 24.6634\n",
            "Epoch 820/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 0.4385 - val_loss: 24.0038\n",
            "Epoch 821/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 0.4129 - val_loss: 21.9711\n",
            "Epoch 822/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 0.4388 - val_loss: 24.0135\n",
            "Epoch 823/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 0.2265 - val_loss: 25.9627\n",
            "Epoch 824/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2570 - val_loss: 23.1891\n",
            "Epoch 825/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 0.1946 - val_loss: 26.1044\n",
            "Epoch 826/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 0.2434 - val_loss: 23.7144\n",
            "Epoch 827/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.1823 - val_loss: 23.9184\n",
            "Epoch 828/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 0.2214 - val_loss: 26.8072\n",
            "Epoch 829/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 0.3948 - val_loss: 21.8029\n",
            "Epoch 830/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 0.4384 - val_loss: 26.0050\n",
            "Epoch 831/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2047 - val_loss: 23.1627\n",
            "Epoch 832/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 0.2216 - val_loss: 23.5906\n",
            "Epoch 833/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 0.1930 - val_loss: 24.5812\n",
            "Epoch 834/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 0.2340 - val_loss: 25.3463\n",
            "Epoch 835/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 0.2822 - val_loss: 20.4015\n",
            "Epoch 836/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 0.6896 - val_loss: 24.8904\n",
            "Epoch 837/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 0.4418 - val_loss: 27.0811\n",
            "Epoch 838/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 0.8231 - val_loss: 18.4228\n",
            "Epoch 839/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6569 - val_loss: 32.7878\n",
            "Epoch 840/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.3619 - val_loss: 17.5839\n",
            "Epoch 841/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.6946 - val_loss: 25.2136\n",
            "Epoch 842/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 0.8283 - val_loss: 34.9058\n",
            "Epoch 843/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 2.3564 - val_loss: 19.1973\n",
            "Epoch 844/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 3.5998 - val_loss: 43.1658\n",
            "Epoch 845/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 1.4856 - val_loss: 25.7224\n",
            "Epoch 846/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 2.3767 - val_loss: 18.5533\n",
            "Epoch 847/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 5.2195 - val_loss: 40.7177\n",
            "Epoch 848/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 5.5312 - val_loss: 13.1676\n",
            "Epoch 849/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 3.4977 - val_loss: 27.8386\n",
            "Epoch 850/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 4.4062 - val_loss: 29.3993\n",
            "Epoch 851/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 2.4160 - val_loss: 18.9610\n",
            "Epoch 852/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 1.2995 - val_loss: 30.2917\n",
            "Epoch 853/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 1.0231 - val_loss: 17.9535\n",
            "Epoch 854/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 0.9138 - val_loss: 19.6043\n",
            "Epoch 855/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.1596 - val_loss: 26.0293\n",
            "Epoch 856/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2.1689 - val_loss: 16.2325\n",
            "Epoch 857/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6455 - val_loss: 35.4434\n",
            "Epoch 858/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 1.0654 - val_loss: 16.7671\n",
            "Epoch 859/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 2.2133 - val_loss: 24.1060\n",
            "Epoch 860/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 1.0620 - val_loss: 22.0522\n",
            "Epoch 861/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 0.5859 - val_loss: 11.9565\n",
            "Epoch 862/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 2.1144 - val_loss: 36.4037\n",
            "Epoch 863/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 5.4367 - val_loss: 10.2988\n",
            "Epoch 864/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 6.3273 - val_loss: 40.1980\n",
            "Epoch 865/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 8.7748 - val_loss: 29.5727\n",
            "Epoch 866/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 19.5422 - val_loss: 26.2535\n",
            "Epoch 867/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 57.2779 - val_loss: 127.0390\n",
            "Epoch 868/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 60.3457 - val_loss: 147.4211\n",
            "Epoch 869/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 89.4630 - val_loss: 114.7784\n",
            "Epoch 870/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 41.4224 - val_loss: 67.0113\n",
            "Epoch 871/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 56.1047 - val_loss: 87.1978\n",
            "Epoch 872/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 48.8992 - val_loss: 22.3811\n",
            "Epoch 873/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 35.2478 - val_loss: 32.4121\n",
            "Epoch 874/1000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 26.3398 - val_loss: 75.8335\n",
            "Epoch 875/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 16.3256 - val_loss: 31.6438\n",
            "Epoch 876/1000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 27.0908 - val_loss: 123.4566\n",
            "Epoch 877/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 42.8289 - val_loss: 37.5637\n",
            "Epoch 878/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 24.6563 - val_loss: 41.4446\n",
            "Epoch 879/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 15.5203 - val_loss: 31.5943\n",
            "Epoch 880/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 24.6648 - val_loss: 26.4667\n",
            "Epoch 881/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 11.0980 - val_loss: 18.0827\n",
            "Epoch 882/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 6.2285 - val_loss: 35.2485\n",
            "Epoch 883/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 5.6435 - val_loss: 20.4451\n",
            "Epoch 884/1000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 6.6246 - val_loss: 18.1889\n",
            "Epoch 885/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 4.9134 - val_loss: 35.1604\n",
            "Epoch 886/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3.8728 - val_loss: 12.7460\n",
            "Epoch 887/1000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 4.2417 - val_loss: 30.2430\n",
            "Epoch 888/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 2.9106 - val_loss: 15.4098\n",
            "Epoch 889/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.9689 - val_loss: 24.9504\n",
            "Epoch 890/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.9192 - val_loss: 15.1341\n",
            "Epoch 891/1000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 1.8435 - val_loss: 22.8168\n",
            "Epoch 892/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 1.7340 - val_loss: 21.7417\n",
            "Epoch 893/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 1.4748 - val_loss: 11.9022\n",
            "Epoch 894/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 1.5783 - val_loss: 24.6249\n",
            "Epoch 895/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 2.3365 - val_loss: 16.3788\n",
            "Epoch 896/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.5586 - val_loss: 20.4771\n",
            "Epoch 897/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 1.3420 - val_loss: 21.4336\n",
            "Epoch 898/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 1.3586 - val_loss: 16.0999\n",
            "Epoch 899/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 0.7850 - val_loss: 17.6982\n",
            "Epoch 900/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 0.7453 - val_loss: 12.9325\n",
            "Epoch 901/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 1.4493 - val_loss: 16.4884\n",
            "Epoch 902/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 1.0932 - val_loss: 20.1499\n",
            "Epoch 903/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 1.5293 - val_loss: 12.1949\n",
            "Epoch 904/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.5757 - val_loss: 25.5442\n",
            "Epoch 905/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 1.4320 - val_loss: 17.6380\n",
            "Epoch 906/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.7593 - val_loss: 17.0088\n",
            "Epoch 907/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6880 - val_loss: 19.6278\n",
            "Epoch 908/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 0.6366 - val_loss: 11.8883\n",
            "Epoch 909/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 0.7811 - val_loss: 22.7028\n",
            "Epoch 910/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 0.8118 - val_loss: 12.2946\n",
            "Epoch 911/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 1.4071 - val_loss: 18.8558\n",
            "Epoch 912/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 0.9984 - val_loss: 17.4364\n",
            "Epoch 913/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 1.0780 - val_loss: 12.5545\n",
            "Epoch 914/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.4563 - val_loss: 19.5144\n",
            "Epoch 915/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 1.7224 - val_loss: 21.6191\n",
            "Epoch 916/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 2.2906 - val_loss: 12.7014\n",
            "Epoch 917/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 3.1730 - val_loss: 20.8265\n",
            "Epoch 918/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 1.9183 - val_loss: 27.5147\n",
            "Epoch 919/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 1.8389 - val_loss: 15.4884\n",
            "Epoch 920/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 1.6815 - val_loss: 31.6825\n",
            "Epoch 921/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 3.2569 - val_loss: 18.4207\n",
            "Epoch 922/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 3.0989 - val_loss: 17.5013\n",
            "Epoch 923/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 1.6036 - val_loss: 26.8376\n",
            "Epoch 924/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.2539 - val_loss: 14.0750\n",
            "Epoch 925/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 1.2284 - val_loss: 23.8859\n",
            "Epoch 926/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 0.9169 - val_loss: 14.7057\n",
            "Epoch 927/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 0.5487 - val_loss: 17.6473\n",
            "Epoch 928/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 0.4891 - val_loss: 18.5250\n",
            "Epoch 929/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 0.4504 - val_loss: 14.0341\n",
            "Epoch 930/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 0.5359 - val_loss: 18.3033\n",
            "Epoch 931/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 0.3210 - val_loss: 18.0097\n",
            "Epoch 932/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1512 - val_loss: 17.6298\n",
            "Epoch 933/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 0.1683 - val_loss: 19.9929\n",
            "Epoch 934/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 0.1874 - val_loss: 17.3609\n",
            "Epoch 935/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 0.1596 - val_loss: 20.0752\n",
            "Epoch 936/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 0.1909 - val_loss: 18.8843\n",
            "Epoch 937/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 0.1223 - val_loss: 17.7823\n",
            "Epoch 938/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 0.1207 - val_loss: 19.0462\n",
            "Epoch 939/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 0.1419 - val_loss: 18.0376\n",
            "Epoch 940/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 0.1202 - val_loss: 17.5334\n",
            "Epoch 941/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 0.1287 - val_loss: 18.0954\n",
            "Epoch 942/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.1077 - val_loss: 17.6665\n",
            "Epoch 943/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 0.1145 - val_loss: 18.0550\n",
            "Epoch 944/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 0.1052 - val_loss: 17.6642\n",
            "Epoch 945/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 0.0917 - val_loss: 16.1763\n",
            "Epoch 946/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 0.1192 - val_loss: 18.0664\n",
            "Epoch 947/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 0.0911 - val_loss: 16.5519\n",
            "Epoch 948/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 0.1390 - val_loss: 18.7396\n",
            "Epoch 949/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 0.1968 - val_loss: 19.2705\n",
            "Epoch 950/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2618 - val_loss: 13.3266\n",
            "Epoch 951/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.6286 - val_loss: 22.8773\n",
            "Epoch 952/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 0.8341 - val_loss: 16.7090\n",
            "Epoch 953/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 1.1236 - val_loss: 15.8172\n",
            "Epoch 954/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 0.8199 - val_loss: 18.5473\n",
            "Epoch 955/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 0.2217 - val_loss: 17.8061\n",
            "Epoch 956/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 0.2148 - val_loss: 18.3252\n",
            "Epoch 957/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 0.1067 - val_loss: 17.4040\n",
            "Epoch 958/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1236 - val_loss: 18.8766\n",
            "Epoch 959/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 0.1591 - val_loss: 16.8489\n",
            "Epoch 960/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 0.1272 - val_loss: 18.2445\n",
            "Epoch 961/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1004 - val_loss: 16.7457\n",
            "Epoch 962/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 0.0994 - val_loss: 17.9440\n",
            "Epoch 963/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1309 - val_loss: 17.7243\n",
            "Epoch 964/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1136 - val_loss: 16.0468\n",
            "Epoch 965/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2277 - val_loss: 19.2572\n",
            "Epoch 966/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 0.1793 - val_loss: 17.6281\n",
            "Epoch 967/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 0.1244 - val_loss: 15.8351\n",
            "Epoch 968/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 0.1677 - val_loss: 16.5724\n",
            "Epoch 969/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 0.1223 - val_loss: 16.0203\n",
            "Epoch 970/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 0.1230 - val_loss: 20.2137\n",
            "Epoch 971/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 0.1831 - val_loss: 17.7519\n",
            "Epoch 972/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 0.1412 - val_loss: 17.0741\n",
            "Epoch 973/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1248 - val_loss: 17.9685\n",
            "Epoch 974/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 0.1136 - val_loss: 16.1245\n",
            "Epoch 975/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0959 - val_loss: 16.6372\n",
            "Epoch 976/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 0.0895 - val_loss: 18.0275\n",
            "Epoch 977/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1263 - val_loss: 18.2614\n",
            "Epoch 978/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2095 - val_loss: 14.8590\n",
            "Epoch 979/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2639 - val_loss: 19.7072\n",
            "Epoch 980/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 0.2675 - val_loss: 17.4503\n",
            "Epoch 981/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 0.2179 - val_loss: 14.7136\n",
            "Epoch 982/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 0.2714 - val_loss: 21.0933\n",
            "Epoch 983/1000\n",
            "80/80 [==============================] - 0s 1000us/step - loss: 0.4955 - val_loss: 18.6292\n",
            "Epoch 984/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 0.6270 - val_loss: 14.6449\n",
            "Epoch 985/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 18.3052\n",
            "Epoch 986/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 0.4587 - val_loss: 28.0525\n",
            "Epoch 987/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 1.2153 - val_loss: 14.4549\n",
            "Epoch 988/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.1948 - val_loss: 24.3641\n",
            "Epoch 989/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 1.5921 - val_loss: 19.6827\n",
            "Epoch 990/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 1.7556 - val_loss: 13.6491\n",
            "Epoch 991/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 1.8548 - val_loss: 17.8215\n",
            "Epoch 992/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1.1633 - val_loss: 21.8684\n",
            "Epoch 993/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 2.4632 - val_loss: 14.7934\n",
            "Epoch 994/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 2.3286 - val_loss: 12.7481\n",
            "Epoch 995/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 2.3693 - val_loss: 37.8392\n",
            "Epoch 996/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 8.3100 - val_loss: 11.3195\n",
            "Epoch 997/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 9.8222 - val_loss: 16.5009\n",
            "Epoch 998/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 3.5673 - val_loss: 30.8305\n",
            "Epoch 999/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 4.3058 - val_loss: 12.3552\n",
            "Epoch 1000/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 4.4330 - val_loss: 14.7716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCh_x4H_kzmc",
        "colab_type": "code",
        "outputId": "d2ed4e42-826c-4296-dfa0-528f9854e904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "results = model.predict(x_test)\n",
        "results.shape\n",
        "y_test.shape\n",
        "plt.scatter(range(20),results,c='r')\n",
        "plt.scatter(range(20),y_test,c='g')\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDggzM0vlgDAzs1QOCDMzS+WAMDOzVP8f12/d6WEkcIsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHf9JREFUeJzt3X2UXHWd5/H3p25Vd+eJpCEhhIQY\nwIgGHwJmEHecPTisEBiP6K7rwNkjDMNMnBGOuju7Lqx7Do4Os86Ojg5nXWZQozijID4NGYyDkWFW\nPbtgEkHkKablQRJCEkhCQjrp7qr67h/3V9WVruokdHe6O12f1zmVqvrde6t+t251ffJ7uFWKCMzM\nzBoVJroCZmY2+TgczMysicPBzMyaOBzMzKyJw8HMzJo4HMzMrInDwczMmjgczMysicPBzMyaFCe6\nAiM1d+7cWLJkyURXw8zsuLJx48YXImLekdY7bsNhyZIlbNiwYaKrYWZ2XJH0zNGs524lMzNr4nAw\nM7MmDgczM2vicDAzsyYOBzMza+JwMDOzJg4HMzNrctye5zBS3/nZFp7bc4DZ0zt43SmzePOrupE0\n0dUyM5tU2i4c/vHnz3Hfpp31+3/wttP52O+8zgFhZtag7cLhy1efR1+5wp7eAf7XP/fwxZ88xRsW\nzeay5QsnumpmZpNGW445dBYz5p/QxZ++62wWzpnGmoeem+gqmZlNKm0ZDjWFgnjHsvn8uOcFDg5U\nJro6ZmaTRluHA8C5r+qmv1zlqRf2T3RVzMwmjbYPhzPmzgBwOJiZNWj7cDhldhcAO/f1TXBNzMwm\nj7YPhznTSgDs7u2f4JqYmU0ebR8OxazArK4ie3oHJroqZmaTRtuHA0D39A63HMzMGjgcgO7pJXa7\n5WBmVudwAOZM72CPWw5mZnUOB/KWw679DgczsxqHA7WWg7uVzMxqHA7ArK4ivf1lImKiq2JmNikc\nMRwkrZa0Q9IjDWUfl7RV0kPpcmnDshsk9UjaJOnihvKVqaxH0vUN5adLeiCVf0NSx1ju4NHoLBao\nBpSrDgczMzi6lsNXgJUtyj8bEcvTZS2ApGXA5cDZaZv/LSmTlAGfBy4BlgFXpHUB/iI91quB3cA1\no9mhkegsZgD0lavj/dRmZpPSEcMhIn4E7DrKx7sMuCMi+iLiKaAHOC9deiLiyYjoB+4ALlP+Czu/\nDXwrbX8b8O5XuA+j1lnKX4Y+fzOrmRkwujGH6yQ9nLqdulPZQuDZhnW2pLLhyk8C9kREeUh5S5JW\nSdogacPOnTuHW+0V6yymcHDLwcwMGHk43AKcCSwHtgGfGbMaHUZE3BoRKyJixbx588bscd2tZGZ2\nqBH9TGhEbK/dlvQF4O50dytwWsOqi1IZw5S/CMyRVEyth8b1x81gy8HdSmZmMMKWg6QFDXffA9Rm\nMq0BLpfUKel0YCnwU2A9sDTNTOogH7ReE/nc0fuA96btrwLuGkmdRmNwzMEtBzMzOIqWg6TbgQuA\nuZK2ADcCF0haDgTwNPABgIh4VNKdwGNAGbg2Iirpca4D7gEyYHVEPJqe4r8Cd0j6M+BB4EtjtndH\nqdatdMAD0mZmwFGEQ0Rc0aJ42A/wiLgJuKlF+VpgbYvyJ8lnM02Y6R15OHztgV9z/hknTWRVzMwm\nBZ8hDbxp0RwAZqSQMDNrdw4HoFAQC2Z3UfXXZ5iZAQ6HuqwgyhWHg5kZOBzqSlmBAX+3kpkZ4HCo\nywqiUvVUVjMzcDjUFQtiwN1KZmaAw6GulBWouFvJzAxwONRlBTFQcbeSmRk4HOpKmdxyMDNLHA6J\np7KamQ1yOCSlrEDZs5XMzACHQ11WkH9D2swscTgkxULB3UpmZonDISkW5G4lM7PE4ZAUMw9Im5nV\nOBySoscczMzqHA5JMStQ9klwZmaAw6GulLnlYGZW43BIPJXVzGyQwyHJp7K6W8nMDBwOdR6QNjMb\ndMRwkLRa0g5JjzSU/aWkJyQ9LOm7kuak8iWSDkh6KF3+pmGbN0v6haQeSTdLUio/UdI6SZvTdfex\n2NEjKWYFh4OZWXI0LYevACuHlK0DXh8RbwR+CdzQsOxXEbE8Xf6oofwW4A+BpelSe8zrgXsjYilw\nb7o/7ooFuVvJzCw5YjhExI+AXUPKfhAR5XT3fmDR4R5D0gLghIi4PyIC+Crw7rT4MuC2dPu2hvJx\nVcxENaDq1oOZ2ZiMOfw+8P2G+6dLelDS/5H0W6lsIbClYZ0tqQxgfkRsS7efB+YP90SSVknaIGnD\nzp07x6Dqg4oFAbhrycyMUYaDpI8BZeBrqWgbsDgizgH+E/B1SScc7eOlVsWwn84RcWtErIiIFfPm\nzRtFzZsVs/yl8PcrmZlBcaQbSvo94J3AhelDnYjoA/rS7Y2SfgW8BtjKoV1Pi1IZwHZJCyJiW+p+\n2jHSOo2GWw5mZoNG1HKQtBL4KPCuiOhtKJ8nKUu3zyAfeH4ydRvtlXR+mqV0JXBX2mwNcFW6fVVD\n+biqh4O/fM/M7MgtB0m3AxcAcyVtAW4kn53UCaxLM1LvTzOT/jXwCUkDQBX4o4ioDWZ/kHzm0zTy\nMYraOMWngDslXQM8A7xvTPbsFXK3kpnZoCOGQ0Rc0aL4S8Os+23g28Ms2wC8vkX5i8CFR6rHseaW\ng5nZIJ8hndRaDhWPOZiZORxqai2HAZ8IZ2bmcKgpZnk4uOVgZuZwqBtsOTgczMwcDkmx4NlKZmY1\nDocky3wSnJlZjcMhKdVaDu5WMjNzONRk9a/PcLeSmZnDISllPgnOzKzG4ZD4JDgzs0EOh8QnwZmZ\nDXI4JD4JzsxskMMhqbccHA5mZg6HmvpJcO5WMjNzONRk/iU4M7M6h0NSynwSnJlZjcMhqbUcKj4J\nzszM4VBTOwnO38pqZuZwqBtsOTgczMwcDkltzGHA3UpmZg6Hmtp5DhV3K5mZHV04SFotaYekRxrK\nTpS0TtLmdN2dyiXpZkk9kh6WdG7DNlel9TdLuqqh/M2SfpG2uVmSxnInj0bmk+DMzOqOtuXwFWDl\nkLLrgXsjYilwb7oPcAmwNF1WAbdAHibAjcBbgPOAG2uBktb5w4bthj7XMSeJYkGerWRmxlGGQ0T8\nCNg1pPgy4LZ0+zbg3Q3lX43c/cAcSQuAi4F1EbErInYD64CVadkJEXF/RATw1YbHGldZQT7PwcyM\n0Y05zI+Iben288D8dHsh8GzDeltS2eHKt7QobyJplaQNkjbs3LlzFFVvrZQVPJXVzIwxGpBO/+M/\n5p+qEXFrRKyIiBXz5s0b88fP3K1kZgaMLhy2py4h0vWOVL4VOK1hvUWp7HDli1qUj7tSJg9Im5kx\nunBYA9RmHF0F3NVQfmWatXQ+8FLqfroHuEhSdxqIvgi4Jy3bK+n8NEvpyobHGldZQZ7KamYGFI9m\nJUm3AxcAcyVtIZ919CngTknXAM8A70urrwUuBXqAXuBqgIjYJemTwPq03iciojbI/UHyGVHTgO+n\ny7grFgo+Cc7MjKMMh4i4YphFF7ZYN4Brh3mc1cDqFuUbgNcfTV2OpWImf32GmRk+Q/oQRU9lNTMD\nHA6HKGUFyu5WMjNzODTySXBmZjmHQ4NiVvDPhJqZ4XA4RLEgdyuZmeFwOESxIH99hpkZDodDeCqr\nmVnO4dCgWChQrrhbyczM4dAgH3Nwy8HMzOHQoJh5KquZGTgcDlH0SXBmZoDD4RDuVjIzyzkcGuQD\n0g4HMzOHQwOfBGdmlnM4NPCAtJlZzuHQwGMOZmY5h0ODYuaT4MzMwOFwCLcczMxyDocGxczhYGYG\nDodDZIUClWqQ/wy2mVn7cjg0KBUE4NaDmbW9EYeDpLMkPdRw2SvpI5I+LmlrQ/mlDdvcIKlH0iZJ\nFzeUr0xlPZKuH+1OjVQxy18Of223mbW74kg3jIhNwHIASRmwFfgucDXw2Yj4dOP6kpYBlwNnA6cC\nP5T0mrT488A7gC3AeklrIuKxkdZtpIqp5TBQqdJVysb76c3MJo0Rh8MQFwK/iohnJA23zmXAHRHR\nBzwlqQc4Ly3riYgnASTdkdYd/3DI8rq75WBm7W6sxhwuB25vuH+dpIclrZbUncoWAs82rLMllQ1X\n3kTSKkkbJG3YuXPnGFV90GDLweFgZu1t1OEgqQN4F/DNVHQLcCZ5l9M24DOjfY6aiLg1IlZExIp5\n8+aN1cPWdRTzl6OvXBnzxzYzO56MRcvhEuBnEbEdICK2R0QlIqrAFxjsOtoKnNaw3aJUNlz5uJs9\nrQOAPb0DE/H0ZmaTxliEwxU0dClJWtCw7D3AI+n2GuBySZ2STgeWAj8F1gNLJZ2eWiGXp3XHXff0\nEuBwMDMb1YC0pBnks4w+0FD8PyUtBwJ4urYsIh6VdCf5QHMZuDYiKulxrgPuATJgdUQ8Opp6jdSc\n6anlcKB/Ip7ezGzSGFU4RMR+4KQhZe8/zPo3ATe1KF8LrB1NXcZCreWw2y0HM2tzPkO6wexat9J+\ntxzMrL05HBp0FjOmd2TsOeCWg5m1N4fDEN3TO9jd65aDmbU3h8MQs6eVPFvJzNqew2GIaR0ZBwd8\nEpyZtTeHwxAdWYH+sn8q1Mzam8NhiI5igX7/jrSZtTmHwxAdRbcczMwcDkN0OhzMzBwOQ3UUC/Q5\nHMyszTkchuj0mIOZmcNhKM9WMjNzODTxgLSZmcOhiaeympk5HJp0ZBmValCp+nekzax9ORyGqP2O\ntLuWzKydORyGcDiYmTkcmtTCoa/iL98zs/blcBiiM3PLwczM4TCEu5XMzMYgHCQ9LekXkh6StCGV\nnShpnaTN6bo7lUvSzZJ6JD0s6dyGx7kqrb9Z0lWjrddI1cPB01nNrI2NVcvh7RGxPCJWpPvXA/dG\nxFLg3nQf4BJgabqsAm6BPEyAG4G3AOcBN9YCZbx1uFvJzOyYdStdBtyWbt8GvLuh/KuRux+YI2kB\ncDGwLiJ2RcRuYB2w8hjV7bDcrWRmNjbhEMAPJG2UtCqVzY+Iben288D8dHsh8GzDtltS2XDl487h\nYGYGxTF4jLdFxFZJJwPrJD3RuDAiQtKYnG6cwmcVwOLFi8fiIZsMTmV1OJhZ+xp1yyEitqbrHcB3\nyccMtqfuItL1jrT6VuC0hs0XpbLhyoc+160RsSIiVsybN2+0VW/JYw5mZqMMB0kzJM2q3QYuAh4B\n1gC1GUdXAXel22uAK9OspfOBl1L30z3ARZK600D0Rals3HWmlsOAWw5m1sZG2600H/iupNpjfT0i\n/knSeuBOSdcAzwDvS+uvBS4FeoBe4GqAiNgl6ZPA+rTeJyJi1yjrNiIltxzMzEYXDhHxJPCmFuUv\nAhe2KA/g2mEeazWwejT1GQsltxzMzHyG9FD1MYeKv7LbzNqXw2GIWjgMuFvJzNqYw2GIUlGAu5XM\nrL05HIbwgLSZmcOhSbHgloOZmcNhCEl0ZAUPSJtZW3M4tNBRLLjlYGZtzeHQQimTw8HM2prDoYVS\n5paDmbU3h0MLpaxAn2crmVkbczi0kI85eEDazNqXw6GFUiafIW1mbc3h0IJnK5lZu3M4tFDKCvQ7\nHMysjTkcWihlBX99hpm1NYdDCx2eympmbc7h0EJ+EpxnK5lZ+3I4tOABaTNrdw6HFjwgbWbtzuHQ\ngscczKzdORxa8GwlM2t3Iw4HSadJuk/SY5IelfThVP5xSVslPZQulzZsc4OkHkmbJF3cUL4ylfVI\nun50uzR6paIHpM2svRVHsW0Z+JOI+JmkWcBGSevSss9GxKcbV5a0DLgcOBs4FfihpNekxZ8H3gFs\nAdZLWhMRj42ibqNSygr++gwza2sjDoeI2AZsS7f3SXocWHiYTS4D7oiIPuApST3AeWlZT0Q8CSDp\njrTuhIVDR9ED0mbW3sZkzEHSEuAc4IFUdJ2khyWtltSdyhYCzzZstiWVDVc+YTwgbWbtbtThIGkm\n8G3gIxGxF7gFOBNYTt6y+Mxon6PhuVZJ2iBpw86dO8fqYZuUsgLVgLIDwsza1KjCQVKJPBi+FhHf\nAYiI7RFRiYgq8AUGu462Aqc1bL4olQ1X3iQibo2IFRGxYt68eaOp+mGVsvxl8aC0mbWr0cxWEvAl\n4PGI+KuG8gUNq70HeCTdXgNcLqlT0unAUuCnwHpgqaTTJXWQD1qvGWm9xkIpE4DHHcysbY1mttJv\nAu8HfiHpoVT234ArJC0HAnga+ABARDwq6U7ygeYycG1EVAAkXQfcA2TA6oh4dBT1GrXOYq3l4HAw\ns/Y0mtlKPwHUYtHaw2xzE3BTi/K1h9tuvA12KzkczKw9+QzpFurhUPaYg5m1J4dDC6XUrdRfqUxw\nTczMJobDoYWO2oC0Ww5m1qYcDi14zMHM2p3DoYUOz1YyszbncGih1nLweQ5m1q4cDi3Uw8HfzGpm\nbcrh0EKHvz7DzNqcw6GFUjGfreQxBzNrVw6HFjxbyczancOhhQ6POZhZm3M4tDA4ldVjDmbWnhwO\nLQzOVvLXZ5hZe3I4tFD7PQe3HMysXTkcWhh6EtwDT77Ikuu/x9Mv7J/IapmZjRuHQwsdQ2YrfWP9\nswDc/+SLE1YnM7Px5HBooVAQxYLq4VCu5t1LfZ69ZGZtwuEwjGmljJcPlgFQ+r27J57fO4E1MjMb\nPw6HYSw+aTqbtu/jw3c8yKPP5aGw4endE1wrM7PxMeLfkJ7qzpg3k3/8+XOHlG3e8TJ7Dw5wQldp\ngmplZjY+3HIYxuITp7Us/+KPnxrnmpiZjb9JEw6SVkraJKlH0vUTXZ9zTus+5P5ly08F4OZ7N/MP\nD24dlzo89cJ+PvfDXxLh8y2OpUrVr6/ZUJMiHCRlwOeBS4BlwBWSlk1knS583cn8+KNvr98/aUYn\n73zjAgA+8o2HeOnAwDGvwx///UY+98PNPP1i75g/dqUaRz3AXq3GcRtQ5SN8eeKf3f0YZ/3377Pu\nse3jVKOp67k9B7j26z9j1/5+3vc3/48l13+Pux9+7rh977Q7TYYDJ+mtwMcj4uJ0/waAiPgfw22z\nYsWK2LBhwzGv2/6+Mn93/zNc8RuL6atU+L3V63ls215+5w0L2HtwgIMDFU4+oYuLzz6F154yi2oE\nC2ZP44SuItWArKDDPv6zu3rrU2X/ZdMOTpzRQQTs2HeQP1/7BAAL50zj4+86m2WnnsDJszoRsO9g\nme37DtKRFegqZVSqQVYQJ87ooKuUDft8A5Uq9z2xgw/d8SAHB6qcNX8W1/zW6bx7+cL6d0o1rvuD\nR7fzybsfo7NU4DXzZ/Gpf/sGTprZCUBEUKkGB8tVBspVpnfm9YjIPyjOnDeTwhH2fzQq1aBcrdJZ\nPHR/1z+9iz29A9z/5Ivc9n+fplwNLlo2nwMDFfrLVc59VTfbXzrIM7t62fhMPslgzvQSX7xyBbt7\nB1jUPY3pHRmnzO5qeuyjUa0GBwYqdJWyIx7/40l/uYo0eJJoo2o1+PO1j/PFnzR3uy6Y3cVHV57F\nou7pnNY9nVImpncUmdbxyl/bY+2uh7byZ997nH//5kW8/bUnM7OzyKtPntlyn49XkjZGxIojrjdJ\nwuG9wMqI+IN0//3AWyLiuuG2Ga9waOU/f/PnfGvjFgC6p5fY3dvciihlIgJmTytRzESxkL+5IoJq\nQJBf79zXd9jnKhZUDw/Iw0ZwSNlQ01I4SJBJ9fM2+spVXu4rD7vd3JkddBYz+soV+gaq7DvMukdj\nVleRGR1FqmmfBypVypUq0zvzeRCFVL8sE5mEdOgHaauP1QCqkQfQ1j0HqFSDzmKBiLz8cK9LK8sW\nnMB/WXkWV395fdMyCbqKGcWC8mOYFSgWlB8DgRj83Y++cpX+cpW+cqX+tSudxQIzO4v1/cq3GZwa\nLdRwm0P2X6L+HLXtansW6f0z+DoNfXwNvn4afB3rW8TgVe3vv/Gx8/sxeDtdv7i/j2oVTphWBEQh\n1fHlg2X6ytX6az9vVie/u+I0Lnzdyfz9/b/m4S172Lzj5abXd/a0EjM6MrJMlAoFNIIcjfRPbV9q\nf1sR6XWKSMvS+6a+P2ndVFat5tf7Dja/56d3ZMzsLFJQbZ/z90BBUJCGf6MeuSgvb/EZ3GrdCNh7\ncICuYsYdq85nydwZwzzi4R1tOBxXs5UkrQJWASxevHjC6vGX730jf3zBmcyeVmLuzE527D3Ir3f1\nsnnHy7x8sMzBgQq9AxUiYN/BAcqVYKBarf+h195UEiyYnf8vtSBRrlY5dc40FszuYvvePs5d3M0p\ns7vYvH0fv97Vy1Mv7Gf73oMUswKzuorM7CxSLBToL1eY1pFRDdi1v589vf31ulaq+R/FQCX/H/ac\n6SWmd2S89cyTmNlZpLe/wr9s2sn+vjIv7u+nv1ylq1Sgs5hRykRvf4UPXbiUuTM7+EnPC2x8ZjfV\nGPx7kEgfngX2HRyglBWoRh6aPTteplyJ9EEnskK+37UPz2o1qERQrTZ/qLf+44j66ybgHcvmE5H/\nOFNB+UdkQfkH+bmLu5nVVeQ182ex7aWDnNBVpBLB7Gklfv7sS3SWCpx96gkUCwWygvjuB/8V2/f2\nMaMzY++BMgcGKjy7q5fe/jLlalCuRLqu5q0jBv+oO4oFOor5a5ZfF+p/yP3l6iEfSI0fuLUP+Kh/\nuNX2PQ75wKsta/wcUtrfwQ/1GPIYg2UE9Q0bt6/d12GWDW4nZnZmdJUyXu4rU619wFZhRmexfgz+\n3bmLePXJM+vH7JzF3VSqwf1PvsiB/grb9x2kUo285bv3IAf6K5Sr+ftzpP9NrQVioSEc69eprP6+\nSTtVCzYx+IEPcOKMDn73N05jy+5eHn1uL30DVbbs7qW/UqWa/pYq6T8ntf/0VCNa5sPQ/+w0vsbN\n67ber6FmdBbpL1eZPg6trsnScpi03UpmZlPJ0bYcJktH2npgqaTTJXUAlwNrJrhOZmZta1J0K0VE\nWdJ1wD1ABqyOiEcnuFpmZm1rUoQDQESsBdZOdD3MzGzydCuZmdkk4nAwM7MmDgczM2vicDAzsyYO\nBzMzazIpToIbCUk7gWdGuPlc4IUxrM7xwPvcHrzP7WE0+/yqiJh3pJWO23AYDUkbjuYMwanE+9we\nvM/tYTz22d1KZmbWxOFgZmZN2jUcbp3oCkwA73N78D63h2O+z2055mBmZofXri0HMzM7jLYLB0kr\nJW2S1CPp+omuz1iQdJqk+yQ9JulRSR9O5SdKWidpc7ruTuWSdHN6DR6WdO7E7sHIScokPSjp7nT/\ndEkPpH37RvoKeCR1pvs9afmSiaz3SEmaI+lbkp6Q9Likt0714yzpP6b39SOSbpfUNdWOs6TVknZI\neqSh7BUfV0lXpfU3S7pqNHVqq3CQlAGfBy4BlgFXSFo2sbUaE2XgTyJiGXA+cG3ar+uBeyNiKXBv\nug/5/i9Nl1XALeNf5THzYeDxhvt/AXw2Il4N7AauSeXXALtT+WfTesejvwb+KSJeC7yJfN+n7HGW\ntBD4ELAiIl5P/pX+lzP1jvNXgJVDyl7RcZV0InAj8BbgPODGWqCMSES0zQV4K3BPw/0bgBsmul7H\nYD/vAt4BbAIWpLIFwKZ0+2+BKxrWr693PF2ARemP5reBu8l/WfEFoDj0eJP/Vshb0+1iWk8TvQ+v\ncH9nA08NrfdUPs7AQuBZ4MR03O4GLp6KxxlYAjwy0uMKXAH8bUP5Ieu90ktbtRwYfKPVbEllU0Zq\nRp8DPADMj4htadHzwPx0e6q8Dp8DPgpU0/2TgD0RUfuV+Mb9qu9zWv5SWv94cjqwE/hy6kr7oqQZ\nTOHjHBFbgU8Dvwa2kR+3jUzt41zzSo/rmB7vdguHKU3STODbwEciYm/jssj/KzFlpqZJeiewIyI2\nTnRdxlEROBe4JSLOAfYz2NUATMnj3A1cRh6MpwIzaO5+mfIm4ri2WzhsBU5ruL8olR33JJXIg+Fr\nEfGdVLxd0oK0fAGwI5VPhdfhN4F3SXoauIO8a+mvgTmSar9w2Lhf9X1Oy2cDL45nhcfAFmBLRDyQ\n7n+LPCym8nH+N8BTEbEzIgaA75Af+6l8nGte6XEd0+PdbuGwHliaZjp0kA9srZngOo2aJAFfAh6P\niL9qWLQGqM1YuIp8LKJWfmWa9XA+8FJD8/W4EBE3RMSiiFhCfhz/OSL+A3Af8N602tB9rr0W703r\nH1f/w46I54FnJZ2Vii4EHmMKH2fy7qTzJU1P7/PaPk/Z49zglR7Xe4CLJHWnFtdFqWxkJnoQZgIG\nfS4Ffgn8CvjYRNdnjPbpbeRNzoeBh9LlUvK+1nuBzcAPgRPT+iKftfUr4BfkM0EmfD9Gsf8XAHen\n22cAPwV6gG8Cnam8K93vScvPmOh6j3BflwMb0rH+B6B7qh9n4E+BJ4BHgL8DOqfacQZuJx9TGSBv\nIV4zkuMK/H7a9x7g6tHUyWdIm5lZk3brVjIzs6PgcDAzsyYOBzMza+JwMDOzJg4HMzNr4nAwM7Mm\nDgczM2vicDAzsyb/HyfYXLzZ0LkoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGUO1yHzll4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}