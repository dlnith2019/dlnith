{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gSTv-pHAnVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfOxSbMOEKo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voay5z4kBEj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a7d1c649-bf38-4ec3-ca72-38255c328861"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "X = [((x/2)/100) for x in range(200)]\n",
        "Y = [(y * 10) for y in X]\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1, 0.105, 0.11, 0.115, 0.12, 0.125, 0.13, 0.135, 0.14, 0.145, 0.15, 0.155, 0.16, 0.165, 0.17, 0.175, 0.18, 0.185, 0.19, 0.195, 0.2, 0.205, 0.21, 0.215, 0.22, 0.225, 0.23, 0.235, 0.24, 0.245, 0.25, 0.255, 0.26, 0.265, 0.27, 0.275, 0.28, 0.285, 0.29, 0.295, 0.3, 0.305, 0.31, 0.315, 0.32, 0.325, 0.33, 0.335, 0.34, 0.345, 0.35, 0.355, 0.36, 0.365, 0.37, 0.375, 0.38, 0.385, 0.39, 0.395, 0.4, 0.405, 0.41, 0.415, 0.42, 0.425, 0.43, 0.435, 0.44, 0.445, 0.45, 0.455, 0.46, 0.465, 0.47, 0.475, 0.48, 0.485, 0.49, 0.495, 0.5, 0.505, 0.51, 0.515, 0.52, 0.525, 0.53, 0.535, 0.54, 0.545, 0.55, 0.555, 0.56, 0.565, 0.57, 0.575, 0.58, 0.585, 0.59, 0.595, 0.6, 0.605, 0.61, 0.615, 0.62, 0.625, 0.63, 0.635, 0.64, 0.645, 0.65, 0.655, 0.66, 0.665, 0.67, 0.675, 0.68, 0.685, 0.69, 0.695, 0.7, 0.705, 0.71, 0.715, 0.72, 0.725, 0.73, 0.735, 0.74, 0.745, 0.75, 0.755, 0.76, 0.765, 0.77, 0.775, 0.78, 0.785, 0.79, 0.795, 0.8, 0.805, 0.81, 0.815, 0.82, 0.825, 0.83, 0.835, 0.84, 0.845, 0.85, 0.855, 0.86, 0.865, 0.87, 0.875, 0.88, 0.885, 0.89, 0.895, 0.9, 0.905, 0.91, 0.915, 0.92, 0.925, 0.93, 0.935, 0.94, 0.945, 0.95, 0.955, 0.96, 0.965, 0.97, 0.975, 0.98, 0.985, 0.99, 0.995]\n",
            "[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35000000000000003, 0.4, 0.44999999999999996, 0.5, 0.55, 0.6, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.8999999999999999, 0.95, 1.0, 1.05, 1.1, 1.1500000000000001, 1.2, 1.25, 1.3, 1.35, 1.4000000000000001, 1.45, 1.5, 1.55, 1.6, 1.6500000000000001, 1.7000000000000002, 1.75, 1.7999999999999998, 1.85, 1.9, 1.9500000000000002, 2.0, 2.05, 2.1, 2.15, 2.2, 2.25, 2.3000000000000003, 2.3499999999999996, 2.4, 2.45, 2.5, 2.55, 2.6, 2.6500000000000004, 2.7, 2.75, 2.8000000000000003, 2.8499999999999996, 2.9, 2.9499999999999997, 3.0, 3.05, 3.1, 3.15, 3.2, 3.25, 3.3000000000000003, 3.35, 3.4000000000000004, 3.4499999999999997, 3.5, 3.55, 3.5999999999999996, 3.65, 3.7, 3.75, 3.8, 3.85, 3.9000000000000004, 3.95, 4.0, 4.050000000000001, 4.1, 4.1499999999999995, 4.2, 4.25, 4.3, 4.35, 4.4, 4.45, 4.5, 4.55, 4.6000000000000005, 4.65, 4.699999999999999, 4.75, 4.8, 4.85, 4.9, 4.95, 5.0, 5.05, 5.1, 5.15, 5.2, 5.25, 5.300000000000001, 5.3500000000000005, 5.4, 5.45, 5.5, 5.550000000000001, 5.6000000000000005, 5.6499999999999995, 5.699999999999999, 5.75, 5.8, 5.85, 5.8999999999999995, 5.949999999999999, 6.0, 6.05, 6.1, 6.15, 6.2, 6.25, 6.3, 6.35, 6.4, 6.45, 6.5, 6.550000000000001, 6.6000000000000005, 6.65, 6.7, 6.75, 6.800000000000001, 6.8500000000000005, 6.8999999999999995, 6.949999999999999, 7.0, 7.05, 7.1, 7.1499999999999995, 7.199999999999999, 7.25, 7.3, 7.35, 7.4, 7.45, 7.5, 7.55, 7.6, 7.65, 7.7, 7.75, 7.800000000000001, 7.8500000000000005, 7.9, 7.95, 8.0, 8.05, 8.100000000000001, 8.149999999999999, 8.2, 8.25, 8.299999999999999, 8.35, 8.4, 8.45, 8.5, 8.55, 8.6, 8.65, 8.7, 8.75, 8.8, 8.85, 8.9, 8.95, 9.0, 9.05, 9.1, 9.15, 9.200000000000001, 9.25, 9.3, 9.350000000000001, 9.399999999999999, 9.45, 9.5, 9.549999999999999, 9.6, 9.65, 9.7, 9.75, 9.8, 9.85, 9.9, 9.95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5E0zkCQBrtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array(X,dtype=float)\n",
        "y = np.array(Y,dtype=float)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0mNBxfREVwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array(x).reshape(200, 1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN_8-XX4EN1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "182550c9-a05d-48b7-ed14-337c0b47cb12"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ALX5maCESkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bca95249-6dec-4ae2-85fd-c220fc5e06c3"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft3RJf5NB5i2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD_wpUX5B-Lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "1bc09b26-52ae-4166-8b82-a5ce1e8ef1ee"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, 1)))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_7 (LSTM)                (None, 1, 50)             10400     \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 30,651\n",
            "Trainable params: 30,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTtz1rwBCByn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "958a9bdb-7637-4c45-9244-8245edb58f77"
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs=1000,validation_data=(x_test,y_test))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 160 samples, validate on 40 samples\n",
            "Epoch 1/1000\n",
            "160/160 [==============================] - 2s 15ms/step - loss: 31.8455 - val_loss: 37.8344\n",
            "Epoch 2/1000\n",
            "160/160 [==============================] - 0s 169us/step - loss: 31.7371 - val_loss: 37.7272\n",
            "Epoch 3/1000\n",
            "160/160 [==============================] - 0s 175us/step - loss: 31.6434 - val_loss: 37.6147\n",
            "Epoch 4/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 31.5364 - val_loss: 37.4924\n",
            "Epoch 5/1000\n",
            "160/160 [==============================] - 0s 182us/step - loss: 31.4231 - val_loss: 37.3599\n",
            "Epoch 6/1000\n",
            "160/160 [==============================] - 0s 181us/step - loss: 31.3004 - val_loss: 37.2158\n",
            "Epoch 7/1000\n",
            "160/160 [==============================] - 0s 175us/step - loss: 31.1675 - val_loss: 37.0559\n",
            "Epoch 8/1000\n",
            "160/160 [==============================] - 0s 168us/step - loss: 31.0219 - val_loss: 36.8764\n",
            "Epoch 9/1000\n",
            "160/160 [==============================] - 0s 171us/step - loss: 30.8531 - val_loss: 36.6755\n",
            "Epoch 10/1000\n",
            "160/160 [==============================] - 0s 168us/step - loss: 30.6707 - val_loss: 36.4461\n",
            "Epoch 11/1000\n",
            "160/160 [==============================] - 0s 176us/step - loss: 30.4655 - val_loss: 36.1832\n",
            "Epoch 12/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 30.2100 - val_loss: 35.8874\n",
            "Epoch 13/1000\n",
            "160/160 [==============================] - 0s 173us/step - loss: 29.9389 - val_loss: 35.5428\n",
            "Epoch 14/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 29.6241 - val_loss: 35.1424\n",
            "Epoch 15/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 29.2560 - val_loss: 34.6773\n",
            "Epoch 16/1000\n",
            "160/160 [==============================] - 0s 179us/step - loss: 28.8226 - val_loss: 34.1380\n",
            "Epoch 17/1000\n",
            "160/160 [==============================] - 0s 181us/step - loss: 28.3379 - val_loss: 33.5072\n",
            "Epoch 18/1000\n",
            "160/160 [==============================] - 0s 179us/step - loss: 27.7331 - val_loss: 32.7811\n",
            "Epoch 19/1000\n",
            "160/160 [==============================] - 0s 173us/step - loss: 27.0653 - val_loss: 31.9279\n",
            "Epoch 20/1000\n",
            "160/160 [==============================] - 0s 173us/step - loss: 26.3025 - val_loss: 30.9241\n",
            "Epoch 21/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 25.3939 - val_loss: 29.7571\n",
            "Epoch 22/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 24.3205 - val_loss: 28.4079\n",
            "Epoch 23/1000\n",
            "160/160 [==============================] - 0s 156us/step - loss: 23.1019 - val_loss: 26.8480\n",
            "Epoch 24/1000\n",
            "160/160 [==============================] - 0s 162us/step - loss: 21.6968 - val_loss: 25.0545\n",
            "Epoch 25/1000\n",
            "160/160 [==============================] - 0s 166us/step - loss: 20.1100 - val_loss: 23.0092\n",
            "Epoch 26/1000\n",
            "160/160 [==============================] - 0s 162us/step - loss: 18.3224 - val_loss: 20.6984\n",
            "Epoch 27/1000\n",
            "160/160 [==============================] - 0s 162us/step - loss: 16.2606 - val_loss: 18.1505\n",
            "Epoch 28/1000\n",
            "160/160 [==============================] - 0s 181us/step - loss: 14.1275 - val_loss: 15.3457\n",
            "Epoch 29/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 11.6432 - val_loss: 12.4320\n",
            "Epoch 30/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 9.2701 - val_loss: 9.4371\n",
            "Epoch 31/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 6.8823 - val_loss: 6.5737\n",
            "Epoch 32/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 4.6431 - val_loss: 4.0998\n",
            "Epoch 33/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 2.7717 - val_loss: 2.2507\n",
            "Epoch 34/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 1.5890 - val_loss: 1.1408\n",
            "Epoch 35/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.9482 - val_loss: 0.7539\n",
            "Epoch 36/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.8396 - val_loss: 0.7674\n",
            "Epoch 37/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.9088 - val_loss: 0.8327\n",
            "Epoch 38/1000\n",
            "160/160 [==============================] - 0s 179us/step - loss: 0.9381 - val_loss: 0.8072\n",
            "Epoch 39/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.8836 - val_loss: 0.7338\n",
            "Epoch 40/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.8110 - val_loss: 0.6863\n",
            "Epoch 41/1000\n",
            "160/160 [==============================] - 0s 236us/step - loss: 0.7718 - val_loss: 0.6736\n",
            "Epoch 42/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.7528 - val_loss: 0.6733\n",
            "Epoch 43/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.7432 - val_loss: 0.6670\n",
            "Epoch 44/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.7331 - val_loss: 0.6545\n",
            "Epoch 45/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.7192 - val_loss: 0.6367\n",
            "Epoch 46/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.7038 - val_loss: 0.6219\n",
            "Epoch 47/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.6908 - val_loss: 0.6091\n",
            "Epoch 48/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.6788 - val_loss: 0.5984\n",
            "Epoch 49/1000\n",
            "160/160 [==============================] - 0s 177us/step - loss: 0.6681 - val_loss: 0.5887\n",
            "Epoch 50/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.6568 - val_loss: 0.5795\n",
            "Epoch 51/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.6461 - val_loss: 0.5706\n",
            "Epoch 52/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.6361 - val_loss: 0.5623\n",
            "Epoch 53/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.6249 - val_loss: 0.5538\n",
            "Epoch 54/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.6148 - val_loss: 0.5453\n",
            "Epoch 55/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.6045 - val_loss: 0.5372\n",
            "Epoch 56/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.5957 - val_loss: 0.5293\n",
            "Epoch 57/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.5856 - val_loss: 0.5218\n",
            "Epoch 58/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.5767 - val_loss: 0.5144\n",
            "Epoch 59/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.5678 - val_loss: 0.5074\n",
            "Epoch 60/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 0.5598 - val_loss: 0.5005\n",
            "Epoch 61/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.5505 - val_loss: 0.4939\n",
            "Epoch 62/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.5425 - val_loss: 0.4876\n",
            "Epoch 63/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.5347 - val_loss: 0.4814\n",
            "Epoch 64/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.5272 - val_loss: 0.4754\n",
            "Epoch 65/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.5193 - val_loss: 0.4697\n",
            "Epoch 66/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.5120 - val_loss: 0.4643\n",
            "Epoch 67/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.5049 - val_loss: 0.4590\n",
            "Epoch 68/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.4981 - val_loss: 0.4540\n",
            "Epoch 69/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.4914 - val_loss: 0.4492\n",
            "Epoch 70/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.4852 - val_loss: 0.4445\n",
            "Epoch 71/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.4789 - val_loss: 0.4400\n",
            "Epoch 72/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.4726 - val_loss: 0.4357\n",
            "Epoch 73/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.4671 - val_loss: 0.4314\n",
            "Epoch 74/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.4613 - val_loss: 0.4275\n",
            "Epoch 75/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.4558 - val_loss: 0.4239\n",
            "Epoch 76/1000\n",
            "160/160 [==============================] - 0s 238us/step - loss: 0.4509 - val_loss: 0.4201\n",
            "Epoch 77/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.4451 - val_loss: 0.4170\n",
            "Epoch 78/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.4404 - val_loss: 0.4137\n",
            "Epoch 79/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.4353 - val_loss: 0.4107\n",
            "Epoch 80/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.4309 - val_loss: 0.4074\n",
            "Epoch 81/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.4260 - val_loss: 0.4048\n",
            "Epoch 82/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.4219 - val_loss: 0.4020\n",
            "Epoch 83/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.4176 - val_loss: 0.3999\n",
            "Epoch 84/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.4140 - val_loss: 0.3971\n",
            "Epoch 85/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.4094 - val_loss: 0.3947\n",
            "Epoch 86/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.4058 - val_loss: 0.3920\n",
            "Epoch 87/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.4023 - val_loss: 0.3896\n",
            "Epoch 88/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.3984 - val_loss: 0.3874\n",
            "Epoch 89/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.3951 - val_loss: 0.3855\n",
            "Epoch 90/1000\n",
            "160/160 [==============================] - 0s 261us/step - loss: 0.3916 - val_loss: 0.3838\n",
            "Epoch 91/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.3883 - val_loss: 0.3818\n",
            "Epoch 92/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.3851 - val_loss: 0.3799\n",
            "Epoch 93/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.3822 - val_loss: 0.3785\n",
            "Epoch 94/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.3792 - val_loss: 0.3779\n",
            "Epoch 95/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.3762 - val_loss: 0.3753\n",
            "Epoch 96/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.3735 - val_loss: 0.3741\n",
            "Epoch 97/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.3705 - val_loss: 0.3723\n",
            "Epoch 98/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.3677 - val_loss: 0.3702\n",
            "Epoch 99/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.3652 - val_loss: 0.3687\n",
            "Epoch 100/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.3628 - val_loss: 0.3670\n",
            "Epoch 101/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.3597 - val_loss: 0.3663\n",
            "Epoch 102/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.3575 - val_loss: 0.3655\n",
            "Epoch 103/1000\n",
            "160/160 [==============================] - 0s 260us/step - loss: 0.3551 - val_loss: 0.3635\n",
            "Epoch 104/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.3526 - val_loss: 0.3614\n",
            "Epoch 105/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.3501 - val_loss: 0.3606\n",
            "Epoch 106/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.3476 - val_loss: 0.3595\n",
            "Epoch 107/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.3456 - val_loss: 0.3574\n",
            "Epoch 108/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.3432 - val_loss: 0.3563\n",
            "Epoch 109/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.3409 - val_loss: 0.3544\n",
            "Epoch 110/1000\n",
            "160/160 [==============================] - 0s 246us/step - loss: 0.3387 - val_loss: 0.3531\n",
            "Epoch 111/1000\n",
            "160/160 [==============================] - 0s 274us/step - loss: 0.3360 - val_loss: 0.3528\n",
            "Epoch 112/1000\n",
            "160/160 [==============================] - 0s 315us/step - loss: 0.3341 - val_loss: 0.3514\n",
            "Epoch 113/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.3323 - val_loss: 0.3510\n",
            "Epoch 114/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.3298 - val_loss: 0.3492\n",
            "Epoch 115/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.3272 - val_loss: 0.3462\n",
            "Epoch 116/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.3257 - val_loss: 0.3435\n",
            "Epoch 117/1000\n",
            "160/160 [==============================] - 0s 235us/step - loss: 0.3232 - val_loss: 0.3417\n",
            "Epoch 118/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.3210 - val_loss: 0.3413\n",
            "Epoch 119/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.3185 - val_loss: 0.3407\n",
            "Epoch 120/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.3165 - val_loss: 0.3396\n",
            "Epoch 121/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.3144 - val_loss: 0.3378\n",
            "Epoch 122/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.3124 - val_loss: 0.3344\n",
            "Epoch 123/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.3101 - val_loss: 0.3321\n",
            "Epoch 124/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.3079 - val_loss: 0.3305\n",
            "Epoch 125/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.3059 - val_loss: 0.3300\n",
            "Epoch 126/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.3035 - val_loss: 0.3277\n",
            "Epoch 127/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.3013 - val_loss: 0.3254\n",
            "Epoch 128/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.2993 - val_loss: 0.3237\n",
            "Epoch 129/1000\n",
            "160/160 [==============================] - 0s 255us/step - loss: 0.2976 - val_loss: 0.3229\n",
            "Epoch 130/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.2950 - val_loss: 0.3204\n",
            "Epoch 131/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.2931 - val_loss: 0.3187\n",
            "Epoch 132/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.2907 - val_loss: 0.3151\n",
            "Epoch 133/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.2886 - val_loss: 0.3133\n",
            "Epoch 134/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.2865 - val_loss: 0.3110\n",
            "Epoch 135/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.2847 - val_loss: 0.3085\n",
            "Epoch 136/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.2822 - val_loss: 0.3080\n",
            "Epoch 137/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.2803 - val_loss: 0.3069\n",
            "Epoch 138/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.2781 - val_loss: 0.3049\n",
            "Epoch 139/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.2761 - val_loss: 0.3015\n",
            "Epoch 140/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.2736 - val_loss: 0.2996\n",
            "Epoch 141/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 0.2717 - val_loss: 0.2972\n",
            "Epoch 142/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.2696 - val_loss: 0.2949\n",
            "Epoch 143/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.2677 - val_loss: 0.2928\n",
            "Epoch 144/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.2654 - val_loss: 0.2911\n",
            "Epoch 145/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.2638 - val_loss: 0.2903\n",
            "Epoch 146/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.2621 - val_loss: 0.2885\n",
            "Epoch 147/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.2594 - val_loss: 0.2854\n",
            "Epoch 148/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.2574 - val_loss: 0.2825\n",
            "Epoch 149/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.2561 - val_loss: 0.2800\n",
            "Epoch 150/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.2538 - val_loss: 0.2783\n",
            "Epoch 151/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.2520 - val_loss: 0.2777\n",
            "Epoch 152/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.2498 - val_loss: 0.2759\n",
            "Epoch 153/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.2481 - val_loss: 0.2744\n",
            "Epoch 154/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.2464 - val_loss: 0.2711\n",
            "Epoch 155/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.2444 - val_loss: 0.2692\n",
            "Epoch 156/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.2424 - val_loss: 0.2671\n",
            "Epoch 157/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.2408 - val_loss: 0.2653\n",
            "Epoch 158/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.2389 - val_loss: 0.2637\n",
            "Epoch 159/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.2373 - val_loss: 0.2628\n",
            "Epoch 160/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.2356 - val_loss: 0.2606\n",
            "Epoch 161/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.2338 - val_loss: 0.2585\n",
            "Epoch 162/1000\n",
            "160/160 [==============================] - 0s 245us/step - loss: 0.2322 - val_loss: 0.2563\n",
            "Epoch 163/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.2311 - val_loss: 0.2555\n",
            "Epoch 164/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 0.2292 - val_loss: 0.2530\n",
            "Epoch 165/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.2275 - val_loss: 0.2515\n",
            "Epoch 166/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.2257 - val_loss: 0.2502\n",
            "Epoch 167/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.2242 - val_loss: 0.2484\n",
            "Epoch 168/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.2227 - val_loss: 0.2470\n",
            "Epoch 169/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.2211 - val_loss: 0.2455\n",
            "Epoch 170/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.2200 - val_loss: 0.2434\n",
            "Epoch 171/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.2185 - val_loss: 0.2426\n",
            "Epoch 172/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.2169 - val_loss: 0.2407\n",
            "Epoch 173/1000\n",
            "160/160 [==============================] - 0s 282us/step - loss: 0.2154 - val_loss: 0.2394\n",
            "Epoch 174/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.2139 - val_loss: 0.2376\n",
            "Epoch 175/1000\n",
            "160/160 [==============================] - 0s 286us/step - loss: 0.2125 - val_loss: 0.2363\n",
            "Epoch 176/1000\n",
            "160/160 [==============================] - 0s 270us/step - loss: 0.2113 - val_loss: 0.2354\n",
            "Epoch 177/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.2098 - val_loss: 0.2337\n",
            "Epoch 178/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.2089 - val_loss: 0.2316\n",
            "Epoch 179/1000\n",
            "160/160 [==============================] - 0s 244us/step - loss: 0.2074 - val_loss: 0.2309\n",
            "Epoch 180/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.2058 - val_loss: 0.2293\n",
            "Epoch 181/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.2044 - val_loss: 0.2284\n",
            "Epoch 182/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.2033 - val_loss: 0.2276\n",
            "Epoch 183/1000\n",
            "160/160 [==============================] - 0s 247us/step - loss: 0.2020 - val_loss: 0.2258\n",
            "Epoch 184/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.2009 - val_loss: 0.2239\n",
            "Epoch 185/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.1995 - val_loss: 0.2227\n",
            "Epoch 186/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.1983 - val_loss: 0.2218\n",
            "Epoch 187/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.1971 - val_loss: 0.2205\n",
            "Epoch 188/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1960 - val_loss: 0.2191\n",
            "Epoch 189/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.1947 - val_loss: 0.2180\n",
            "Epoch 190/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.1935 - val_loss: 0.2169\n",
            "Epoch 191/1000\n",
            "160/160 [==============================] - 0s 233us/step - loss: 0.1923 - val_loss: 0.2158\n",
            "Epoch 192/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.1911 - val_loss: 0.2144\n",
            "Epoch 193/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.1902 - val_loss: 0.2133\n",
            "Epoch 194/1000\n",
            "160/160 [==============================] - 0s 269us/step - loss: 0.1891 - val_loss: 0.2115\n",
            "Epoch 195/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.1878 - val_loss: 0.2108\n",
            "Epoch 196/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.1868 - val_loss: 0.2094\n",
            "Epoch 197/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.1860 - val_loss: 0.2092\n",
            "Epoch 198/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.1846 - val_loss: 0.2077\n",
            "Epoch 199/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.1837 - val_loss: 0.2057\n",
            "Epoch 200/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.1825 - val_loss: 0.2045\n",
            "Epoch 201/1000\n",
            "160/160 [==============================] - 0s 167us/step - loss: 0.1814 - val_loss: 0.2037\n",
            "Epoch 202/1000\n",
            "160/160 [==============================] - 0s 182us/step - loss: 0.1804 - val_loss: 0.2032\n",
            "Epoch 203/1000\n",
            "160/160 [==============================] - 0s 175us/step - loss: 0.1794 - val_loss: 0.2021\n",
            "Epoch 204/1000\n",
            "160/160 [==============================] - 0s 160us/step - loss: 0.1785 - val_loss: 0.2002\n",
            "Epoch 205/1000\n",
            "160/160 [==============================] - 0s 161us/step - loss: 0.1780 - val_loss: 0.2003\n",
            "Epoch 206/1000\n",
            "160/160 [==============================] - 0s 176us/step - loss: 0.1763 - val_loss: 0.1983\n",
            "Epoch 207/1000\n",
            "160/160 [==============================] - 0s 181us/step - loss: 0.1754 - val_loss: 0.1968\n",
            "Epoch 208/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.1744 - val_loss: 0.1962\n",
            "Epoch 209/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.1734 - val_loss: 0.1955\n",
            "Epoch 210/1000\n",
            "160/160 [==============================] - 0s 183us/step - loss: 0.1727 - val_loss: 0.1939\n",
            "Epoch 211/1000\n",
            "160/160 [==============================] - 0s 169us/step - loss: 0.1714 - val_loss: 0.1933\n",
            "Epoch 212/1000\n",
            "160/160 [==============================] - 0s 179us/step - loss: 0.1708 - val_loss: 0.1934\n",
            "Epoch 213/1000\n",
            "160/160 [==============================] - 0s 177us/step - loss: 0.1698 - val_loss: 0.1914\n",
            "Epoch 214/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.1690 - val_loss: 0.1911\n",
            "Epoch 215/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.1677 - val_loss: 0.1892\n",
            "Epoch 216/1000\n",
            "160/160 [==============================] - 0s 167us/step - loss: 0.1672 - val_loss: 0.1877\n",
            "Epoch 217/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.1661 - val_loss: 0.1871\n",
            "Epoch 218/1000\n",
            "160/160 [==============================] - 0s 179us/step - loss: 0.1650 - val_loss: 0.1869\n",
            "Epoch 219/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.1643 - val_loss: 0.1867\n",
            "Epoch 220/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.1638 - val_loss: 0.1849\n",
            "Epoch 221/1000\n",
            "160/160 [==============================] - 0s 159us/step - loss: 0.1626 - val_loss: 0.1842\n",
            "Epoch 222/1000\n",
            "160/160 [==============================] - 0s 172us/step - loss: 0.1618 - val_loss: 0.1836\n",
            "Epoch 223/1000\n",
            "160/160 [==============================] - 0s 175us/step - loss: 0.1611 - val_loss: 0.1826\n",
            "Epoch 224/1000\n",
            "160/160 [==============================] - 0s 172us/step - loss: 0.1605 - val_loss: 0.1808\n",
            "Epoch 225/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.1593 - val_loss: 0.1801\n",
            "Epoch 226/1000\n",
            "160/160 [==============================] - 0s 172us/step - loss: 0.1585 - val_loss: 0.1797\n",
            "Epoch 227/1000\n",
            "160/160 [==============================] - 0s 174us/step - loss: 0.1578 - val_loss: 0.1796\n",
            "Epoch 228/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.1571 - val_loss: 0.1781\n",
            "Epoch 229/1000\n",
            "160/160 [==============================] - 0s 168us/step - loss: 0.1563 - val_loss: 0.1775\n",
            "Epoch 230/1000\n",
            "160/160 [==============================] - 0s 163us/step - loss: 0.1554 - val_loss: 0.1765\n",
            "Epoch 231/1000\n",
            "160/160 [==============================] - 0s 171us/step - loss: 0.1547 - val_loss: 0.1758\n",
            "Epoch 232/1000\n",
            "160/160 [==============================] - 0s 172us/step - loss: 0.1540 - val_loss: 0.1751\n",
            "Epoch 233/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.1533 - val_loss: 0.1743\n",
            "Epoch 234/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.1526 - val_loss: 0.1738\n",
            "Epoch 235/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.1518 - val_loss: 0.1723\n",
            "Epoch 236/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.1511 - val_loss: 0.1714\n",
            "Epoch 237/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.1504 - val_loss: 0.1711\n",
            "Epoch 238/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.1497 - val_loss: 0.1703\n",
            "Epoch 239/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.1489 - val_loss: 0.1697\n",
            "Epoch 240/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1483 - val_loss: 0.1692\n",
            "Epoch 241/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.1476 - val_loss: 0.1686\n",
            "Epoch 242/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.1469 - val_loss: 0.1680\n",
            "Epoch 243/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 0.1465 - val_loss: 0.1678\n",
            "Epoch 244/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.1457 - val_loss: 0.1658\n",
            "Epoch 245/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.1452 - val_loss: 0.1646\n",
            "Epoch 246/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1443 - val_loss: 0.1643\n",
            "Epoch 247/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.1441 - val_loss: 0.1650\n",
            "Epoch 248/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.1430 - val_loss: 0.1639\n",
            "Epoch 249/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.1426 - val_loss: 0.1636\n",
            "Epoch 250/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1416 - val_loss: 0.1622\n",
            "Epoch 251/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 0.1411 - val_loss: 0.1610\n",
            "Epoch 252/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.1406 - val_loss: 0.1601\n",
            "Epoch 253/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.1399 - val_loss: 0.1598\n",
            "Epoch 254/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.1393 - val_loss: 0.1600\n",
            "Epoch 255/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1389 - val_loss: 0.1591\n",
            "Epoch 256/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.1383 - val_loss: 0.1575\n",
            "Epoch 257/1000\n",
            "160/160 [==============================] - 0s 230us/step - loss: 0.1380 - val_loss: 0.1582\n",
            "Epoch 258/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.1369 - val_loss: 0.1572\n",
            "Epoch 259/1000\n",
            "160/160 [==============================] - 0s 182us/step - loss: 0.1362 - val_loss: 0.1565\n",
            "Epoch 260/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.1363 - val_loss: 0.1544\n",
            "Epoch 261/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.1352 - val_loss: 0.1549\n",
            "Epoch 262/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.1345 - val_loss: 0.1551\n",
            "Epoch 263/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.1341 - val_loss: 0.1532\n",
            "Epoch 264/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.1334 - val_loss: 0.1533\n",
            "Epoch 265/1000\n",
            "160/160 [==============================] - 0s 262us/step - loss: 0.1327 - val_loss: 0.1528\n",
            "Epoch 266/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.1322 - val_loss: 0.1516\n",
            "Epoch 267/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.1315 - val_loss: 0.1512\n",
            "Epoch 268/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.1311 - val_loss: 0.1510\n",
            "Epoch 269/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.1306 - val_loss: 0.1492\n",
            "Epoch 270/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.1299 - val_loss: 0.1485\n",
            "Epoch 271/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.1293 - val_loss: 0.1484\n",
            "Epoch 272/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.1286 - val_loss: 0.1487\n",
            "Epoch 273/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.1284 - val_loss: 0.1475\n",
            "Epoch 274/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.1275 - val_loss: 0.1472\n",
            "Epoch 275/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.1272 - val_loss: 0.1470\n",
            "Epoch 276/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.1265 - val_loss: 0.1453\n",
            "Epoch 277/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.1260 - val_loss: 0.1448\n",
            "Epoch 278/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.1254 - val_loss: 0.1437\n",
            "Epoch 279/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.1250 - val_loss: 0.1440\n",
            "Epoch 280/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.1244 - val_loss: 0.1438\n",
            "Epoch 281/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.1239 - val_loss: 0.1424\n",
            "Epoch 282/1000\n",
            "160/160 [==============================] - 0s 232us/step - loss: 0.1233 - val_loss: 0.1419\n",
            "Epoch 283/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1227 - val_loss: 0.1409\n",
            "Epoch 284/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.1225 - val_loss: 0.1399\n",
            "Epoch 285/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.1217 - val_loss: 0.1407\n",
            "Epoch 286/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.1214 - val_loss: 0.1414\n",
            "Epoch 287/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.1210 - val_loss: 0.1401\n",
            "Epoch 288/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.1199 - val_loss: 0.1383\n",
            "Epoch 289/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.1195 - val_loss: 0.1366\n",
            "Epoch 290/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.1192 - val_loss: 0.1362\n",
            "Epoch 291/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.1185 - val_loss: 0.1366\n",
            "Epoch 292/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.1179 - val_loss: 0.1366\n",
            "Epoch 293/1000\n",
            "160/160 [==============================] - 0s 177us/step - loss: 0.1175 - val_loss: 0.1358\n",
            "Epoch 294/1000\n",
            "160/160 [==============================] - 0s 168us/step - loss: 0.1171 - val_loss: 0.1346\n",
            "Epoch 295/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.1164 - val_loss: 0.1346\n",
            "Epoch 296/1000\n",
            "160/160 [==============================] - 0s 180us/step - loss: 0.1160 - val_loss: 0.1349\n",
            "Epoch 297/1000\n",
            "160/160 [==============================] - 0s 163us/step - loss: 0.1157 - val_loss: 0.1343\n",
            "Epoch 298/1000\n",
            "160/160 [==============================] - 0s 170us/step - loss: 0.1155 - val_loss: 0.1312\n",
            "Epoch 299/1000\n",
            "160/160 [==============================] - 0s 175us/step - loss: 0.1149 - val_loss: 0.1306\n",
            "Epoch 300/1000\n",
            "160/160 [==============================] - 0s 163us/step - loss: 0.1138 - val_loss: 0.1319\n",
            "Epoch 301/1000\n",
            "160/160 [==============================] - 0s 180us/step - loss: 0.1138 - val_loss: 0.1330\n",
            "Epoch 302/1000\n",
            "160/160 [==============================] - 0s 162us/step - loss: 0.1131 - val_loss: 0.1308\n",
            "Epoch 303/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.1130 - val_loss: 0.1286\n",
            "Epoch 304/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.1119 - val_loss: 0.1288\n",
            "Epoch 305/1000\n",
            "160/160 [==============================] - 0s 169us/step - loss: 0.1114 - val_loss: 0.1290\n",
            "Epoch 306/1000\n",
            "160/160 [==============================] - 0s 166us/step - loss: 0.1109 - val_loss: 0.1280\n",
            "Epoch 307/1000\n",
            "160/160 [==============================] - 0s 178us/step - loss: 0.1104 - val_loss: 0.1269\n",
            "Epoch 308/1000\n",
            "160/160 [==============================] - 0s 165us/step - loss: 0.1099 - val_loss: 0.1267\n",
            "Epoch 309/1000\n",
            "160/160 [==============================] - 0s 174us/step - loss: 0.1094 - val_loss: 0.1262\n",
            "Epoch 310/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.1089 - val_loss: 0.1259\n",
            "Epoch 311/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.1084 - val_loss: 0.1250\n",
            "Epoch 312/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.1080 - val_loss: 0.1238\n",
            "Epoch 313/1000\n",
            "160/160 [==============================] - 0s 160us/step - loss: 0.1075 - val_loss: 0.1241\n",
            "Epoch 314/1000\n",
            "160/160 [==============================] - 0s 168us/step - loss: 0.1071 - val_loss: 0.1236\n",
            "Epoch 315/1000\n",
            "160/160 [==============================] - 0s 176us/step - loss: 0.1064 - val_loss: 0.1226\n",
            "Epoch 316/1000\n",
            "160/160 [==============================] - 0s 182us/step - loss: 0.1058 - val_loss: 0.1217\n",
            "Epoch 317/1000\n",
            "160/160 [==============================] - 0s 180us/step - loss: 0.1055 - val_loss: 0.1216\n",
            "Epoch 318/1000\n",
            "160/160 [==============================] - 0s 169us/step - loss: 0.1049 - val_loss: 0.1204\n",
            "Epoch 319/1000\n",
            "160/160 [==============================] - 0s 160us/step - loss: 0.1044 - val_loss: 0.1203\n",
            "Epoch 320/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.1040 - val_loss: 0.1194\n",
            "Epoch 321/1000\n",
            "160/160 [==============================] - 0s 176us/step - loss: 0.1034 - val_loss: 0.1193\n",
            "Epoch 322/1000\n",
            "160/160 [==============================] - 0s 160us/step - loss: 0.1029 - val_loss: 0.1186\n",
            "Epoch 323/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.1024 - val_loss: 0.1185\n",
            "Epoch 324/1000\n",
            "160/160 [==============================] - 0s 183us/step - loss: 0.1019 - val_loss: 0.1177\n",
            "Epoch 325/1000\n",
            "160/160 [==============================] - 0s 226us/step - loss: 0.1015 - val_loss: 0.1166\n",
            "Epoch 326/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.1011 - val_loss: 0.1165\n",
            "Epoch 327/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.1005 - val_loss: 0.1158\n",
            "Epoch 328/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.1001 - val_loss: 0.1146\n",
            "Epoch 329/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.1001 - val_loss: 0.1138\n",
            "Epoch 330/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0990 - val_loss: 0.1155\n",
            "Epoch 331/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.0988 - val_loss: 0.1149\n",
            "Epoch 332/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0980 - val_loss: 0.1135\n",
            "Epoch 333/1000\n",
            "160/160 [==============================] - 0s 236us/step - loss: 0.0975 - val_loss: 0.1113\n",
            "Epoch 334/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0971 - val_loss: 0.1109\n",
            "Epoch 335/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 0.0968 - val_loss: 0.1105\n",
            "Epoch 336/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.0962 - val_loss: 0.1119\n",
            "Epoch 337/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0961 - val_loss: 0.1123\n",
            "Epoch 338/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0951 - val_loss: 0.1090\n",
            "Epoch 339/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0948 - val_loss: 0.1077\n",
            "Epoch 340/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0942 - val_loss: 0.1077\n",
            "Epoch 341/1000\n",
            "160/160 [==============================] - 0s 233us/step - loss: 0.0935 - val_loss: 0.1079\n",
            "Epoch 342/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0932 - val_loss: 0.1085\n",
            "Epoch 343/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0927 - val_loss: 0.1074\n",
            "Epoch 344/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0922 - val_loss: 0.1065\n",
            "Epoch 345/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0916 - val_loss: 0.1051\n",
            "Epoch 346/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0912 - val_loss: 0.1043\n",
            "Epoch 347/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0908 - val_loss: 0.1047\n",
            "Epoch 348/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0903 - val_loss: 0.1040\n",
            "Epoch 349/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0897 - val_loss: 0.1028\n",
            "Epoch 350/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0893 - val_loss: 0.1020\n",
            "Epoch 351/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0890 - val_loss: 0.1015\n",
            "Epoch 352/1000\n",
            "160/160 [==============================] - 0s 182us/step - loss: 0.0882 - val_loss: 0.1025\n",
            "Epoch 353/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.0879 - val_loss: 0.1018\n",
            "Epoch 354/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0874 - val_loss: 0.1002\n",
            "Epoch 355/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0869 - val_loss: 0.0992\n",
            "Epoch 356/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0865 - val_loss: 0.0999\n",
            "Epoch 357/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.0858 - val_loss: 0.0988\n",
            "Epoch 358/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0855 - val_loss: 0.0984\n",
            "Epoch 359/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.0852 - val_loss: 0.0967\n",
            "Epoch 360/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0846 - val_loss: 0.0963\n",
            "Epoch 361/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0840 - val_loss: 0.0974\n",
            "Epoch 362/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0835 - val_loss: 0.0963\n",
            "Epoch 363/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0830 - val_loss: 0.0953\n",
            "Epoch 364/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0825 - val_loss: 0.0945\n",
            "Epoch 365/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0820 - val_loss: 0.0943\n",
            "Epoch 366/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0815 - val_loss: 0.0936\n",
            "Epoch 367/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.0811 - val_loss: 0.0935\n",
            "Epoch 368/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0808 - val_loss: 0.0920\n",
            "Epoch 369/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0803 - val_loss: 0.0913\n",
            "Epoch 370/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.0796 - val_loss: 0.0914\n",
            "Epoch 371/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.0792 - val_loss: 0.0914\n",
            "Epoch 372/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0787 - val_loss: 0.0908\n",
            "Epoch 373/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0783 - val_loss: 0.0900\n",
            "Epoch 374/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0779 - val_loss: 0.0881\n",
            "Epoch 375/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0774 - val_loss: 0.0881\n",
            "Epoch 376/1000\n",
            "160/160 [==============================] - 0s 183us/step - loss: 0.0767 - val_loss: 0.0883\n",
            "Epoch 377/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0764 - val_loss: 0.0881\n",
            "Epoch 378/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0759 - val_loss: 0.0868\n",
            "Epoch 379/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0754 - val_loss: 0.0858\n",
            "Epoch 380/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0752 - val_loss: 0.0867\n",
            "Epoch 381/1000\n",
            "160/160 [==============================] - 0s 249us/step - loss: 0.0747 - val_loss: 0.0846\n",
            "Epoch 382/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0740 - val_loss: 0.0846\n",
            "Epoch 383/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0735 - val_loss: 0.0841\n",
            "Epoch 384/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0730 - val_loss: 0.0838\n",
            "Epoch 385/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0726 - val_loss: 0.0831\n",
            "Epoch 386/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0721 - val_loss: 0.0823\n",
            "Epoch 387/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0716 - val_loss: 0.0820\n",
            "Epoch 388/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0712 - val_loss: 0.0813\n",
            "Epoch 389/1000\n",
            "160/160 [==============================] - 0s 174us/step - loss: 0.0709 - val_loss: 0.0806\n",
            "Epoch 390/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0702 - val_loss: 0.0805\n",
            "Epoch 391/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.0697 - val_loss: 0.0796\n",
            "Epoch 392/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0696 - val_loss: 0.0784\n",
            "Epoch 393/1000\n",
            "160/160 [==============================] - 0s 240us/step - loss: 0.0688 - val_loss: 0.0790\n",
            "Epoch 394/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0687 - val_loss: 0.0778\n",
            "Epoch 395/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0679 - val_loss: 0.0778\n",
            "Epoch 396/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0680 - val_loss: 0.0785\n",
            "Epoch 397/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0670 - val_loss: 0.0758\n",
            "Epoch 398/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0668 - val_loss: 0.0747\n",
            "Epoch 399/1000\n",
            "160/160 [==============================] - 0s 179us/step - loss: 0.0662 - val_loss: 0.0755\n",
            "Epoch 400/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0656 - val_loss: 0.0754\n",
            "Epoch 401/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0657 - val_loss: 0.0754\n",
            "Epoch 402/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0648 - val_loss: 0.0728\n",
            "Epoch 403/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0645 - val_loss: 0.0726\n",
            "Epoch 404/1000\n",
            "160/160 [==============================] - 0s 267us/step - loss: 0.0640 - val_loss: 0.0732\n",
            "Epoch 405/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0636 - val_loss: 0.0722\n",
            "Epoch 406/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0630 - val_loss: 0.0710\n",
            "Epoch 407/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0626 - val_loss: 0.0709\n",
            "Epoch 408/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.0622 - val_loss: 0.0709\n",
            "Epoch 409/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0617 - val_loss: 0.0702\n",
            "Epoch 410/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0612 - val_loss: 0.0694\n",
            "Epoch 411/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0608 - val_loss: 0.0687\n",
            "Epoch 412/1000\n",
            "160/160 [==============================] - 0s 231us/step - loss: 0.0603 - val_loss: 0.0683\n",
            "Epoch 413/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0602 - val_loss: 0.0679\n",
            "Epoch 414/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0595 - val_loss: 0.0674\n",
            "Epoch 415/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0591 - val_loss: 0.0672\n",
            "Epoch 416/1000\n",
            "160/160 [==============================] - 0s 251us/step - loss: 0.0587 - val_loss: 0.0661\n",
            "Epoch 417/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0583 - val_loss: 0.0653\n",
            "Epoch 418/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0579 - val_loss: 0.0658\n",
            "Epoch 419/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0575 - val_loss: 0.0648\n",
            "Epoch 420/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0571 - val_loss: 0.0649\n",
            "Epoch 421/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0565 - val_loss: 0.0639\n",
            "Epoch 422/1000\n",
            "160/160 [==============================] - 0s 226us/step - loss: 0.0562 - val_loss: 0.0632\n",
            "Epoch 423/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0559 - val_loss: 0.0636\n",
            "Epoch 424/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0553 - val_loss: 0.0625\n",
            "Epoch 425/1000\n",
            "160/160 [==============================] - 0s 181us/step - loss: 0.0550 - val_loss: 0.0609\n",
            "Epoch 426/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0547 - val_loss: 0.0610\n",
            "Epoch 427/1000\n",
            "160/160 [==============================] - 0s 241us/step - loss: 0.0540 - val_loss: 0.0614\n",
            "Epoch 428/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0538 - val_loss: 0.0617\n",
            "Epoch 429/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0534 - val_loss: 0.0601\n",
            "Epoch 430/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0528 - val_loss: 0.0591\n",
            "Epoch 431/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0525 - val_loss: 0.0588\n",
            "Epoch 432/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0521 - val_loss: 0.0589\n",
            "Epoch 433/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0517 - val_loss: 0.0585\n",
            "Epoch 434/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0513 - val_loss: 0.0572\n",
            "Epoch 435/1000\n",
            "160/160 [==============================] - 0s 180us/step - loss: 0.0510 - val_loss: 0.0569\n",
            "Epoch 436/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0505 - val_loss: 0.0573\n",
            "Epoch 437/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0502 - val_loss: 0.0566\n",
            "Epoch 438/1000\n",
            "160/160 [==============================] - 0s 246us/step - loss: 0.0499 - val_loss: 0.0552\n",
            "Epoch 439/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0494 - val_loss: 0.0554\n",
            "Epoch 440/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0491 - val_loss: 0.0556\n",
            "Epoch 441/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.0488 - val_loss: 0.0539\n",
            "Epoch 442/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0483 - val_loss: 0.0538\n",
            "Epoch 443/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0479 - val_loss: 0.0541\n",
            "Epoch 444/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0475 - val_loss: 0.0535\n",
            "Epoch 445/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0471 - val_loss: 0.0524\n",
            "Epoch 446/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0469 - val_loss: 0.0520\n",
            "Epoch 447/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.0464 - val_loss: 0.0527\n",
            "Epoch 448/1000\n",
            "160/160 [==============================] - 0s 171us/step - loss: 0.0461 - val_loss: 0.0517\n",
            "Epoch 449/1000\n",
            "160/160 [==============================] - 0s 252us/step - loss: 0.0457 - val_loss: 0.0506\n",
            "Epoch 450/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 0.0453 - val_loss: 0.0505\n",
            "Epoch 451/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0449 - val_loss: 0.0509\n",
            "Epoch 452/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.0446 - val_loss: 0.0500\n",
            "Epoch 453/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.0445 - val_loss: 0.0488\n",
            "Epoch 454/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0439 - val_loss: 0.0489\n",
            "Epoch 455/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0435 - val_loss: 0.0494\n",
            "Epoch 456/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0433 - val_loss: 0.0482\n",
            "Epoch 457/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0430 - val_loss: 0.0475\n",
            "Epoch 458/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.0425 - val_loss: 0.0480\n",
            "Epoch 459/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0422 - val_loss: 0.0474\n",
            "Epoch 460/1000\n",
            "160/160 [==============================] - 0s 241us/step - loss: 0.0418 - val_loss: 0.0466\n",
            "Epoch 461/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0416 - val_loss: 0.0460\n",
            "Epoch 462/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0412 - val_loss: 0.0460\n",
            "Epoch 463/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0409 - val_loss: 0.0456\n",
            "Epoch 464/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.0405 - val_loss: 0.0453\n",
            "Epoch 465/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0403 - val_loss: 0.0445\n",
            "Epoch 466/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0399 - val_loss: 0.0445\n",
            "Epoch 467/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0396 - val_loss: 0.0443\n",
            "Epoch 468/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0394 - val_loss: 0.0432\n",
            "Epoch 469/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0390 - val_loss: 0.0436\n",
            "Epoch 470/1000\n",
            "160/160 [==============================] - 0s 226us/step - loss: 0.0387 - val_loss: 0.0432\n",
            "Epoch 471/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0386 - val_loss: 0.0420\n",
            "Epoch 472/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.0381 - val_loss: 0.0425\n",
            "Epoch 473/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.0378 - val_loss: 0.0421\n",
            "Epoch 474/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0374 - val_loss: 0.0415\n",
            "Epoch 475/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0371 - val_loss: 0.0414\n",
            "Epoch 476/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0368 - val_loss: 0.0408\n",
            "Epoch 477/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0365 - val_loss: 0.0406\n",
            "Epoch 478/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0363 - val_loss: 0.0397\n",
            "Epoch 479/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0359 - val_loss: 0.0399\n",
            "Epoch 480/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0357 - val_loss: 0.0401\n",
            "Epoch 481/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0354 - val_loss: 0.0393\n",
            "Epoch 482/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0351 - val_loss: 0.0385\n",
            "Epoch 483/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0348 - val_loss: 0.0386\n",
            "Epoch 484/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0346 - val_loss: 0.0389\n",
            "Epoch 485/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0342 - val_loss: 0.0380\n",
            "Epoch 486/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0339 - val_loss: 0.0374\n",
            "Epoch 487/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0339 - val_loss: 0.0371\n",
            "Epoch 488/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0335 - val_loss: 0.0371\n",
            "Epoch 489/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.0332 - val_loss: 0.0375\n",
            "Epoch 490/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0329 - val_loss: 0.0364\n",
            "Epoch 491/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0327 - val_loss: 0.0357\n",
            "Epoch 492/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0324 - val_loss: 0.0364\n",
            "Epoch 493/1000\n",
            "160/160 [==============================] - 0s 244us/step - loss: 0.0322 - val_loss: 0.0358\n",
            "Epoch 494/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0317 - val_loss: 0.0349\n",
            "Epoch 495/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.0315 - val_loss: 0.0343\n",
            "Epoch 496/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0311 - val_loss: 0.0345\n",
            "Epoch 497/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0309 - val_loss: 0.0344\n",
            "Epoch 498/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.0305 - val_loss: 0.0329\n",
            "Epoch 499/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0301 - val_loss: 0.0329\n",
            "Epoch 500/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0296 - val_loss: 0.0331\n",
            "Epoch 501/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.0294 - val_loss: 0.0327\n",
            "Epoch 502/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0290 - val_loss: 0.0313\n",
            "Epoch 503/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0287 - val_loss: 0.0314\n",
            "Epoch 504/1000\n",
            "160/160 [==============================] - 0s 245us/step - loss: 0.0284 - val_loss: 0.0322\n",
            "Epoch 505/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0280 - val_loss: 0.0302\n",
            "Epoch 506/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0276 - val_loss: 0.0301\n",
            "Epoch 507/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0272 - val_loss: 0.0301\n",
            "Epoch 508/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0270 - val_loss: 0.0295\n",
            "Epoch 509/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0264 - val_loss: 0.0292\n",
            "Epoch 510/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0261 - val_loss: 0.0291\n",
            "Epoch 511/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0257 - val_loss: 0.0282\n",
            "Epoch 512/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0254 - val_loss: 0.0275\n",
            "Epoch 513/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0250 - val_loss: 0.0279\n",
            "Epoch 514/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0246 - val_loss: 0.0274\n",
            "Epoch 515/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0242 - val_loss: 0.0267\n",
            "Epoch 516/1000\n",
            "160/160 [==============================] - 0s 235us/step - loss: 0.0240 - val_loss: 0.0259\n",
            "Epoch 517/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0235 - val_loss: 0.0263\n",
            "Epoch 518/1000\n",
            "160/160 [==============================] - 0s 230us/step - loss: 0.0232 - val_loss: 0.0258\n",
            "Epoch 519/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.0229 - val_loss: 0.0248\n",
            "Epoch 520/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0226 - val_loss: 0.0254\n",
            "Epoch 521/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0222 - val_loss: 0.0240\n",
            "Epoch 522/1000\n",
            "160/160 [==============================] - 0s 231us/step - loss: 0.0218 - val_loss: 0.0245\n",
            "Epoch 523/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0214 - val_loss: 0.0235\n",
            "Epoch 524/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 0.0210 - val_loss: 0.0231\n",
            "Epoch 525/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0206 - val_loss: 0.0231\n",
            "Epoch 526/1000\n",
            "160/160 [==============================] - 0s 245us/step - loss: 0.0203 - val_loss: 0.0226\n",
            "Epoch 527/1000\n",
            "160/160 [==============================] - 0s 242us/step - loss: 0.0200 - val_loss: 0.0219\n",
            "Epoch 528/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.0196 - val_loss: 0.0223\n",
            "Epoch 529/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 0.0194 - val_loss: 0.0213\n",
            "Epoch 530/1000\n",
            "160/160 [==============================] - 0s 234us/step - loss: 0.0190 - val_loss: 0.0208\n",
            "Epoch 531/1000\n",
            "160/160 [==============================] - 0s 237us/step - loss: 0.0186 - val_loss: 0.0211\n",
            "Epoch 532/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.0183 - val_loss: 0.0205\n",
            "Epoch 533/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0181 - val_loss: 0.0197\n",
            "Epoch 534/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0176 - val_loss: 0.0202\n",
            "Epoch 535/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.0174 - val_loss: 0.0194\n",
            "Epoch 536/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0170 - val_loss: 0.0188\n",
            "Epoch 537/1000\n",
            "160/160 [==============================] - 0s 238us/step - loss: 0.0168 - val_loss: 0.0189\n",
            "Epoch 538/1000\n",
            "160/160 [==============================] - 0s 243us/step - loss: 0.0164 - val_loss: 0.0185\n",
            "Epoch 539/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0161 - val_loss: 0.0180\n",
            "Epoch 540/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0159 - val_loss: 0.0176\n",
            "Epoch 541/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0156 - val_loss: 0.0179\n",
            "Epoch 542/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0155 - val_loss: 0.0167\n",
            "Epoch 543/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0151 - val_loss: 0.0173\n",
            "Epoch 544/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 0.0148 - val_loss: 0.0165\n",
            "Epoch 545/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0146 - val_loss: 0.0159\n",
            "Epoch 546/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0143 - val_loss: 0.0167\n",
            "Epoch 547/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0141 - val_loss: 0.0155\n",
            "Epoch 548/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 0.0138 - val_loss: 0.0153\n",
            "Epoch 549/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0136 - val_loss: 0.0152\n",
            "Epoch 550/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0133 - val_loss: 0.0150\n",
            "Epoch 551/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.0131 - val_loss: 0.0147\n",
            "Epoch 552/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0129 - val_loss: 0.0144\n",
            "Epoch 553/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 0.0127 - val_loss: 0.0141\n",
            "Epoch 554/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.0125 - val_loss: 0.0143\n",
            "Epoch 555/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0122 - val_loss: 0.0134\n",
            "Epoch 556/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0121 - val_loss: 0.0136\n",
            "Epoch 557/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0119 - val_loss: 0.0133\n",
            "Epoch 558/1000\n",
            "160/160 [==============================] - 0s 247us/step - loss: 0.0117 - val_loss: 0.0129\n",
            "Epoch 559/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0115 - val_loss: 0.0131\n",
            "Epoch 560/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.0114 - val_loss: 0.0126\n",
            "Epoch 561/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0111 - val_loss: 0.0123\n",
            "Epoch 562/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0109 - val_loss: 0.0122\n",
            "Epoch 563/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.0108 - val_loss: 0.0122\n",
            "Epoch 564/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 565/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0105 - val_loss: 0.0118\n",
            "Epoch 566/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0103 - val_loss: 0.0114\n",
            "Epoch 567/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0102 - val_loss: 0.0112\n",
            "Epoch 568/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.0100 - val_loss: 0.0111\n",
            "Epoch 569/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0099 - val_loss: 0.0110\n",
            "Epoch 570/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0098 - val_loss: 0.0105\n",
            "Epoch 571/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0097 - val_loss: 0.0109\n",
            "Epoch 572/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0094 - val_loss: 0.0103\n",
            "Epoch 573/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0094 - val_loss: 0.0102\n",
            "Epoch 574/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0092 - val_loss: 0.0103\n",
            "Epoch 575/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0091 - val_loss: 0.0099\n",
            "Epoch 576/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0090 - val_loss: 0.0097\n",
            "Epoch 577/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0088 - val_loss: 0.0099\n",
            "Epoch 578/1000\n",
            "160/160 [==============================] - 0s 180us/step - loss: 0.0087 - val_loss: 0.0094\n",
            "Epoch 579/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.0086 - val_loss: 0.0095\n",
            "Epoch 580/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0085 - val_loss: 0.0095\n",
            "Epoch 581/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0085 - val_loss: 0.0092\n",
            "Epoch 582/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0083 - val_loss: 0.0090\n",
            "Epoch 583/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.0082 - val_loss: 0.0090\n",
            "Epoch 584/1000\n",
            "160/160 [==============================] - 0s 250us/step - loss: 0.0081 - val_loss: 0.0087\n",
            "Epoch 585/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0080 - val_loss: 0.0087\n",
            "Epoch 586/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0079 - val_loss: 0.0087\n",
            "Epoch 587/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0078 - val_loss: 0.0085\n",
            "Epoch 588/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0077 - val_loss: 0.0085\n",
            "Epoch 589/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.0076 - val_loss: 0.0081\n",
            "Epoch 590/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0075 - val_loss: 0.0082\n",
            "Epoch 591/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.0074 - val_loss: 0.0080\n",
            "Epoch 592/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0074 - val_loss: 0.0078\n",
            "Epoch 593/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.0073 - val_loss: 0.0080\n",
            "Epoch 594/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0072 - val_loss: 0.0077\n",
            "Epoch 595/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0071 - val_loss: 0.0077\n",
            "Epoch 596/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0070 - val_loss: 0.0075\n",
            "Epoch 597/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0069 - val_loss: 0.0075\n",
            "Epoch 598/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0068 - val_loss: 0.0073\n",
            "Epoch 599/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0068 - val_loss: 0.0072\n",
            "Epoch 600/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0067 - val_loss: 0.0071\n",
            "Epoch 601/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0066 - val_loss: 0.0071\n",
            "Epoch 602/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0065 - val_loss: 0.0070\n",
            "Epoch 603/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0065 - val_loss: 0.0070\n",
            "Epoch 604/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.0064 - val_loss: 0.0069\n",
            "Epoch 605/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.0063 - val_loss: 0.0067\n",
            "Epoch 606/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.0063 - val_loss: 0.0067\n",
            "Epoch 607/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0062 - val_loss: 0.0065\n",
            "Epoch 608/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0062 - val_loss: 0.0067\n",
            "Epoch 609/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0061 - val_loss: 0.0063\n",
            "Epoch 610/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0060 - val_loss: 0.0064\n",
            "Epoch 611/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.0059 - val_loss: 0.0063\n",
            "Epoch 612/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 0.0059 - val_loss: 0.0062\n",
            "Epoch 613/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.0058 - val_loss: 0.0061\n",
            "Epoch 614/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0058 - val_loss: 0.0061\n",
            "Epoch 615/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0057 - val_loss: 0.0059\n",
            "Epoch 616/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0056 - val_loss: 0.0059\n",
            "Epoch 617/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0056 - val_loss: 0.0058\n",
            "Epoch 618/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0055 - val_loss: 0.0058\n",
            "Epoch 619/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0055 - val_loss: 0.0057\n",
            "Epoch 620/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0054 - val_loss: 0.0058\n",
            "Epoch 621/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0053 - val_loss: 0.0056\n",
            "Epoch 622/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0053 - val_loss: 0.0056\n",
            "Epoch 623/1000\n",
            "160/160 [==============================] - 0s 246us/step - loss: 0.0053 - val_loss: 0.0056\n",
            "Epoch 624/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0052 - val_loss: 0.0053\n",
            "Epoch 625/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0052 - val_loss: 0.0055\n",
            "Epoch 626/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0051 - val_loss: 0.0052\n",
            "Epoch 627/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 628/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.0050 - val_loss: 0.0053\n",
            "Epoch 629/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.0049 - val_loss: 0.0052\n",
            "Epoch 630/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 631/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 632/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0048 - val_loss: 0.0050\n",
            "Epoch 633/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 634/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 635/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 636/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 637/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0046 - val_loss: 0.0050\n",
            "Epoch 638/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 639/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 640/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 0.0045 - val_loss: 0.0048\n",
            "Epoch 641/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 642/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 643/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 644/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0043 - val_loss: 0.0044\n",
            "Epoch 645/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 646/1000\n",
            "160/160 [==============================] - 0s 237us/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 647/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 648/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 649/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 650/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 651/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0040 - val_loss: 0.0041\n",
            "Epoch 652/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0040 - val_loss: 0.0041\n",
            "Epoch 653/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 654/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 655/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 656/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0040 - val_loss: 0.0039\n",
            "Epoch 657/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 658/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 659/1000\n",
            "160/160 [==============================] - 0s 244us/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 660/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 661/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 662/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 663/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 664/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 665/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 666/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 667/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 668/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0035 - val_loss: 0.0035\n",
            "Epoch 669/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 670/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 671/1000\n",
            "160/160 [==============================] - 0s 258us/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 672/1000\n",
            "160/160 [==============================] - 0s 231us/step - loss: 0.0033 - val_loss: 0.0034\n",
            "Epoch 673/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 674/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 675/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0033 - val_loss: 0.0032\n",
            "Epoch 676/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 677/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 678/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0032 - val_loss: 0.0031\n",
            "Epoch 679/1000\n",
            "160/160 [==============================] - 0s 242us/step - loss: 0.0031 - val_loss: 0.0033\n",
            "Epoch 680/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 681/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 682/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0031 - val_loss: 0.0030\n",
            "Epoch 683/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 684/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 685/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 686/1000\n",
            "160/160 [==============================] - 0s 233us/step - loss: 0.0030 - val_loss: 0.0029\n",
            "Epoch 687/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 688/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 689/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 690/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 691/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 692/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 693/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 694/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 0.0028 - val_loss: 0.0027\n",
            "Epoch 695/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 696/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 697/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 698/1000\n",
            "160/160 [==============================] - 0s 249us/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 699/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 700/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 701/1000\n",
            "160/160 [==============================] - 0s 182us/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 702/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 703/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 704/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 705/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 706/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 707/1000\n",
            "160/160 [==============================] - 0s 183us/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 708/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 709/1000\n",
            "160/160 [==============================] - 0s 181us/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 710/1000\n",
            "160/160 [==============================] - 0s 282us/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 711/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 712/1000\n",
            "160/160 [==============================] - 0s 226us/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 713/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 714/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 715/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 716/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 717/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 718/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 719/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 720/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 721/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 722/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 723/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 724/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 725/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 726/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 727/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 728/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 729/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 730/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 731/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 732/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 733/1000\n",
            "160/160 [==============================] - 0s 235us/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 734/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 735/1000\n",
            "160/160 [==============================] - 0s 188us/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 736/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 737/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 738/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 739/1000\n",
            "160/160 [==============================] - 0s 238us/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 740/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 741/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 742/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 743/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 744/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 745/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 746/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 747/1000\n",
            "160/160 [==============================] - 0s 253us/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 748/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 749/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 750/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 751/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 752/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 753/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 754/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 755/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 756/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 757/1000\n",
            "160/160 [==============================] - 0s 248us/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 758/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 759/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 760/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 761/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 762/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 763/1000\n",
            "160/160 [==============================] - 0s 225us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 764/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 765/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 766/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 767/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 768/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 769/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 770/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 771/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 772/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 773/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 774/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 775/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 776/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 777/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 778/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 779/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 780/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 781/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 782/1000\n",
            "160/160 [==============================] - 0s 244us/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 783/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 784/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 785/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 786/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 787/1000\n",
            "160/160 [==============================] - 0s 233us/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 788/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 789/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 790/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 791/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 792/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 793/1000\n",
            "160/160 [==============================] - 0s 240us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 794/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 795/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 796/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 797/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 798/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 799/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 800/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 801/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 802/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 803/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 804/1000\n",
            "160/160 [==============================] - 0s 260us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 805/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 806/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 807/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 808/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 809/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 810/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 0.0010 - val_loss: 9.8058e-04\n",
            "Epoch 811/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 812/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 0.0011 - val_loss: 9.5420e-04\n",
            "Epoch 813/1000\n",
            "160/160 [==============================] - 0s 239us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 814/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 0.0010 - val_loss: 9.5890e-04\n",
            "Epoch 815/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 816/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 9.8101e-04 - val_loss: 9.2720e-04\n",
            "Epoch 817/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 9.8181e-04 - val_loss: 9.8918e-04\n",
            "Epoch 818/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 9.6811e-04 - val_loss: 9.2555e-04\n",
            "Epoch 819/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 9.5599e-04 - val_loss: 9.5003e-04\n",
            "Epoch 820/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 9.6095e-04 - val_loss: 9.0647e-04\n",
            "Epoch 821/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 9.7949e-04 - val_loss: 9.9240e-04\n",
            "Epoch 822/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 9.5965e-04 - val_loss: 8.8418e-04\n",
            "Epoch 823/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 9.4546e-04 - val_loss: 0.0010\n",
            "Epoch 824/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 9.5199e-04 - val_loss: 8.9110e-04\n",
            "Epoch 825/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 9.5178e-04 - val_loss: 9.8790e-04\n",
            "Epoch 826/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 9.0955e-04 - val_loss: 8.5767e-04\n",
            "Epoch 827/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 8.9887e-04 - val_loss: 9.3511e-04\n",
            "Epoch 828/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 9.0907e-04 - val_loss: 8.5702e-04\n",
            "Epoch 829/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 8.9193e-04 - val_loss: 8.5432e-04\n",
            "Epoch 830/1000\n",
            "160/160 [==============================] - 0s 258us/step - loss: 8.7460e-04 - val_loss: 8.7441e-04\n",
            "Epoch 831/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 8.7726e-04 - val_loss: 8.3287e-04\n",
            "Epoch 832/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 8.6875e-04 - val_loss: 8.9372e-04\n",
            "Epoch 833/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 8.8621e-04 - val_loss: 8.0582e-04\n",
            "Epoch 834/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 8.6981e-04 - val_loss: 9.5109e-04\n",
            "Epoch 835/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 8.9479e-04 - val_loss: 8.2455e-04\n",
            "Epoch 836/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 8.6122e-04 - val_loss: 9.3204e-04\n",
            "Epoch 837/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 8.6138e-04 - val_loss: 7.8187e-04\n",
            "Epoch 838/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 8.2573e-04 - val_loss: 8.3364e-04\n",
            "Epoch 839/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 8.0107e-04 - val_loss: 7.7446e-04\n",
            "Epoch 840/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 8.0664e-04 - val_loss: 8.0775e-04\n",
            "Epoch 841/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 7.9803e-04 - val_loss: 7.7702e-04\n",
            "Epoch 842/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 7.9443e-04 - val_loss: 8.0988e-04\n",
            "Epoch 843/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 7.8271e-04 - val_loss: 7.6251e-04\n",
            "Epoch 844/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 7.7457e-04 - val_loss: 7.6050e-04\n",
            "Epoch 845/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 7.6997e-04 - val_loss: 7.6272e-04\n",
            "Epoch 846/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 7.5840e-04 - val_loss: 7.3015e-04\n",
            "Epoch 847/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 7.5487e-04 - val_loss: 7.7742e-04\n",
            "Epoch 848/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 7.5082e-04 - val_loss: 7.2535e-04\n",
            "Epoch 849/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 7.3659e-04 - val_loss: 7.4017e-04\n",
            "Epoch 850/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 7.3537e-04 - val_loss: 7.3283e-04\n",
            "Epoch 851/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 7.2711e-04 - val_loss: 7.1824e-04\n",
            "Epoch 852/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 7.2590e-04 - val_loss: 7.3990e-04\n",
            "Epoch 853/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 7.2026e-04 - val_loss: 6.9781e-04\n",
            "Epoch 854/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 7.1163e-04 - val_loss: 7.3267e-04\n",
            "Epoch 855/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 7.1652e-04 - val_loss: 6.8473e-04\n",
            "Epoch 856/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 7.0302e-04 - val_loss: 7.2178e-04\n",
            "Epoch 857/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 6.9442e-04 - val_loss: 6.6891e-04\n",
            "Epoch 858/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 6.8166e-04 - val_loss: 7.0446e-04\n",
            "Epoch 859/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 6.8032e-04 - val_loss: 6.7011e-04\n",
            "Epoch 860/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 6.7387e-04 - val_loss: 6.8765e-04\n",
            "Epoch 861/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 6.7003e-04 - val_loss: 6.6452e-04\n",
            "Epoch 862/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 6.7399e-04 - val_loss: 6.6839e-04\n",
            "Epoch 863/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 6.6248e-04 - val_loss: 6.4269e-04\n",
            "Epoch 864/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 6.6196e-04 - val_loss: 6.3434e-04\n",
            "Epoch 865/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 6.4984e-04 - val_loss: 6.8053e-04\n",
            "Epoch 866/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 6.5875e-04 - val_loss: 6.2903e-04\n",
            "Epoch 867/1000\n",
            "160/160 [==============================] - 0s 185us/step - loss: 6.3554e-04 - val_loss: 6.6788e-04\n",
            "Epoch 868/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 6.5477e-04 - val_loss: 6.7168e-04\n",
            "Epoch 869/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 6.4443e-04 - val_loss: 6.0659e-04\n",
            "Epoch 870/1000\n",
            "160/160 [==============================] - 0s 198us/step - loss: 6.2843e-04 - val_loss: 6.4116e-04\n",
            "Epoch 871/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 6.1994e-04 - val_loss: 6.0867e-04\n",
            "Epoch 872/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 6.2985e-04 - val_loss: 5.9740e-04\n",
            "Epoch 873/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 6.5053e-04 - val_loss: 6.9785e-04\n",
            "Epoch 874/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 6.2463e-04 - val_loss: 5.8129e-04\n",
            "Epoch 875/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 6.0928e-04 - val_loss: 6.5641e-04\n",
            "Epoch 876/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 5.9627e-04 - val_loss: 5.8724e-04\n",
            "Epoch 877/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 5.8944e-04 - val_loss: 5.8909e-04\n",
            "Epoch 878/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 5.8822e-04 - val_loss: 6.0406e-04\n",
            "Epoch 879/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 5.8343e-04 - val_loss: 5.6909e-04\n",
            "Epoch 880/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 5.8012e-04 - val_loss: 6.0334e-04\n",
            "Epoch 881/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 5.7710e-04 - val_loss: 5.5732e-04\n",
            "Epoch 882/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 5.7908e-04 - val_loss: 6.6029e-04\n",
            "Epoch 883/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 5.8064e-04 - val_loss: 5.5016e-04\n",
            "Epoch 884/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 5.7371e-04 - val_loss: 5.6448e-04\n",
            "Epoch 885/1000\n",
            "160/160 [==============================] - 0s 183us/step - loss: 5.5409e-04 - val_loss: 5.5664e-04\n",
            "Epoch 886/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 5.5273e-04 - val_loss: 5.3534e-04\n",
            "Epoch 887/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 5.4923e-04 - val_loss: 5.8803e-04\n",
            "Epoch 888/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 5.4891e-04 - val_loss: 5.2704e-04\n",
            "Epoch 889/1000\n",
            "160/160 [==============================] - 0s 212us/step - loss: 5.4044e-04 - val_loss: 5.3912e-04\n",
            "Epoch 890/1000\n",
            "160/160 [==============================] - 0s 195us/step - loss: 5.3001e-04 - val_loss: 5.2863e-04\n",
            "Epoch 891/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 5.3079e-04 - val_loss: 5.3685e-04\n",
            "Epoch 892/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 5.2208e-04 - val_loss: 5.2380e-04\n",
            "Epoch 893/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 5.1818e-04 - val_loss: 5.2161e-04\n",
            "Epoch 894/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 5.1683e-04 - val_loss: 5.4471e-04\n",
            "Epoch 895/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 5.1625e-04 - val_loss: 5.0714e-04\n",
            "Epoch 896/1000\n",
            "160/160 [==============================] - 0s 239us/step - loss: 5.1454e-04 - val_loss: 5.4150e-04\n",
            "Epoch 897/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 5.0320e-04 - val_loss: 4.9576e-04\n",
            "Epoch 898/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 5.0257e-04 - val_loss: 5.2734e-04\n",
            "Epoch 899/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 5.0452e-04 - val_loss: 4.9159e-04\n",
            "Epoch 900/1000\n",
            "160/160 [==============================] - 0s 232us/step - loss: 4.9134e-04 - val_loss: 5.2381e-04\n",
            "Epoch 901/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 4.9751e-04 - val_loss: 4.8055e-04\n",
            "Epoch 902/1000\n",
            "160/160 [==============================] - 0s 230us/step - loss: 4.9373e-04 - val_loss: 5.0910e-04\n",
            "Epoch 903/1000\n",
            "160/160 [==============================] - 0s 216us/step - loss: 4.9601e-04 - val_loss: 5.0957e-04\n",
            "Epoch 904/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 4.8229e-04 - val_loss: 4.7599e-04\n",
            "Epoch 905/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 4.8233e-04 - val_loss: 5.1727e-04\n",
            "Epoch 906/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 4.8100e-04 - val_loss: 4.6793e-04\n",
            "Epoch 907/1000\n",
            "160/160 [==============================] - 0s 241us/step - loss: 4.7713e-04 - val_loss: 4.7480e-04\n",
            "Epoch 908/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 4.6899e-04 - val_loss: 4.7466e-04\n",
            "Epoch 909/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 4.6624e-04 - val_loss: 4.5876e-04\n",
            "Epoch 910/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 4.6062e-04 - val_loss: 4.7804e-04\n",
            "Epoch 911/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 4.5631e-04 - val_loss: 4.5481e-04\n",
            "Epoch 912/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 4.5218e-04 - val_loss: 4.6307e-04\n",
            "Epoch 913/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 4.4946e-04 - val_loss: 4.4381e-04\n",
            "Epoch 914/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 4.5264e-04 - val_loss: 4.7002e-04\n",
            "Epoch 915/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 4.4509e-04 - val_loss: 4.4418e-04\n",
            "Epoch 916/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 4.3933e-04 - val_loss: 4.5050e-04\n",
            "Epoch 917/1000\n",
            "160/160 [==============================] - 0s 228us/step - loss: 4.3784e-04 - val_loss: 4.3945e-04\n",
            "Epoch 918/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 4.3580e-04 - val_loss: 4.3057e-04\n",
            "Epoch 919/1000\n",
            "160/160 [==============================] - 0s 244us/step - loss: 4.3134e-04 - val_loss: 4.4477e-04\n",
            "Epoch 920/1000\n",
            "160/160 [==============================] - 0s 202us/step - loss: 4.3168e-04 - val_loss: 4.2565e-04\n",
            "Epoch 921/1000\n",
            "160/160 [==============================] - 0s 180us/step - loss: 4.3322e-04 - val_loss: 4.2002e-04\n",
            "Epoch 922/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 4.3241e-04 - val_loss: 4.8081e-04\n",
            "Epoch 923/1000\n",
            "160/160 [==============================] - 0s 224us/step - loss: 4.2841e-04 - val_loss: 4.2041e-04\n",
            "Epoch 924/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 4.2781e-04 - val_loss: 4.9462e-04\n",
            "Epoch 925/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 4.4566e-04 - val_loss: 4.1135e-04\n",
            "Epoch 926/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 4.3952e-04 - val_loss: 4.1984e-04\n",
            "Epoch 927/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 4.1751e-04 - val_loss: 4.2966e-04\n",
            "Epoch 928/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 4.2311e-04 - val_loss: 4.0126e-04\n",
            "Epoch 929/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 4.1945e-04 - val_loss: 4.6669e-04\n",
            "Epoch 930/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 4.2144e-04 - val_loss: 3.9571e-04\n",
            "Epoch 931/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 4.1915e-04 - val_loss: 3.9478e-04\n",
            "Epoch 932/1000\n",
            "160/160 [==============================] - 0s 233us/step - loss: 4.0366e-04 - val_loss: 4.2132e-04\n",
            "Epoch 933/1000\n",
            "160/160 [==============================] - 0s 217us/step - loss: 3.9261e-04 - val_loss: 3.8654e-04\n",
            "Epoch 934/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 3.9813e-04 - val_loss: 4.0094e-04\n",
            "Epoch 935/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 3.8850e-04 - val_loss: 3.9551e-04\n",
            "Epoch 936/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 3.9111e-04 - val_loss: 3.8467e-04\n",
            "Epoch 937/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 3.9869e-04 - val_loss: 3.9986e-04\n",
            "Epoch 938/1000\n",
            "160/160 [==============================] - 0s 196us/step - loss: 3.8629e-04 - val_loss: 3.8609e-04\n",
            "Epoch 939/1000\n",
            "160/160 [==============================] - 0s 205us/step - loss: 3.8073e-04 - val_loss: 3.7615e-04\n",
            "Epoch 940/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 3.8526e-04 - val_loss: 3.7106e-04\n",
            "Epoch 941/1000\n",
            "160/160 [==============================] - 0s 215us/step - loss: 3.8275e-04 - val_loss: 3.9352e-04\n",
            "Epoch 942/1000\n",
            "160/160 [==============================] - 0s 229us/step - loss: 3.8443e-04 - val_loss: 3.6685e-04\n",
            "Epoch 943/1000\n",
            "160/160 [==============================] - 0s 214us/step - loss: 3.7372e-04 - val_loss: 3.7238e-04\n",
            "Epoch 944/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 3.6758e-04 - val_loss: 3.7515e-04\n",
            "Epoch 945/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 3.6429e-04 - val_loss: 3.7406e-04\n",
            "Epoch 946/1000\n",
            "160/160 [==============================] - 0s 220us/step - loss: 3.6550e-04 - val_loss: 3.6187e-04\n",
            "Epoch 947/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 3.7137e-04 - val_loss: 3.7617e-04\n",
            "Epoch 948/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 3.6394e-04 - val_loss: 3.7126e-04\n",
            "Epoch 949/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 3.5914e-04 - val_loss: 3.5561e-04\n",
            "Epoch 950/1000\n",
            "160/160 [==============================] - 0s 184us/step - loss: 3.5560e-04 - val_loss: 3.6571e-04\n",
            "Epoch 951/1000\n",
            "160/160 [==============================] - 0s 209us/step - loss: 3.5196e-04 - val_loss: 3.4833e-04\n",
            "Epoch 952/1000\n",
            "160/160 [==============================] - 0s 219us/step - loss: 3.5218e-04 - val_loss: 3.6180e-04\n",
            "Epoch 953/1000\n",
            "160/160 [==============================] - 0s 222us/step - loss: 3.4804e-04 - val_loss: 3.4465e-04\n",
            "Epoch 954/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 3.5068e-04 - val_loss: 3.9545e-04\n",
            "Epoch 955/1000\n",
            "160/160 [==============================] - 0s 221us/step - loss: 3.5156e-04 - val_loss: 3.4047e-04\n",
            "Epoch 956/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 3.4070e-04 - val_loss: 3.6957e-04\n",
            "Epoch 957/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 3.4070e-04 - val_loss: 3.3775e-04\n",
            "Epoch 958/1000\n",
            "160/160 [==============================] - 0s 200us/step - loss: 3.3720e-04 - val_loss: 3.6286e-04\n",
            "Epoch 959/1000\n",
            "160/160 [==============================] - 0s 193us/step - loss: 3.4347e-04 - val_loss: 3.3295e-04\n",
            "Epoch 960/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 3.4355e-04 - val_loss: 3.5296e-04\n",
            "Epoch 961/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 3.3426e-04 - val_loss: 3.4070e-04\n",
            "Epoch 962/1000\n",
            "160/160 [==============================] - 0s 253us/step - loss: 3.3196e-04 - val_loss: 3.2903e-04\n",
            "Epoch 963/1000\n",
            "160/160 [==============================] - 0s 186us/step - loss: 3.3503e-04 - val_loss: 3.5301e-04\n",
            "Epoch 964/1000\n",
            "160/160 [==============================] - 0s 223us/step - loss: 3.3038e-04 - val_loss: 3.3195e-04\n",
            "Epoch 965/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 3.4206e-04 - val_loss: 3.3378e-04\n",
            "Epoch 966/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 3.3949e-04 - val_loss: 4.1289e-04\n",
            "Epoch 967/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 3.5347e-04 - val_loss: 3.1819e-04\n",
            "Epoch 968/1000\n",
            "160/160 [==============================] - 0s 206us/step - loss: 3.3295e-04 - val_loss: 3.1770e-04\n",
            "Epoch 969/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 3.3109e-04 - val_loss: 3.8630e-04\n",
            "Epoch 970/1000\n",
            "160/160 [==============================] - 0s 187us/step - loss: 3.3220e-04 - val_loss: 3.3522e-04\n",
            "Epoch 971/1000\n",
            "160/160 [==============================] - 0s 201us/step - loss: 3.2600e-04 - val_loss: 3.9379e-04\n",
            "Epoch 972/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 3.3090e-04 - val_loss: 3.1450e-04\n",
            "Epoch 973/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 3.1388e-04 - val_loss: 3.0974e-04\n",
            "Epoch 974/1000\n",
            "160/160 [==============================] - 0s 207us/step - loss: 3.1095e-04 - val_loss: 3.3468e-04\n",
            "Epoch 975/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 3.1546e-04 - val_loss: 3.0793e-04\n",
            "Epoch 976/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 3.1304e-04 - val_loss: 3.0944e-04\n",
            "Epoch 977/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 3.1017e-04 - val_loss: 3.3313e-04\n",
            "Epoch 978/1000\n",
            "160/160 [==============================] - 0s 208us/step - loss: 3.0290e-04 - val_loss: 3.0222e-04\n",
            "Epoch 979/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 2.9807e-04 - val_loss: 3.1359e-04\n",
            "Epoch 980/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 2.9638e-04 - val_loss: 3.0451e-04\n",
            "Epoch 981/1000\n",
            "160/160 [==============================] - 0s 197us/step - loss: 2.9532e-04 - val_loss: 3.0820e-04\n",
            "Epoch 982/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 2.9324e-04 - val_loss: 2.9679e-04\n",
            "Epoch 983/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 3.0098e-04 - val_loss: 3.1828e-04\n",
            "Epoch 984/1000\n",
            "160/160 [==============================] - 0s 191us/step - loss: 2.9149e-04 - val_loss: 2.9148e-04\n",
            "Epoch 985/1000\n",
            "160/160 [==============================] - 0s 210us/step - loss: 2.8860e-04 - val_loss: 3.0032e-04\n",
            "Epoch 986/1000\n",
            "160/160 [==============================] - 0s 227us/step - loss: 2.8612e-04 - val_loss: 2.8867e-04\n",
            "Epoch 987/1000\n",
            "160/160 [==============================] - 0s 211us/step - loss: 2.8465e-04 - val_loss: 3.2882e-04\n",
            "Epoch 988/1000\n",
            "160/160 [==============================] - 0s 199us/step - loss: 2.9324e-04 - val_loss: 3.0348e-04\n",
            "Epoch 989/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 2.9735e-04 - val_loss: 2.9766e-04\n",
            "Epoch 990/1000\n",
            "160/160 [==============================] - 0s 254us/step - loss: 2.8457e-04 - val_loss: 2.9582e-04\n",
            "Epoch 991/1000\n",
            "160/160 [==============================] - 0s 204us/step - loss: 2.8289e-04 - val_loss: 2.8096e-04\n",
            "Epoch 992/1000\n",
            "160/160 [==============================] - 0s 190us/step - loss: 2.8483e-04 - val_loss: 3.0565e-04\n",
            "Epoch 993/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 2.8307e-04 - val_loss: 2.9865e-04\n",
            "Epoch 994/1000\n",
            "160/160 [==============================] - 0s 189us/step - loss: 2.8203e-04 - val_loss: 2.8540e-04\n",
            "Epoch 995/1000\n",
            "160/160 [==============================] - 0s 203us/step - loss: 2.7697e-04 - val_loss: 2.9350e-04\n",
            "Epoch 996/1000\n",
            "160/160 [==============================] - 0s 192us/step - loss: 2.7154e-04 - val_loss: 2.8259e-04\n",
            "Epoch 997/1000\n",
            "160/160 [==============================] - 0s 218us/step - loss: 2.6892e-04 - val_loss: 2.8065e-04\n",
            "Epoch 998/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 2.6663e-04 - val_loss: 2.7512e-04\n",
            "Epoch 999/1000\n",
            "160/160 [==============================] - 0s 213us/step - loss: 2.6995e-04 - val_loss: 2.8829e-04\n",
            "Epoch 1000/1000\n",
            "160/160 [==============================] - 0s 194us/step - loss: 2.6976e-04 - val_loss: 2.7119e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJrPNKhgCIEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee9HR3NGFo7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66c941be-3ec0-44f7-8b4d-17c173f7b81f"
      },
      "source": [
        "z.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xf06QLHCRCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9801ffd5-2f99-4b41-81a9-f3a66bf7cfb6"
      },
      "source": [
        "plt.scatter(range(40),results,c='r')\n",
        "plt.scatter(range(40),y_test,c='g')\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAQrPssYDsq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}