{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15MI449.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uiOG8-DPNvn",
        "colab_type": "code",
        "outputId": "058aa236-ded5-4e7a-a9af-c6a546dbdded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.layers import SimpleRNN,LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8q1bztQTGO",
        "colab_type": "code",
        "outputId": "2eab587a-e889-45ea-d5a9-cab4bbdd90cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X = list()\n",
        "Y = list()\n",
        "X = [x+1 for x in range(20)]\n",
        "Y = [y * 10 for y in X]\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guraDXW9QgFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = array(X).reshape(20, 1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7o1FJWxRZch",
        "colab_type": "code",
        "outputId": "36e23cfa-89b9-477d-bfc4-f64fce20caa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTJxxE3Sbpr",
        "colab_type": "code",
        "outputId": "b90f326b-d388-409c-bda3-7c7b0d597e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(1, 1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGCyrnzNTFom",
        "colab_type": "code",
        "outputId": "6c538180-cd47-4ddb-c648-07cc7a2151ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 50)                10400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 10,451\n",
            "Trainable params: 10,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjDtDx1qTlHh",
        "colab_type": "code",
        "outputId": "fb107299-a5f0-4233-d700-f602b5370ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, Y, epochs=1000, validation_split=0.2, batch_size=5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 16 samples, validate on 4 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "16/16 [==============================] - 1s 78ms/step - loss: 9411.1310 - val_loss: 34574.5938\n",
            "Epoch 2/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9390.5413 - val_loss: 34500.1797\n",
            "Epoch 3/1000\n",
            "16/16 [==============================] - 0s 939us/step - loss: 9372.3376 - val_loss: 34427.8594\n",
            "Epoch 4/1000\n",
            "16/16 [==============================] - 0s 885us/step - loss: 9350.7268 - val_loss: 34354.5273\n",
            "Epoch 5/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9334.1232 - val_loss: 34274.6172\n",
            "Epoch 6/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9310.9553 - val_loss: 34198.0273\n",
            "Epoch 7/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9290.8647 - val_loss: 34118.8594\n",
            "Epoch 8/1000\n",
            "16/16 [==============================] - 0s 854us/step - loss: 9270.5091 - val_loss: 34030.1172\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - 0s 870us/step - loss: 9248.4915 - val_loss: 33938.0352\n",
            "Epoch 10/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9226.9653 - val_loss: 33847.4062\n",
            "Epoch 11/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9202.2787 - val_loss: 33751.4219\n",
            "Epoch 12/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9179.2549 - val_loss: 33644.4609\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 9153.2298 - val_loss: 33532.3477\n",
            "Epoch 14/1000\n",
            "16/16 [==============================] - 0s 900us/step - loss: 9127.5831 - val_loss: 33407.5859\n",
            "Epoch 15/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9095.2687 - val_loss: 33283.5586\n",
            "Epoch 16/1000\n",
            "16/16 [==============================] - 0s 948us/step - loss: 9064.6016 - val_loss: 33148.3398\n",
            "Epoch 17/1000\n",
            "16/16 [==============================] - 0s 879us/step - loss: 9030.1976 - val_loss: 32994.1289\n",
            "Epoch 18/1000\n",
            "16/16 [==============================] - 0s 884us/step - loss: 8996.9469 - val_loss: 32821.0508\n",
            "Epoch 19/1000\n",
            "16/16 [==============================] - 0s 831us/step - loss: 8951.4623 - val_loss: 32642.8770\n",
            "Epoch 20/1000\n",
            "16/16 [==============================] - 0s 815us/step - loss: 8909.2097 - val_loss: 32461.5117\n",
            "Epoch 21/1000\n",
            "16/16 [==============================] - 0s 828us/step - loss: 8864.0140 - val_loss: 32279.6035\n",
            "Epoch 22/1000\n",
            "16/16 [==============================] - 0s 856us/step - loss: 8821.5447 - val_loss: 32080.5117\n",
            "Epoch 23/1000\n",
            "16/16 [==============================] - 0s 856us/step - loss: 8771.8315 - val_loss: 31851.3457\n",
            "Epoch 24/1000\n",
            "16/16 [==============================] - 0s 815us/step - loss: 8719.9660 - val_loss: 31591.2148\n",
            "Epoch 25/1000\n",
            "16/16 [==============================] - 0s 802us/step - loss: 8656.0016 - val_loss: 31319.2754\n",
            "Epoch 26/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 8592.6174 - val_loss: 31032.2617\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 8520.3980 - val_loss: 30748.2812\n",
            "Epoch 28/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 8449.5257 - val_loss: 30450.3555\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - 0s 960us/step - loss: 8383.1887 - val_loss: 30162.6406\n",
            "Epoch 30/1000\n",
            "16/16 [==============================] - 0s 895us/step - loss: 8310.3275 - val_loss: 29873.7734\n",
            "Epoch 31/1000\n",
            "16/16 [==============================] - 0s 894us/step - loss: 8234.5468 - val_loss: 29557.4512\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - 0s 888us/step - loss: 8163.6380 - val_loss: 29233.3008\n",
            "Epoch 33/1000\n",
            "16/16 [==============================] - 0s 920us/step - loss: 8088.7502 - val_loss: 28918.4258\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - 0s 926us/step - loss: 8006.6974 - val_loss: 28621.1445\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7934.9183 - val_loss: 28326.8145\n",
            "Epoch 36/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7857.5166 - val_loss: 28037.5449\n",
            "Epoch 37/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7778.2097 - val_loss: 27734.5430\n",
            "Epoch 38/1000\n",
            "16/16 [==============================] - 0s 953us/step - loss: 7698.1478 - val_loss: 27385.3926\n",
            "Epoch 39/1000\n",
            "16/16 [==============================] - 0s 985us/step - loss: 7596.9340 - val_loss: 27029.3340\n",
            "Epoch 40/1000\n",
            "16/16 [==============================] - 0s 965us/step - loss: 7505.8563 - val_loss: 26680.8574\n",
            "Epoch 41/1000\n",
            "16/16 [==============================] - 0s 869us/step - loss: 7414.8302 - val_loss: 26326.2266\n",
            "Epoch 42/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7313.2967 - val_loss: 25947.4707\n",
            "Epoch 43/1000\n",
            "16/16 [==============================] - 0s 909us/step - loss: 7217.8953 - val_loss: 25577.8926\n",
            "Epoch 44/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7117.0913 - val_loss: 25229.1445\n",
            "Epoch 45/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7023.9415 - val_loss: 24871.9551\n",
            "Epoch 46/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6922.0410 - val_loss: 24496.5430\n",
            "Epoch 47/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6832.6829 - val_loss: 24126.4043\n",
            "Epoch 48/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6723.1985 - val_loss: 23767.1094\n",
            "Epoch 49/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 6633.2246 - val_loss: 23378.4160\n",
            "Epoch 50/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6496.0609 - val_loss: 23035.1582\n",
            "Epoch 51/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6406.1773 - val_loss: 22667.0547\n",
            "Epoch 52/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6292.5416 - val_loss: 22315.2695\n",
            "Epoch 53/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6198.7825 - val_loss: 21941.9141\n",
            "Epoch 54/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6090.6282 - val_loss: 21578.1406\n",
            "Epoch 55/1000\n",
            "16/16 [==============================] - 0s 909us/step - loss: 5983.9945 - val_loss: 21220.2305\n",
            "Epoch 56/1000\n",
            "16/16 [==============================] - 0s 915us/step - loss: 5874.4728 - val_loss: 20864.4609\n",
            "Epoch 57/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5772.7386 - val_loss: 20507.8027\n",
            "Epoch 58/1000\n",
            "16/16 [==============================] - 0s 994us/step - loss: 5669.1761 - val_loss: 20153.7461\n",
            "Epoch 59/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5553.9958 - val_loss: 19821.3633\n",
            "Epoch 60/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5455.8758 - val_loss: 19472.6270\n",
            "Epoch 61/1000\n",
            "16/16 [==============================] - 0s 966us/step - loss: 5356.4511 - val_loss: 19143.0957\n",
            "Epoch 62/1000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 5254.1168 - val_loss: 18834.0215\n",
            "Epoch 63/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5163.2591 - val_loss: 18526.3633\n",
            "Epoch 64/1000\n",
            "16/16 [==============================] - 0s 935us/step - loss: 5077.1549 - val_loss: 18222.0000\n",
            "Epoch 65/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4982.7228 - val_loss: 17915.9805\n",
            "Epoch 66/1000\n",
            "16/16 [==============================] - 0s 993us/step - loss: 4890.3353 - val_loss: 17580.1855\n",
            "Epoch 67/1000\n",
            "16/16 [==============================] - 0s 1000us/step - loss: 4803.5808 - val_loss: 17253.7207\n",
            "Epoch 68/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4705.9508 - val_loss: 16938.3262\n",
            "Epoch 69/1000\n",
            "16/16 [==============================] - 0s 987us/step - loss: 4617.3667 - val_loss: 16626.3164\n",
            "Epoch 70/1000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 4528.0690 - val_loss: 16320.2900\n",
            "Epoch 71/1000\n",
            "16/16 [==============================] - 0s 870us/step - loss: 4439.2359 - val_loss: 16023.0312\n",
            "Epoch 72/1000\n",
            "16/16 [==============================] - 0s 992us/step - loss: 4350.0567 - val_loss: 15733.9160\n",
            "Epoch 73/1000\n",
            "16/16 [==============================] - 0s 864us/step - loss: 4258.6124 - val_loss: 15441.7266\n",
            "Epoch 74/1000\n",
            "16/16 [==============================] - 0s 928us/step - loss: 4176.6262 - val_loss: 15106.8438\n",
            "Epoch 75/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4087.5669 - val_loss: 14765.8613\n",
            "Epoch 76/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3985.1645 - val_loss: 14451.9629\n",
            "Epoch 77/1000\n",
            "16/16 [==============================] - 0s 967us/step - loss: 3900.6855 - val_loss: 14127.7461\n",
            "Epoch 78/1000\n",
            "16/16 [==============================] - 0s 914us/step - loss: 3811.3161 - val_loss: 13829.1279\n",
            "Epoch 79/1000\n",
            "16/16 [==============================] - 0s 984us/step - loss: 3726.5846 - val_loss: 13548.2852\n",
            "Epoch 80/1000\n",
            "16/16 [==============================] - 0s 907us/step - loss: 3646.7869 - val_loss: 13277.8164\n",
            "Epoch 81/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3566.9336 - val_loss: 12994.0352\n",
            "Epoch 82/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3482.8616 - val_loss: 12678.0732\n",
            "Epoch 83/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3399.9318 - val_loss: 12353.6025\n",
            "Epoch 84/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3310.5685 - val_loss: 12040.1562\n",
            "Epoch 85/1000\n",
            "16/16 [==============================] - 0s 903us/step - loss: 3220.4902 - val_loss: 11729.8711\n",
            "Epoch 86/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3141.0009 - val_loss: 11425.0215\n",
            "Epoch 87/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3054.2476 - val_loss: 11147.8184\n",
            "Epoch 88/1000\n",
            "16/16 [==============================] - 0s 994us/step - loss: 2972.0404 - val_loss: 10879.3730\n",
            "Epoch 89/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2898.0007 - val_loss: 10598.4834\n",
            "Epoch 90/1000\n",
            "16/16 [==============================] - 0s 911us/step - loss: 2815.2710 - val_loss: 10306.2344\n",
            "Epoch 91/1000\n",
            "16/16 [==============================] - 0s 759us/step - loss: 2733.4769 - val_loss: 10002.7129\n",
            "Epoch 92/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2657.9584 - val_loss: 9693.2051\n",
            "Epoch 93/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2569.6542 - val_loss: 9421.9238\n",
            "Epoch 94/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2494.0558 - val_loss: 9168.0674\n",
            "Epoch 95/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2425.2667 - val_loss: 8929.7061\n",
            "Epoch 96/1000\n",
            "16/16 [==============================] - 0s 974us/step - loss: 2358.6425 - val_loss: 8702.5293\n",
            "Epoch 97/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2296.1308 - val_loss: 8485.2441\n",
            "Epoch 98/1000\n",
            "16/16 [==============================] - 0s 833us/step - loss: 2233.3363 - val_loss: 8279.1201\n",
            "Epoch 99/1000\n",
            "16/16 [==============================] - 0s 930us/step - loss: 2179.0845 - val_loss: 8071.4443\n",
            "Epoch 100/1000\n",
            "16/16 [==============================] - 0s 964us/step - loss: 2118.2999 - val_loss: 7870.8203\n",
            "Epoch 101/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2060.0907 - val_loss: 7665.6772\n",
            "Epoch 102/1000\n",
            "16/16 [==============================] - 0s 955us/step - loss: 2006.1299 - val_loss: 7452.7759\n",
            "Epoch 103/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1942.4249 - val_loss: 7247.1787\n",
            "Epoch 104/1000\n",
            "16/16 [==============================] - 0s 962us/step - loss: 1885.8833 - val_loss: 7038.7852\n",
            "Epoch 105/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1825.8596 - val_loss: 6818.5454\n",
            "Epoch 106/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1765.4124 - val_loss: 6595.0229\n",
            "Epoch 107/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1707.1823 - val_loss: 6365.8418\n",
            "Epoch 108/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1646.8468 - val_loss: 6159.2124\n",
            "Epoch 109/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1588.8214 - val_loss: 5965.4766\n",
            "Epoch 110/1000\n",
            "16/16 [==============================] - 0s 820us/step - loss: 1534.1057 - val_loss: 5766.5400\n",
            "Epoch 111/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1482.1379 - val_loss: 5570.7617\n",
            "Epoch 112/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1430.2041 - val_loss: 5388.5332\n",
            "Epoch 113/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1377.6172 - val_loss: 5216.1528\n",
            "Epoch 114/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1335.8826 - val_loss: 5032.6719\n",
            "Epoch 115/1000\n",
            "16/16 [==============================] - 0s 994us/step - loss: 1282.6802 - val_loss: 4863.8408\n",
            "Epoch 116/1000\n",
            "16/16 [==============================] - 0s 970us/step - loss: 1232.1198 - val_loss: 4691.1206\n",
            "Epoch 117/1000\n",
            "16/16 [==============================] - 0s 958us/step - loss: 1185.3011 - val_loss: 4515.4521\n",
            "Epoch 118/1000\n",
            "16/16 [==============================] - 0s 900us/step - loss: 1137.1251 - val_loss: 4338.6514\n",
            "Epoch 119/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1093.3163 - val_loss: 4168.2217\n",
            "Epoch 120/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1044.7158 - val_loss: 3998.8608\n",
            "Epoch 121/1000\n",
            "16/16 [==============================] - 0s 916us/step - loss: 1002.3460 - val_loss: 3837.7554\n",
            "Epoch 122/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 957.9074 - val_loss: 3693.0542\n",
            "Epoch 123/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 918.8826 - val_loss: 3560.7324\n",
            "Epoch 124/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 881.3687 - val_loss: 3436.5796\n",
            "Epoch 125/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 850.0489 - val_loss: 3299.7407\n",
            "Epoch 126/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 813.3165 - val_loss: 3164.5640\n",
            "Epoch 127/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 775.9087 - val_loss: 3042.1934\n",
            "Epoch 128/1000\n",
            "16/16 [==============================] - 0s 970us/step - loss: 745.7368 - val_loss: 2918.2180\n",
            "Epoch 129/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 714.0419 - val_loss: 2805.6052\n",
            "Epoch 130/1000\n",
            "16/16 [==============================] - 0s 915us/step - loss: 681.4647 - val_loss: 2701.0459\n",
            "Epoch 131/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 655.4375 - val_loss: 2591.0034\n",
            "Epoch 132/1000\n",
            "16/16 [==============================] - 0s 810us/step - loss: 623.1731 - val_loss: 2487.0120\n",
            "Epoch 133/1000\n",
            "16/16 [==============================] - 0s 827us/step - loss: 598.2303 - val_loss: 2381.3115\n",
            "Epoch 134/1000\n",
            "16/16 [==============================] - 0s 921us/step - loss: 571.2832 - val_loss: 2286.0952\n",
            "Epoch 135/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 547.1249 - val_loss: 2196.7527\n",
            "Epoch 136/1000\n",
            "16/16 [==============================] - 0s 927us/step - loss: 523.1049 - val_loss: 2114.5229\n",
            "Epoch 137/1000\n",
            "16/16 [==============================] - 0s 833us/step - loss: 500.6621 - val_loss: 2036.4487\n",
            "Epoch 138/1000\n",
            "16/16 [==============================] - 0s 919us/step - loss: 479.2328 - val_loss: 1957.4254\n",
            "Epoch 139/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 459.4142 - val_loss: 1881.1566\n",
            "Epoch 140/1000\n",
            "16/16 [==============================] - 0s 797us/step - loss: 441.0569 - val_loss: 1800.6980\n",
            "Epoch 141/1000\n",
            "16/16 [==============================] - 0s 797us/step - loss: 417.7401 - val_loss: 1721.8079\n",
            "Epoch 142/1000\n",
            "16/16 [==============================] - 0s 968us/step - loss: 397.9679 - val_loss: 1648.5540\n",
            "Epoch 143/1000\n",
            "16/16 [==============================] - 0s 988us/step - loss: 379.0478 - val_loss: 1582.1329\n",
            "Epoch 144/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 363.0725 - val_loss: 1517.9482\n",
            "Epoch 145/1000\n",
            "16/16 [==============================] - 0s 838us/step - loss: 346.2382 - val_loss: 1455.6042\n",
            "Epoch 146/1000\n",
            "16/16 [==============================] - 0s 835us/step - loss: 330.4980 - val_loss: 1395.6564\n",
            "Epoch 147/1000\n",
            "16/16 [==============================] - 0s 912us/step - loss: 314.5053 - val_loss: 1336.4827\n",
            "Epoch 148/1000\n",
            "16/16 [==============================] - 0s 917us/step - loss: 299.8161 - val_loss: 1278.8302\n",
            "Epoch 149/1000\n",
            "16/16 [==============================] - 0s 902us/step - loss: 284.0236 - val_loss: 1216.9487\n",
            "Epoch 150/1000\n",
            "16/16 [==============================] - 0s 789us/step - loss: 267.5151 - val_loss: 1155.2543\n",
            "Epoch 151/1000\n",
            "16/16 [==============================] - 0s 821us/step - loss: 253.2280 - val_loss: 1091.4453\n",
            "Epoch 152/1000\n",
            "16/16 [==============================] - 0s 797us/step - loss: 237.5065 - val_loss: 1030.1642\n",
            "Epoch 153/1000\n",
            "16/16 [==============================] - 0s 832us/step - loss: 223.2950 - val_loss: 977.6266\n",
            "Epoch 154/1000\n",
            "16/16 [==============================] - 0s 837us/step - loss: 209.8023 - val_loss: 930.4330\n",
            "Epoch 155/1000\n",
            "16/16 [==============================] - 0s 814us/step - loss: 198.7512 - val_loss: 880.9414\n",
            "Epoch 156/1000\n",
            "16/16 [==============================] - 0s 877us/step - loss: 187.1340 - val_loss: 835.5263\n",
            "Epoch 157/1000\n",
            "16/16 [==============================] - 0s 742us/step - loss: 175.4408 - val_loss: 796.4701\n",
            "Epoch 158/1000\n",
            "16/16 [==============================] - 0s 856us/step - loss: 166.9574 - val_loss: 759.9839\n",
            "Epoch 159/1000\n",
            "16/16 [==============================] - 0s 738us/step - loss: 157.7092 - val_loss: 726.8164\n",
            "Epoch 160/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 150.3248 - val_loss: 695.1316\n",
            "Epoch 161/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 142.2672 - val_loss: 666.2317\n",
            "Epoch 162/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 135.5453 - val_loss: 638.3914\n",
            "Epoch 163/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 128.8137 - val_loss: 611.9133\n",
            "Epoch 164/1000\n",
            "16/16 [==============================] - 0s 927us/step - loss: 122.6528 - val_loss: 586.1801\n",
            "Epoch 165/1000\n",
            "16/16 [==============================] - 0s 793us/step - loss: 116.4365 - val_loss: 561.8891\n",
            "Epoch 166/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 110.6876 - val_loss: 538.7732\n",
            "Epoch 167/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 105.4695 - val_loss: 516.4973\n",
            "Epoch 168/1000\n",
            "16/16 [==============================] - 0s 996us/step - loss: 100.2613 - val_loss: 495.7144\n",
            "Epoch 169/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 95.7609 - val_loss: 475.4990\n",
            "Epoch 170/1000\n",
            "16/16 [==============================] - 0s 965us/step - loss: 90.8810 - val_loss: 456.3979\n",
            "Epoch 171/1000\n",
            "16/16 [==============================] - 0s 853us/step - loss: 86.2803 - val_loss: 437.2950\n",
            "Epoch 172/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 81.9246 - val_loss: 416.4522\n",
            "Epoch 173/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 77.0865 - val_loss: 397.2153\n",
            "Epoch 174/1000\n",
            "16/16 [==============================] - 0s 795us/step - loss: 73.1824 - val_loss: 377.8732\n",
            "Epoch 175/1000\n",
            "16/16 [==============================] - 0s 998us/step - loss: 68.9182 - val_loss: 360.3482\n",
            "Epoch 176/1000\n",
            "16/16 [==============================] - 0s 991us/step - loss: 64.9531 - val_loss: 344.4398\n",
            "Epoch 177/1000\n",
            "16/16 [==============================] - 0s 959us/step - loss: 61.6374 - val_loss: 328.7487\n",
            "Epoch 178/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 57.9680 - val_loss: 311.9446\n",
            "Epoch 179/1000\n",
            "16/16 [==============================] - 0s 892us/step - loss: 54.6466 - val_loss: 294.8996\n",
            "Epoch 180/1000\n",
            "16/16 [==============================] - 0s 858us/step - loss: 50.8832 - val_loss: 280.2485\n",
            "Epoch 181/1000\n",
            "16/16 [==============================] - 0s 887us/step - loss: 47.8645 - val_loss: 266.8431\n",
            "Epoch 182/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 45.2256 - val_loss: 254.4882\n",
            "Epoch 183/1000\n",
            "16/16 [==============================] - 0s 876us/step - loss: 42.7219 - val_loss: 242.5674\n",
            "Epoch 184/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 40.1888 - val_loss: 230.5818\n",
            "Epoch 185/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 37.7940 - val_loss: 218.7188\n",
            "Epoch 186/1000\n",
            "16/16 [==============================] - 0s 957us/step - loss: 35.7050 - val_loss: 207.8394\n",
            "Epoch 187/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 33.5186 - val_loss: 198.4048\n",
            "Epoch 188/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 31.7824 - val_loss: 189.8376\n",
            "Epoch 189/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 30.1036 - val_loss: 182.0623\n",
            "Epoch 190/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 28.5390 - val_loss: 174.4356\n",
            "Epoch 191/1000\n",
            "16/16 [==============================] - 0s 951us/step - loss: 27.2545 - val_loss: 165.7791\n",
            "Epoch 192/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 25.5937 - val_loss: 158.2979\n",
            "Epoch 193/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 24.2147 - val_loss: 151.5700\n",
            "Epoch 194/1000\n",
            "16/16 [==============================] - 0s 910us/step - loss: 23.0234 - val_loss: 144.7218\n",
            "Epoch 195/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 21.7160 - val_loss: 138.0280\n",
            "Epoch 196/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 20.6072 - val_loss: 132.0154\n",
            "Epoch 197/1000\n",
            "16/16 [==============================] - 0s 949us/step - loss: 19.5424 - val_loss: 126.3281\n",
            "Epoch 198/1000\n",
            "16/16 [==============================] - 0s 836us/step - loss: 18.5784 - val_loss: 120.0553\n",
            "Epoch 199/1000\n",
            "16/16 [==============================] - 0s 914us/step - loss: 17.5647 - val_loss: 113.7048\n",
            "Epoch 200/1000\n",
            "16/16 [==============================] - 0s 992us/step - loss: 16.5249 - val_loss: 108.0539\n",
            "Epoch 201/1000\n",
            "16/16 [==============================] - 0s 981us/step - loss: 15.6543 - val_loss: 102.4009\n",
            "Epoch 202/1000\n",
            "16/16 [==============================] - 0s 840us/step - loss: 14.7956 - val_loss: 97.7292\n",
            "Epoch 203/1000\n",
            "16/16 [==============================] - 0s 972us/step - loss: 14.0442 - val_loss: 93.7957\n",
            "Epoch 204/1000\n",
            "16/16 [==============================] - 0s 906us/step - loss: 13.4903 - val_loss: 89.9780\n",
            "Epoch 205/1000\n",
            "16/16 [==============================] - 0s 940us/step - loss: 12.9379 - val_loss: 85.9579\n",
            "Epoch 206/1000\n",
            "16/16 [==============================] - 0s 837us/step - loss: 12.2585 - val_loss: 82.4083\n",
            "Epoch 207/1000\n",
            "16/16 [==============================] - 0s 931us/step - loss: 11.8322 - val_loss: 78.9441\n",
            "Epoch 208/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 11.4651 - val_loss: 75.8322\n",
            "Epoch 209/1000\n",
            "16/16 [==============================] - 0s 927us/step - loss: 10.9814 - val_loss: 73.2767\n",
            "Epoch 210/1000\n",
            "16/16 [==============================] - 0s 925us/step - loss: 10.6342 - val_loss: 71.0697\n",
            "Epoch 211/1000\n",
            "16/16 [==============================] - 0s 882us/step - loss: 10.3736 - val_loss: 69.0114\n",
            "Epoch 212/1000\n",
            "16/16 [==============================] - 0s 868us/step - loss: 10.1041 - val_loss: 67.0829\n",
            "Epoch 213/1000\n",
            "16/16 [==============================] - 0s 892us/step - loss: 9.8418 - val_loss: 65.2998\n",
            "Epoch 214/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 9.6432 - val_loss: 63.3192\n",
            "Epoch 215/1000\n",
            "16/16 [==============================] - 0s 970us/step - loss: 9.4172 - val_loss: 61.5249\n",
            "Epoch 216/1000\n",
            "16/16 [==============================] - 0s 845us/step - loss: 9.2056 - val_loss: 59.9921\n",
            "Epoch 217/1000\n",
            "16/16 [==============================] - 0s 986us/step - loss: 9.0075 - val_loss: 58.6026\n",
            "Epoch 218/1000\n",
            "16/16 [==============================] - 0s 869us/step - loss: 8.8281 - val_loss: 57.2684\n",
            "Epoch 219/1000\n",
            "16/16 [==============================] - 0s 919us/step - loss: 8.6907 - val_loss: 55.9690\n",
            "Epoch 220/1000\n",
            "16/16 [==============================] - 0s 997us/step - loss: 8.5341 - val_loss: 54.8083\n",
            "Epoch 221/1000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 8.3920 - val_loss: 53.6556\n",
            "Epoch 222/1000\n",
            "16/16 [==============================] - 0s 938us/step - loss: 8.2723 - val_loss: 52.4506\n",
            "Epoch 223/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 8.1554 - val_loss: 51.2621\n",
            "Epoch 224/1000\n",
            "16/16 [==============================] - 0s 870us/step - loss: 8.0291 - val_loss: 50.1430\n",
            "Epoch 225/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.8857 - val_loss: 49.0572\n",
            "Epoch 226/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.7737 - val_loss: 47.9269\n",
            "Epoch 227/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.6867 - val_loss: 46.7748\n",
            "Epoch 228/1000\n",
            "16/16 [==============================] - 0s 961us/step - loss: 7.5560 - val_loss: 45.8193\n",
            "Epoch 229/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.4597 - val_loss: 44.5820\n",
            "Epoch 230/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.3674 - val_loss: 43.4633\n",
            "Epoch 231/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.2613 - val_loss: 42.5968\n",
            "Epoch 232/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.1818 - val_loss: 41.9031\n",
            "Epoch 233/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.1256 - val_loss: 41.3163\n",
            "Epoch 234/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 7.0699 - val_loss: 40.7497\n",
            "Epoch 235/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 7.0128 - val_loss: 40.1039\n",
            "Epoch 236/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.9577 - val_loss: 39.4370\n",
            "Epoch 237/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.8952 - val_loss: 38.9324\n",
            "Epoch 238/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.8529 - val_loss: 38.2323\n",
            "Epoch 239/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.7998 - val_loss: 37.4595\n",
            "Epoch 240/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.7311 - val_loss: 36.8137\n",
            "Epoch 241/1000\n",
            "16/16 [==============================] - 0s 990us/step - loss: 6.6904 - val_loss: 36.1418\n",
            "Epoch 242/1000\n",
            "16/16 [==============================] - 0s 915us/step - loss: 6.6474 - val_loss: 35.5308\n",
            "Epoch 243/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.5921 - val_loss: 34.8463\n",
            "Epoch 244/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.5388 - val_loss: 34.1904\n",
            "Epoch 245/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.4954 - val_loss: 33.5852\n",
            "Epoch 246/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.4519 - val_loss: 32.9910\n",
            "Epoch 247/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.4168 - val_loss: 32.2380\n",
            "Epoch 248/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.3783 - val_loss: 31.6123\n",
            "Epoch 249/1000\n",
            "16/16 [==============================] - 0s 942us/step - loss: 6.3496 - val_loss: 31.1769\n",
            "Epoch 250/1000\n",
            "16/16 [==============================] - 0s 969us/step - loss: 6.3260 - val_loss: 30.7062\n",
            "Epoch 251/1000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 6.3028 - val_loss: 30.2954\n",
            "Epoch 252/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.2662 - val_loss: 30.0055\n",
            "Epoch 253/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.2583 - val_loss: 29.6999\n",
            "Epoch 254/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.2324 - val_loss: 29.6540\n",
            "Epoch 255/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.2234 - val_loss: 29.5643\n",
            "Epoch 256/1000\n",
            "16/16 [==============================] - 0s 932us/step - loss: 6.2064 - val_loss: 29.3832\n",
            "Epoch 257/1000\n",
            "16/16 [==============================] - 0s 966us/step - loss: 6.1903 - val_loss: 29.1477\n",
            "Epoch 258/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.1736 - val_loss: 28.7796\n",
            "Epoch 259/1000\n",
            "16/16 [==============================] - 0s 910us/step - loss: 6.1523 - val_loss: 28.2605\n",
            "Epoch 260/1000\n",
            "16/16 [==============================] - 0s 993us/step - loss: 6.1242 - val_loss: 27.9468\n",
            "Epoch 261/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.1128 - val_loss: 27.6455\n",
            "Epoch 262/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.0949 - val_loss: 27.3996\n",
            "Epoch 263/1000\n",
            "16/16 [==============================] - 0s 946us/step - loss: 6.0721 - val_loss: 27.1740\n",
            "Epoch 264/1000\n",
            "16/16 [==============================] - 0s 958us/step - loss: 6.0512 - val_loss: 26.9305\n",
            "Epoch 265/1000\n",
            "16/16 [==============================] - 0s 869us/step - loss: 6.0402 - val_loss: 26.6876\n",
            "Epoch 266/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.0179 - val_loss: 26.4922\n",
            "Epoch 267/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 6.0075 - val_loss: 26.2818\n",
            "Epoch 268/1000\n",
            "16/16 [==============================] - 0s 879us/step - loss: 5.9895 - val_loss: 26.1173\n",
            "Epoch 269/1000\n",
            "16/16 [==============================] - 0s 931us/step - loss: 5.9734 - val_loss: 25.9795\n",
            "Epoch 270/1000\n",
            "16/16 [==============================] - 0s 864us/step - loss: 5.9599 - val_loss: 25.9554\n",
            "Epoch 271/1000\n",
            "16/16 [==============================] - 0s 993us/step - loss: 5.9509 - val_loss: 25.9608\n",
            "Epoch 272/1000\n",
            "16/16 [==============================] - 0s 871us/step - loss: 5.9460 - val_loss: 26.0667\n",
            "Epoch 273/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.9299 - val_loss: 25.9672\n",
            "Epoch 274/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.9159 - val_loss: 25.8477\n",
            "Epoch 275/1000\n",
            "16/16 [==============================] - 0s 922us/step - loss: 5.9046 - val_loss: 25.6906\n",
            "Epoch 276/1000\n",
            "16/16 [==============================] - 0s 864us/step - loss: 5.8872 - val_loss: 25.6622\n",
            "Epoch 277/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.8766 - val_loss: 25.6716\n",
            "Epoch 278/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.8617 - val_loss: 25.7035\n",
            "Epoch 279/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.8542 - val_loss: 25.7603\n",
            "Epoch 280/1000\n",
            "16/16 [==============================] - 0s 933us/step - loss: 5.8387 - val_loss: 25.8762\n",
            "Epoch 281/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.8316 - val_loss: 26.0616\n",
            "Epoch 282/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.8251 - val_loss: 26.1328\n",
            "Epoch 283/1000\n",
            "16/16 [==============================] - 0s 883us/step - loss: 5.8174 - val_loss: 26.3423\n",
            "Epoch 284/1000\n",
            "16/16 [==============================] - 0s 951us/step - loss: 5.8066 - val_loss: 26.4742\n",
            "Epoch 285/1000\n",
            "16/16 [==============================] - 0s 895us/step - loss: 5.8000 - val_loss: 26.6265\n",
            "Epoch 286/1000\n",
            "16/16 [==============================] - 0s 972us/step - loss: 5.7915 - val_loss: 26.8021\n",
            "Epoch 287/1000\n",
            "16/16 [==============================] - 0s 968us/step - loss: 5.7860 - val_loss: 26.9011\n",
            "Epoch 288/1000\n",
            "16/16 [==============================] - 0s 861us/step - loss: 5.7823 - val_loss: 26.9771\n",
            "Epoch 289/1000\n",
            "16/16 [==============================] - 0s 842us/step - loss: 5.7738 - val_loss: 26.7801\n",
            "Epoch 290/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.7561 - val_loss: 26.5706\n",
            "Epoch 291/1000\n",
            "16/16 [==============================] - 0s 911us/step - loss: 5.7429 - val_loss: 26.2069\n",
            "Epoch 292/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.7305 - val_loss: 25.8762\n",
            "Epoch 293/1000\n",
            "16/16 [==============================] - 0s 815us/step - loss: 5.7148 - val_loss: 25.5964\n",
            "Epoch 294/1000\n",
            "16/16 [==============================] - 0s 841us/step - loss: 5.6985 - val_loss: 25.2465\n",
            "Epoch 295/1000\n",
            "16/16 [==============================] - 0s 909us/step - loss: 5.6920 - val_loss: 24.7707\n",
            "Epoch 296/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.6852 - val_loss: 24.3977\n",
            "Epoch 297/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.6614 - val_loss: 24.2334\n",
            "Epoch 298/1000\n",
            "16/16 [==============================] - 0s 914us/step - loss: 5.6470 - val_loss: 23.9971\n",
            "Epoch 299/1000\n",
            "16/16 [==============================] - 0s 923us/step - loss: 5.6336 - val_loss: 23.6951\n",
            "Epoch 300/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.6265 - val_loss: 23.6089\n",
            "Epoch 301/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.6159 - val_loss: 23.6545\n",
            "Epoch 302/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.6023 - val_loss: 23.6566\n",
            "Epoch 303/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5886 - val_loss: 23.8285\n",
            "Epoch 304/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5719 - val_loss: 24.0142\n",
            "Epoch 305/1000\n",
            "16/16 [==============================] - 0s 984us/step - loss: 5.5635 - val_loss: 24.1334\n",
            "Epoch 306/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5536 - val_loss: 24.3620\n",
            "Epoch 307/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5457 - val_loss: 24.6449\n",
            "Epoch 308/1000\n",
            "16/16 [==============================] - 0s 867us/step - loss: 5.5332 - val_loss: 24.9584\n",
            "Epoch 309/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5316 - val_loss: 25.2946\n",
            "Epoch 310/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5291 - val_loss: 25.6160\n",
            "Epoch 311/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5191 - val_loss: 25.9074\n",
            "Epoch 312/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5145 - val_loss: 26.0598\n",
            "Epoch 313/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.5029 - val_loss: 26.0739\n",
            "Epoch 314/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.4920 - val_loss: 26.1956\n",
            "Epoch 315/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 5.4843 - val_loss: 26.2245\n",
            "Epoch 316/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.4768 - val_loss: 26.1831\n",
            "Epoch 317/1000\n",
            "16/16 [==============================] - 0s 873us/step - loss: 5.4636 - val_loss: 25.8909\n",
            "Epoch 318/1000\n",
            "16/16 [==============================] - 0s 953us/step - loss: 5.4492 - val_loss: 25.4905\n",
            "Epoch 319/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.4428 - val_loss: 25.1319\n",
            "Epoch 320/1000\n",
            "16/16 [==============================] - 0s 869us/step - loss: 5.4330 - val_loss: 24.8508\n",
            "Epoch 321/1000\n",
            "16/16 [==============================] - 0s 934us/step - loss: 5.4248 - val_loss: 24.6751\n",
            "Epoch 322/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.4120 - val_loss: 24.6195\n",
            "Epoch 323/1000\n",
            "16/16 [==============================] - 0s 799us/step - loss: 5.4000 - val_loss: 24.5278\n",
            "Epoch 324/1000\n",
            "16/16 [==============================] - 0s 987us/step - loss: 5.3909 - val_loss: 24.3064\n",
            "Epoch 325/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.3771 - val_loss: 23.9097\n",
            "Epoch 326/1000\n",
            "16/16 [==============================] - 0s 886us/step - loss: 5.3694 - val_loss: 23.6771\n",
            "Epoch 327/1000\n",
            "16/16 [==============================] - 0s 909us/step - loss: 5.3609 - val_loss: 23.6458\n",
            "Epoch 328/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.3509 - val_loss: 23.6554\n",
            "Epoch 329/1000\n",
            "16/16 [==============================] - 0s 918us/step - loss: 5.3411 - val_loss: 23.8722\n",
            "Epoch 330/1000\n",
            "16/16 [==============================] - 0s 900us/step - loss: 5.3363 - val_loss: 24.1493\n",
            "Epoch 331/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.3183 - val_loss: 24.2867\n",
            "Epoch 332/1000\n",
            "16/16 [==============================] - 0s 840us/step - loss: 5.3139 - val_loss: 24.4252\n",
            "Epoch 333/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.3003 - val_loss: 24.5591\n",
            "Epoch 334/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2937 - val_loss: 24.7620\n",
            "Epoch 335/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2891 - val_loss: 24.9873\n",
            "Epoch 336/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2764 - val_loss: 24.9299\n",
            "Epoch 337/1000\n",
            "16/16 [==============================] - 0s 957us/step - loss: 5.2719 - val_loss: 24.8654\n",
            "Epoch 338/1000\n",
            "16/16 [==============================] - 0s 886us/step - loss: 5.2591 - val_loss: 25.0296\n",
            "Epoch 339/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2503 - val_loss: 25.2182\n",
            "Epoch 340/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2429 - val_loss: 25.3500\n",
            "Epoch 341/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2348 - val_loss: 25.4006\n",
            "Epoch 342/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2304 - val_loss: 25.3064\n",
            "Epoch 343/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.2187 - val_loss: 25.3991\n",
            "Epoch 344/1000\n",
            "16/16 [==============================] - 0s 905us/step - loss: 5.2080 - val_loss: 25.3704\n",
            "Epoch 345/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 5.1993 - val_loss: 25.2284\n",
            "Epoch 346/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1894 - val_loss: 24.9291\n",
            "Epoch 347/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1815 - val_loss: 24.7159\n",
            "Epoch 348/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1684 - val_loss: 24.6245\n",
            "Epoch 349/1000\n",
            "16/16 [==============================] - 0s 912us/step - loss: 5.1619 - val_loss: 24.5377\n",
            "Epoch 350/1000\n",
            "16/16 [==============================] - 0s 943us/step - loss: 5.1485 - val_loss: 24.6758\n",
            "Epoch 351/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1417 - val_loss: 24.8552\n",
            "Epoch 352/1000\n",
            "16/16 [==============================] - 0s 993us/step - loss: 5.1301 - val_loss: 24.9721\n",
            "Epoch 353/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1261 - val_loss: 25.1955\n",
            "Epoch 354/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1168 - val_loss: 25.3464\n",
            "Epoch 355/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.1110 - val_loss: 25.5018\n",
            "Epoch 356/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.0977 - val_loss: 25.5602\n",
            "Epoch 357/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.0885 - val_loss: 25.5445\n",
            "Epoch 358/1000\n",
            "16/16 [==============================] - 0s 996us/step - loss: 5.0804 - val_loss: 25.4698\n",
            "Epoch 359/1000\n",
            "16/16 [==============================] - 0s 941us/step - loss: 5.0716 - val_loss: 25.4209\n",
            "Epoch 360/1000\n",
            "16/16 [==============================] - 0s 988us/step - loss: 5.0607 - val_loss: 25.2398\n",
            "Epoch 361/1000\n",
            "16/16 [==============================] - 0s 934us/step - loss: 5.0607 - val_loss: 24.9632\n",
            "Epoch 362/1000\n",
            "16/16 [==============================] - 0s 960us/step - loss: 5.0392 - val_loss: 24.9108\n",
            "Epoch 363/1000\n",
            "16/16 [==============================] - 0s 910us/step - loss: 5.0301 - val_loss: 24.8714\n",
            "Epoch 364/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.0169 - val_loss: 24.9853\n",
            "Epoch 365/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.0091 - val_loss: 25.1391\n",
            "Epoch 366/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 5.0020 - val_loss: 25.1704\n",
            "Epoch 367/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.9863 - val_loss: 24.9491\n",
            "Epoch 368/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.9782 - val_loss: 24.8456\n",
            "Epoch 369/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.9701 - val_loss: 24.7067\n",
            "Epoch 370/1000\n",
            "16/16 [==============================] - 0s 936us/step - loss: 4.9625 - val_loss: 24.7287\n",
            "Epoch 371/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.9508 - val_loss: 24.6230\n",
            "Epoch 372/1000\n",
            "16/16 [==============================] - 0s 991us/step - loss: 4.9426 - val_loss: 24.6458\n",
            "Epoch 373/1000\n",
            "16/16 [==============================] - 0s 970us/step - loss: 4.9331 - val_loss: 24.8089\n",
            "Epoch 374/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.9263 - val_loss: 24.8412\n",
            "Epoch 375/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.9174 - val_loss: 24.7771\n",
            "Epoch 376/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 4.9087 - val_loss: 24.7742\n",
            "Epoch 377/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.8929 - val_loss: 24.8066\n",
            "Epoch 378/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.8851 - val_loss: 24.8379\n",
            "Epoch 379/1000\n",
            "16/16 [==============================] - 0s 956us/step - loss: 4.8673 - val_loss: 24.8400\n",
            "Epoch 380/1000\n",
            "16/16 [==============================] - 0s 936us/step - loss: 4.8555 - val_loss: 24.9473\n",
            "Epoch 381/1000\n",
            "16/16 [==============================] - 0s 904us/step - loss: 4.8426 - val_loss: 25.1601\n",
            "Epoch 382/1000\n",
            "16/16 [==============================] - 0s 982us/step - loss: 4.8376 - val_loss: 25.4724\n",
            "Epoch 383/1000\n",
            "16/16 [==============================] - 0s 915us/step - loss: 4.8276 - val_loss: 25.6124\n",
            "Epoch 384/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.8142 - val_loss: 25.6114\n",
            "Epoch 385/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.8110 - val_loss: 25.7619\n",
            "Epoch 386/1000\n",
            "16/16 [==============================] - 0s 994us/step - loss: 4.7949 - val_loss: 25.7346\n",
            "Epoch 387/1000\n",
            "16/16 [==============================] - 0s 950us/step - loss: 4.7872 - val_loss: 25.5468\n",
            "Epoch 388/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 4.7756 - val_loss: 25.4181\n",
            "Epoch 389/1000\n",
            "16/16 [==============================] - 0s 819us/step - loss: 4.7631 - val_loss: 25.3859\n",
            "Epoch 390/1000\n",
            "16/16 [==============================] - 0s 985us/step - loss: 4.7542 - val_loss: 25.3484\n",
            "Epoch 391/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.7515 - val_loss: 25.1444\n",
            "Epoch 392/1000\n",
            "16/16 [==============================] - 0s 998us/step - loss: 4.7380 - val_loss: 25.2055\n",
            "Epoch 393/1000\n",
            "16/16 [==============================] - 0s 920us/step - loss: 4.7314 - val_loss: 25.2536\n",
            "Epoch 394/1000\n",
            "16/16 [==============================] - 0s 921us/step - loss: 4.7232 - val_loss: 25.2997\n",
            "Epoch 395/1000\n",
            "16/16 [==============================] - 0s 833us/step - loss: 4.7129 - val_loss: 25.2145\n",
            "Epoch 396/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.7049 - val_loss: 24.9792\n",
            "Epoch 397/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.6843 - val_loss: 24.5151\n",
            "Epoch 398/1000\n",
            "16/16 [==============================] - 0s 918us/step - loss: 4.6745 - val_loss: 24.1728\n",
            "Epoch 399/1000\n",
            "16/16 [==============================] - 0s 869us/step - loss: 4.6668 - val_loss: 23.9845\n",
            "Epoch 400/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.6574 - val_loss: 23.7688\n",
            "Epoch 401/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.6465 - val_loss: 23.5778\n",
            "Epoch 402/1000\n",
            "16/16 [==============================] - 0s 867us/step - loss: 4.6357 - val_loss: 23.5569\n",
            "Epoch 403/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.6279 - val_loss: 23.2862\n",
            "Epoch 404/1000\n",
            "16/16 [==============================] - 0s 920us/step - loss: 4.6234 - val_loss: 23.0742\n",
            "Epoch 405/1000\n",
            "16/16 [==============================] - 0s 955us/step - loss: 4.6075 - val_loss: 23.0732\n",
            "Epoch 406/1000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 4.5976 - val_loss: 22.9524\n",
            "Epoch 407/1000\n",
            "16/16 [==============================] - 0s 864us/step - loss: 4.5885 - val_loss: 22.9782\n",
            "Epoch 408/1000\n",
            "16/16 [==============================] - 0s 875us/step - loss: 4.5779 - val_loss: 23.1833\n",
            "Epoch 409/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.5658 - val_loss: 23.4788\n",
            "Epoch 410/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.5619 - val_loss: 23.8578\n",
            "Epoch 411/1000\n",
            "16/16 [==============================] - 0s 925us/step - loss: 4.5453 - val_loss: 24.0820\n",
            "Epoch 412/1000\n",
            "16/16 [==============================] - 0s 996us/step - loss: 4.5377 - val_loss: 24.3970\n",
            "Epoch 413/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.5284 - val_loss: 24.5767\n",
            "Epoch 414/1000\n",
            "16/16 [==============================] - 0s 1000us/step - loss: 4.5210 - val_loss: 24.8215\n",
            "Epoch 415/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.5176 - val_loss: 24.9133\n",
            "Epoch 416/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.5065 - val_loss: 24.8295\n",
            "Epoch 417/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4982 - val_loss: 24.5973\n",
            "Epoch 418/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4850 - val_loss: 24.4520\n",
            "Epoch 419/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4797 - val_loss: 24.3729\n",
            "Epoch 420/1000\n",
            "16/16 [==============================] - 0s 996us/step - loss: 4.4637 - val_loss: 24.4403\n",
            "Epoch 421/1000\n",
            "16/16 [==============================] - 0s 986us/step - loss: 4.4617 - val_loss: 24.7523\n",
            "Epoch 422/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4462 - val_loss: 24.8136\n",
            "Epoch 423/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4383 - val_loss: 24.7867\n",
            "Epoch 424/1000\n",
            "16/16 [==============================] - 0s 869us/step - loss: 4.4268 - val_loss: 24.9107\n",
            "Epoch 425/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4260 - val_loss: 25.0292\n",
            "Epoch 426/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4116 - val_loss: 24.8413\n",
            "Epoch 427/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.4023 - val_loss: 24.7296\n",
            "Epoch 428/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3927 - val_loss: 24.7571\n",
            "Epoch 429/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3843 - val_loss: 24.7419\n",
            "Epoch 430/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3751 - val_loss: 24.7692\n",
            "Epoch 431/1000\n",
            "16/16 [==============================] - 0s 988us/step - loss: 4.3669 - val_loss: 24.9464\n",
            "Epoch 432/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3599 - val_loss: 24.9510\n",
            "Epoch 433/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3510 - val_loss: 24.8623\n",
            "Epoch 434/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3433 - val_loss: 24.5161\n",
            "Epoch 435/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3279 - val_loss: 24.2674\n",
            "Epoch 436/1000\n",
            "16/16 [==============================] - 0s 942us/step - loss: 4.3204 - val_loss: 24.0826\n",
            "Epoch 437/1000\n",
            "16/16 [==============================] - 0s 741us/step - loss: 4.3099 - val_loss: 24.0595\n",
            "Epoch 438/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.3021 - val_loss: 23.9370\n",
            "Epoch 439/1000\n",
            "16/16 [==============================] - 0s 902us/step - loss: 4.2880 - val_loss: 23.8574\n",
            "Epoch 440/1000\n",
            "16/16 [==============================] - 0s 986us/step - loss: 4.2786 - val_loss: 23.7822\n",
            "Epoch 441/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 4.2684 - val_loss: 23.9389\n",
            "Epoch 442/1000\n",
            "16/16 [==============================] - 0s 957us/step - loss: 4.2604 - val_loss: 24.1417\n",
            "Epoch 443/1000\n",
            "16/16 [==============================] - 0s 953us/step - loss: 4.2489 - val_loss: 24.1174\n",
            "Epoch 444/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.2380 - val_loss: 24.2059\n",
            "Epoch 445/1000\n",
            "16/16 [==============================] - 0s 957us/step - loss: 4.2290 - val_loss: 24.2082\n",
            "Epoch 446/1000\n",
            "16/16 [==============================] - 0s 922us/step - loss: 4.2268 - val_loss: 24.2889\n",
            "Epoch 447/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.2128 - val_loss: 23.9508\n",
            "Epoch 448/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.2006 - val_loss: 23.7869\n",
            "Epoch 449/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1873 - val_loss: 23.4897\n",
            "Epoch 450/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1803 - val_loss: 23.3178\n",
            "Epoch 451/1000\n",
            "16/16 [==============================] - 0s 973us/step - loss: 4.1685 - val_loss: 23.2547\n",
            "Epoch 452/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1585 - val_loss: 23.3212\n",
            "Epoch 453/1000\n",
            "16/16 [==============================] - 0s 882us/step - loss: 4.1495 - val_loss: 23.1178\n",
            "Epoch 454/1000\n",
            "16/16 [==============================] - 0s 889us/step - loss: 4.1370 - val_loss: 23.0343\n",
            "Epoch 455/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1307 - val_loss: 23.1081\n",
            "Epoch 456/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1209 - val_loss: 23.1755\n",
            "Epoch 457/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1106 - val_loss: 22.9394\n",
            "Epoch 458/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.1030 - val_loss: 22.7298\n",
            "Epoch 459/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.0920 - val_loss: 22.6549\n",
            "Epoch 460/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.0845 - val_loss: 22.7199\n",
            "Epoch 461/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4.0742 - val_loss: 22.6316\n",
            "Epoch 462/1000\n",
            "16/16 [==============================] - 0s 919us/step - loss: 4.0607 - val_loss: 22.4096\n",
            "Epoch 463/1000\n",
            "16/16 [==============================] - 0s 940us/step - loss: 4.0537 - val_loss: 22.3243\n",
            "Epoch 464/1000\n",
            "16/16 [==============================] - 0s 991us/step - loss: 4.0433 - val_loss: 21.8858\n",
            "Epoch 465/1000\n",
            "16/16 [==============================] - 0s 874us/step - loss: 4.0339 - val_loss: 21.4928\n",
            "Epoch 466/1000\n",
            "16/16 [==============================] - 0s 903us/step - loss: 4.0200 - val_loss: 21.0465\n",
            "Epoch 467/1000\n",
            "16/16 [==============================] - 0s 991us/step - loss: 4.0218 - val_loss: 20.8298\n",
            "Epoch 468/1000\n",
            "16/16 [==============================] - 0s 908us/step - loss: 4.0158 - val_loss: 20.7717\n",
            "Epoch 469/1000\n",
            "16/16 [==============================] - 0s 929us/step - loss: 3.9979 - val_loss: 20.9625\n",
            "Epoch 470/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.9766 - val_loss: 21.3414\n",
            "Epoch 471/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.9730 - val_loss: 21.8712\n",
            "Epoch 472/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.9422 - val_loss: 22.1803\n",
            "Epoch 473/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.9357 - val_loss: 22.3079\n",
            "Epoch 474/1000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 3.9207 - val_loss: 22.0817\n",
            "Epoch 475/1000\n",
            "16/16 [==============================] - 0s 887us/step - loss: 3.9071 - val_loss: 21.9774\n",
            "Epoch 476/1000\n",
            "16/16 [==============================] - 0s 990us/step - loss: 3.8956 - val_loss: 21.9046\n",
            "Epoch 477/1000\n",
            "16/16 [==============================] - 0s 956us/step - loss: 3.8910 - val_loss: 22.1063\n",
            "Epoch 478/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.8679 - val_loss: 22.0595\n",
            "Epoch 479/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.8528 - val_loss: 22.1011\n",
            "Epoch 480/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.8350 - val_loss: 22.2607\n",
            "Epoch 481/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.8146 - val_loss: 22.5856\n",
            "Epoch 482/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.8050 - val_loss: 22.9181\n",
            "Epoch 483/1000\n",
            "16/16 [==============================] - 0s 945us/step - loss: 3.7896 - val_loss: 22.8758\n",
            "Epoch 484/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.7718 - val_loss: 23.0005\n",
            "Epoch 485/1000\n",
            "16/16 [==============================] - 0s 883us/step - loss: 3.7596 - val_loss: 23.0546\n",
            "Epoch 486/1000\n",
            "16/16 [==============================] - 0s 845us/step - loss: 3.7444 - val_loss: 22.8326\n",
            "Epoch 487/1000\n",
            "16/16 [==============================] - 0s 893us/step - loss: 3.7297 - val_loss: 22.5359\n",
            "Epoch 488/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.7105 - val_loss: 22.1758\n",
            "Epoch 489/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.7010 - val_loss: 21.9433\n",
            "Epoch 490/1000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 3.6919 - val_loss: 21.7217\n",
            "Epoch 491/1000\n",
            "16/16 [==============================] - 0s 954us/step - loss: 3.6834 - val_loss: 21.5175\n",
            "Epoch 492/1000\n",
            "16/16 [==============================] - 0s 924us/step - loss: 3.6699 - val_loss: 21.4615\n",
            "Epoch 493/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 3.6590 - val_loss: 21.6131\n",
            "Epoch 494/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.6446 - val_loss: 21.6179\n",
            "Epoch 495/1000\n",
            "16/16 [==============================] - 0s 968us/step - loss: 3.6304 - val_loss: 21.5311\n",
            "Epoch 496/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 3.6124 - val_loss: 21.1648\n",
            "Epoch 497/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.6157 - val_loss: 20.8672\n",
            "Epoch 498/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.5959 - val_loss: 20.9284\n",
            "Epoch 499/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.5776 - val_loss: 21.0938\n",
            "Epoch 500/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.5657 - val_loss: 21.3586\n",
            "Epoch 501/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.5540 - val_loss: 21.5440\n",
            "Epoch 502/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.5368 - val_loss: 21.5789\n",
            "Epoch 503/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 3.5256 - val_loss: 21.5114\n",
            "Epoch 504/1000\n",
            "16/16 [==============================] - 0s 958us/step - loss: 3.5142 - val_loss: 21.6812\n",
            "Epoch 505/1000\n",
            "16/16 [==============================] - 0s 966us/step - loss: 3.5026 - val_loss: 21.8235\n",
            "Epoch 506/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.4937 - val_loss: 21.5686\n",
            "Epoch 507/1000\n",
            "16/16 [==============================] - 0s 999us/step - loss: 3.4738 - val_loss: 21.5187\n",
            "Epoch 508/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.4609 - val_loss: 21.5881\n",
            "Epoch 509/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.4545 - val_loss: 21.6707\n",
            "Epoch 510/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.4394 - val_loss: 21.6413\n",
            "Epoch 511/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.4293 - val_loss: 21.4370\n",
            "Epoch 512/1000\n",
            "16/16 [==============================] - 0s 948us/step - loss: 3.4202 - val_loss: 21.2633\n",
            "Epoch 513/1000\n",
            "16/16 [==============================] - 0s 967us/step - loss: 3.4093 - val_loss: 21.2567\n",
            "Epoch 514/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.3961 - val_loss: 21.2343\n",
            "Epoch 515/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.3892 - val_loss: 21.1999\n",
            "Epoch 516/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.3762 - val_loss: 21.1876\n",
            "Epoch 517/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 3.3710 - val_loss: 20.8944\n",
            "Epoch 518/1000\n",
            "16/16 [==============================] - 0s 874us/step - loss: 3.3572 - val_loss: 20.5893\n",
            "Epoch 519/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.3485 - val_loss: 20.3549\n",
            "Epoch 520/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.3320 - val_loss: 20.3208\n",
            "Epoch 521/1000\n",
            "16/16 [==============================] - 0s 945us/step - loss: 3.3224 - val_loss: 20.4232\n",
            "Epoch 522/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.3093 - val_loss: 20.2929\n",
            "Epoch 523/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2938 - val_loss: 20.3675\n",
            "Epoch 524/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2822 - val_loss: 20.3417\n",
            "Epoch 525/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2703 - val_loss: 20.0410\n",
            "Epoch 526/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2607 - val_loss: 19.7248\n",
            "Epoch 527/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2489 - val_loss: 19.6955\n",
            "Epoch 528/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2391 - val_loss: 19.6663\n",
            "Epoch 529/1000\n",
            "16/16 [==============================] - 0s 998us/step - loss: 3.2256 - val_loss: 19.5418\n",
            "Epoch 530/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2189 - val_loss: 19.3800\n",
            "Epoch 531/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2064 - val_loss: 19.3957\n",
            "Epoch 532/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.2001 - val_loss: 19.5344\n",
            "Epoch 533/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.1830 - val_loss: 19.3649\n",
            "Epoch 534/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.1709 - val_loss: 19.2696\n",
            "Epoch 535/1000\n",
            "16/16 [==============================] - 0s 957us/step - loss: 3.1665 - val_loss: 19.1857\n",
            "Epoch 536/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.1453 - val_loss: 19.3710\n",
            "Epoch 537/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.1319 - val_loss: 19.6555\n",
            "Epoch 538/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.1153 - val_loss: 19.9202\n",
            "Epoch 539/1000\n",
            "16/16 [==============================] - 0s 1000us/step - loss: 3.0993 - val_loss: 20.2524\n",
            "Epoch 540/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.0872 - val_loss: 20.6757\n",
            "Epoch 541/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.0658 - val_loss: 20.9364\n",
            "Epoch 542/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.0567 - val_loss: 21.0563\n",
            "Epoch 543/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.0388 - val_loss: 21.0191\n",
            "Epoch 544/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.0194 - val_loss: 20.9089\n",
            "Epoch 545/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 3.0089 - val_loss: 20.9463\n",
            "Epoch 546/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.9911 - val_loss: 20.7810\n",
            "Epoch 547/1000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 2.9757 - val_loss: 20.6692\n",
            "Epoch 548/1000\n",
            "16/16 [==============================] - 0s 961us/step - loss: 2.9614 - val_loss: 20.5348\n",
            "Epoch 549/1000\n",
            "16/16 [==============================] - 0s 933us/step - loss: 2.9472 - val_loss: 20.4294\n",
            "Epoch 550/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.9315 - val_loss: 20.3930\n",
            "Epoch 551/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.9183 - val_loss: 20.4272\n",
            "Epoch 552/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.8962 - val_loss: 20.0201\n",
            "Epoch 553/1000\n",
            "16/16 [==============================] - 0s 942us/step - loss: 2.8819 - val_loss: 19.4582\n",
            "Epoch 554/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.8675 - val_loss: 19.2131\n",
            "Epoch 555/1000\n",
            "16/16 [==============================] - 0s 929us/step - loss: 2.8539 - val_loss: 18.9955\n",
            "Epoch 556/1000\n",
            "16/16 [==============================] - 0s 959us/step - loss: 2.8379 - val_loss: 18.8745\n",
            "Epoch 557/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.8239 - val_loss: 18.7738\n",
            "Epoch 558/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.8084 - val_loss: 18.6365\n",
            "Epoch 559/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.7984 - val_loss: 18.4500\n",
            "Epoch 560/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.7821 - val_loss: 18.4725\n",
            "Epoch 561/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.7665 - val_loss: 18.5862\n",
            "Epoch 562/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.7559 - val_loss: 18.7411\n",
            "Epoch 563/1000\n",
            "16/16 [==============================] - 0s 949us/step - loss: 2.7492 - val_loss: 18.9780\n",
            "Epoch 564/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.7194 - val_loss: 18.8172\n",
            "Epoch 565/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.7034 - val_loss: 18.7196\n",
            "Epoch 566/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.6972 - val_loss: 18.4217\n",
            "Epoch 567/1000\n",
            "16/16 [==============================] - 0s 925us/step - loss: 2.6781 - val_loss: 18.3099\n",
            "Epoch 568/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.6653 - val_loss: 18.2517\n",
            "Epoch 569/1000\n",
            "16/16 [==============================] - 0s 926us/step - loss: 2.6521 - val_loss: 18.1969\n",
            "Epoch 570/1000\n",
            "16/16 [==============================] - 0s 995us/step - loss: 2.6381 - val_loss: 17.9786\n",
            "Epoch 571/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.6253 - val_loss: 18.0645\n",
            "Epoch 572/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.6042 - val_loss: 18.0382\n",
            "Epoch 573/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.5827 - val_loss: 18.0713\n",
            "Epoch 574/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.5721 - val_loss: 17.7871\n",
            "Epoch 575/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.5485 - val_loss: 17.7262\n",
            "Epoch 576/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.5354 - val_loss: 17.7731\n",
            "Epoch 577/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.5209 - val_loss: 17.4479\n",
            "Epoch 578/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.5024 - val_loss: 17.3025\n",
            "Epoch 579/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.4886 - val_loss: 17.2737\n",
            "Epoch 580/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.4724 - val_loss: 17.4005\n",
            "Epoch 581/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.4589 - val_loss: 17.6324\n",
            "Epoch 582/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.4351 - val_loss: 17.6024\n",
            "Epoch 583/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.4217 - val_loss: 17.7137\n",
            "Epoch 584/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 2.4040 - val_loss: 17.7762\n",
            "Epoch 585/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.3878 - val_loss: 17.7451\n",
            "Epoch 586/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.3769 - val_loss: 17.6147\n",
            "Epoch 587/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.3562 - val_loss: 17.6581\n",
            "Epoch 588/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.3377 - val_loss: 17.7933\n",
            "Epoch 589/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.3182 - val_loss: 18.0161\n",
            "Epoch 590/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.3117 - val_loss: 18.3978\n",
            "Epoch 591/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.2948 - val_loss: 18.7368\n",
            "Epoch 592/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.2680 - val_loss: 18.9589\n",
            "Epoch 593/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.2575 - val_loss: 19.1039\n",
            "Epoch 594/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.2495 - val_loss: 19.2519\n",
            "Epoch 595/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 2.2356 - val_loss: 19.2970\n",
            "Epoch 596/1000\n",
            "16/16 [==============================] - 0s 956us/step - loss: 2.2204 - val_loss: 19.2018\n",
            "Epoch 597/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.2076 - val_loss: 19.0055\n",
            "Epoch 598/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.1897 - val_loss: 18.7944\n",
            "Epoch 599/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.1773 - val_loss: 18.5372\n",
            "Epoch 600/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.1598 - val_loss: 18.3612\n",
            "Epoch 601/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.1403 - val_loss: 18.1091\n",
            "Epoch 602/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.1254 - val_loss: 17.8613\n",
            "Epoch 603/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.1104 - val_loss: 17.6533\n",
            "Epoch 604/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.0978 - val_loss: 17.5522\n",
            "Epoch 605/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.0814 - val_loss: 17.4632\n",
            "Epoch 606/1000\n",
            "16/16 [==============================] - 0s 805us/step - loss: 2.0713 - val_loss: 17.4774\n",
            "Epoch 607/1000\n",
            "16/16 [==============================] - 0s 950us/step - loss: 2.0447 - val_loss: 17.2432\n",
            "Epoch 608/1000\n",
            "16/16 [==============================] - 0s 928us/step - loss: 2.0317 - val_loss: 16.9121\n",
            "Epoch 609/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 2.0137 - val_loss: 16.6137\n",
            "Epoch 610/1000\n",
            "16/16 [==============================] - 0s 853us/step - loss: 1.9990 - val_loss: 16.4204\n",
            "Epoch 611/1000\n",
            "16/16 [==============================] - 0s 983us/step - loss: 1.9835 - val_loss: 16.4094\n",
            "Epoch 612/1000\n",
            "16/16 [==============================] - 0s 950us/step - loss: 1.9652 - val_loss: 16.3010\n",
            "Epoch 613/1000\n",
            "16/16 [==============================] - 0s 863us/step - loss: 1.9546 - val_loss: 16.0369\n",
            "Epoch 614/1000\n",
            "16/16 [==============================] - 0s 946us/step - loss: 1.9348 - val_loss: 15.9919\n",
            "Epoch 615/1000\n",
            "16/16 [==============================] - 0s 817us/step - loss: 1.9178 - val_loss: 15.9618\n",
            "Epoch 616/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.9044 - val_loss: 15.8824\n",
            "Epoch 617/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.8879 - val_loss: 15.7389\n",
            "Epoch 618/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.8717 - val_loss: 15.7614\n",
            "Epoch 619/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.8571 - val_loss: 15.8949\n",
            "Epoch 620/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.8389 - val_loss: 15.9964\n",
            "Epoch 621/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.8184 - val_loss: 16.1867\n",
            "Epoch 622/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.7969 - val_loss: 16.3736\n",
            "Epoch 623/1000\n",
            "16/16 [==============================] - 0s 988us/step - loss: 1.7835 - val_loss: 16.5380\n",
            "Epoch 624/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.7647 - val_loss: 16.5580\n",
            "Epoch 625/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.7504 - val_loss: 16.5396\n",
            "Epoch 626/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.7364 - val_loss: 16.4213\n",
            "Epoch 627/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.7203 - val_loss: 16.3440\n",
            "Epoch 628/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 1.7032 - val_loss: 16.1624\n",
            "Epoch 629/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.6890 - val_loss: 15.9063\n",
            "Epoch 630/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.6746 - val_loss: 15.6956\n",
            "Epoch 631/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.6598 - val_loss: 15.5129\n",
            "Epoch 632/1000\n",
            "16/16 [==============================] - 0s 990us/step - loss: 1.6447 - val_loss: 15.4200\n",
            "Epoch 633/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.6300 - val_loss: 15.2355\n",
            "Epoch 634/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.6173 - val_loss: 15.0567\n",
            "Epoch 635/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.6030 - val_loss: 14.8725\n",
            "Epoch 636/1000\n",
            "16/16 [==============================] - 0s 953us/step - loss: 1.5938 - val_loss: 14.7002\n",
            "Epoch 637/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.5767 - val_loss: 14.7332\n",
            "Epoch 638/1000\n",
            "16/16 [==============================] - 0s 939us/step - loss: 1.5597 - val_loss: 14.7745\n",
            "Epoch 639/1000\n",
            "16/16 [==============================] - 0s 941us/step - loss: 1.5466 - val_loss: 14.8871\n",
            "Epoch 640/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.5337 - val_loss: 14.9763\n",
            "Epoch 641/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.5193 - val_loss: 15.0320\n",
            "Epoch 642/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.5056 - val_loss: 15.0621\n",
            "Epoch 643/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4934 - val_loss: 15.0383\n",
            "Epoch 644/1000\n",
            "16/16 [==============================] - 0s 960us/step - loss: 1.4792 - val_loss: 15.0765\n",
            "Epoch 645/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4696 - val_loss: 15.1205\n",
            "Epoch 646/1000\n",
            "16/16 [==============================] - 0s 929us/step - loss: 1.4558 - val_loss: 15.0159\n",
            "Epoch 647/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4390 - val_loss: 15.0787\n",
            "Epoch 648/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4289 - val_loss: 15.1893\n",
            "Epoch 649/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4133 - val_loss: 15.1245\n",
            "Epoch 650/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.4007 - val_loss: 15.0012\n",
            "Epoch 651/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.3880 - val_loss: 14.9661\n",
            "Epoch 652/1000\n",
            "16/16 [==============================] - 0s 952us/step - loss: 1.3733 - val_loss: 14.8836\n",
            "Epoch 653/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3620 - val_loss: 14.9410\n",
            "Epoch 654/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.3461 - val_loss: 14.9174\n",
            "Epoch 655/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.3340 - val_loss: 14.7914\n",
            "Epoch 656/1000\n",
            "16/16 [==============================] - 0s 959us/step - loss: 1.3187 - val_loss: 14.5783\n",
            "Epoch 657/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.3034 - val_loss: 14.3835\n",
            "Epoch 658/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2912 - val_loss: 14.2463\n",
            "Epoch 659/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2745 - val_loss: 14.1973\n",
            "Epoch 660/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2614 - val_loss: 14.1644\n",
            "Epoch 661/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2480 - val_loss: 14.0766\n",
            "Epoch 662/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2350 - val_loss: 14.0241\n",
            "Epoch 663/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2236 - val_loss: 14.0073\n",
            "Epoch 664/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.2088 - val_loss: 13.9345\n",
            "Epoch 665/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1956 - val_loss: 13.8274\n",
            "Epoch 666/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1840 - val_loss: 13.5476\n",
            "Epoch 667/1000\n",
            "16/16 [==============================] - 0s 993us/step - loss: 1.1678 - val_loss: 13.3908\n",
            "Epoch 668/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1545 - val_loss: 13.2674\n",
            "Epoch 669/1000\n",
            "16/16 [==============================] - 0s 947us/step - loss: 1.1415 - val_loss: 13.1765\n",
            "Epoch 670/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1290 - val_loss: 13.1169\n",
            "Epoch 671/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1188 - val_loss: 13.0937\n",
            "Epoch 672/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1058 - val_loss: 12.9205\n",
            "Epoch 673/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.0914 - val_loss: 12.5146\n",
            "Epoch 674/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.0791 - val_loss: 12.2024\n",
            "Epoch 675/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.0664 - val_loss: 11.9604\n",
            "Epoch 676/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.0560 - val_loss: 11.5874\n",
            "Epoch 677/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.0424 - val_loss: 11.4138\n",
            "Epoch 678/1000\n",
            "16/16 [==============================] - 0s 998us/step - loss: 1.0314 - val_loss: 11.2757\n",
            "Epoch 679/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.0198 - val_loss: 11.2392\n",
            "Epoch 680/1000\n",
            "16/16 [==============================] - 0s 962us/step - loss: 1.0087 - val_loss: 11.2238\n",
            "Epoch 681/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9979 - val_loss: 11.2725\n",
            "Epoch 682/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9895 - val_loss: 11.3820\n",
            "Epoch 683/1000\n",
            "16/16 [==============================] - 0s 973us/step - loss: 0.9783 - val_loss: 11.4119\n",
            "Epoch 684/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9700 - val_loss: 11.1945\n",
            "Epoch 685/1000\n",
            "16/16 [==============================] - 0s 959us/step - loss: 0.9586 - val_loss: 11.0818\n",
            "Epoch 686/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9496 - val_loss: 11.0684\n",
            "Epoch 687/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 10.9651\n",
            "Epoch 688/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9306 - val_loss: 10.8359\n",
            "Epoch 689/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.9219 - val_loss: 10.6129\n",
            "Epoch 690/1000\n",
            "16/16 [==============================] - 0s 864us/step - loss: 0.9127 - val_loss: 10.4758\n",
            "Epoch 691/1000\n",
            "16/16 [==============================] - 0s 924us/step - loss: 0.9046 - val_loss: 10.4210\n",
            "Epoch 692/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.8957 - val_loss: 10.3906\n",
            "Epoch 693/1000\n",
            "16/16 [==============================] - 0s 969us/step - loss: 0.8868 - val_loss: 10.3984\n",
            "Epoch 694/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.8784 - val_loss: 10.4264\n",
            "Epoch 695/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8694 - val_loss: 10.3807\n",
            "Epoch 696/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.8616 - val_loss: 10.2994\n",
            "Epoch 697/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.8515 - val_loss: 10.2956\n",
            "Epoch 698/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8437 - val_loss: 10.3188\n",
            "Epoch 699/1000\n",
            "16/16 [==============================] - 0s 950us/step - loss: 0.8365 - val_loss: 10.3103\n",
            "Epoch 700/1000\n",
            "16/16 [==============================] - 0s 953us/step - loss: 0.8292 - val_loss: 10.1752\n",
            "Epoch 701/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.8215 - val_loss: 9.8498\n",
            "Epoch 702/1000\n",
            "16/16 [==============================] - 0s 999us/step - loss: 0.8068 - val_loss: 9.6106\n",
            "Epoch 703/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7976 - val_loss: 9.2534\n",
            "Epoch 704/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7907 - val_loss: 8.9718\n",
            "Epoch 705/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7828 - val_loss: 8.7600\n",
            "Epoch 706/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7783 - val_loss: 8.5657\n",
            "Epoch 707/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7717 - val_loss: 8.4775\n",
            "Epoch 708/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7646 - val_loss: 8.4589\n",
            "Epoch 709/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7565 - val_loss: 8.4158\n",
            "Epoch 710/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7489 - val_loss: 8.3372\n",
            "Epoch 711/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7440 - val_loss: 8.2051\n",
            "Epoch 712/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7350 - val_loss: 8.1800\n",
            "Epoch 713/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7272 - val_loss: 8.2274\n",
            "Epoch 714/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7197 - val_loss: 8.2733\n",
            "Epoch 715/1000\n",
            "16/16 [==============================] - 0s 881us/step - loss: 0.7128 - val_loss: 8.3688\n",
            "Epoch 716/1000\n",
            "16/16 [==============================] - 0s 853us/step - loss: 0.7059 - val_loss: 8.4629\n",
            "Epoch 717/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6997 - val_loss: 8.5360\n",
            "Epoch 718/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6943 - val_loss: 8.5900\n",
            "Epoch 719/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6898 - val_loss: 8.6385\n",
            "Epoch 720/1000\n",
            "16/16 [==============================] - 0s 981us/step - loss: 0.6856 - val_loss: 8.6634\n",
            "Epoch 721/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6819 - val_loss: 8.6651\n",
            "Epoch 722/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6777 - val_loss: 8.6660\n",
            "Epoch 723/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6743 - val_loss: 8.6726\n",
            "Epoch 724/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6712 - val_loss: 8.6424\n",
            "Epoch 725/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6664 - val_loss: 8.4852\n",
            "Epoch 726/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6606 - val_loss: 8.2873\n",
            "Epoch 727/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6543 - val_loss: 8.1526\n",
            "Epoch 728/1000\n",
            "16/16 [==============================] - 0s 946us/step - loss: 0.6484 - val_loss: 8.0233\n",
            "Epoch 729/1000\n",
            "16/16 [==============================] - 0s 905us/step - loss: 0.6416 - val_loss: 7.7868\n",
            "Epoch 730/1000\n",
            "16/16 [==============================] - 0s 873us/step - loss: 0.6360 - val_loss: 7.5479\n",
            "Epoch 731/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6324 - val_loss: 7.2769\n",
            "Epoch 732/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6288 - val_loss: 7.1490\n",
            "Epoch 733/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6236 - val_loss: 7.0993\n",
            "Epoch 734/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6170 - val_loss: 7.1203\n",
            "Epoch 735/1000\n",
            "16/16 [==============================] - 0s 962us/step - loss: 0.6120 - val_loss: 7.1569\n",
            "Epoch 736/1000\n",
            "16/16 [==============================] - 0s 880us/step - loss: 0.6065 - val_loss: 7.1688\n",
            "Epoch 737/1000\n",
            "16/16 [==============================] - 0s 987us/step - loss: 0.6018 - val_loss: 7.1383\n",
            "Epoch 738/1000\n",
            "16/16 [==============================] - 0s 855us/step - loss: 0.5972 - val_loss: 7.1508\n",
            "Epoch 739/1000\n",
            "16/16 [==============================] - 0s 778us/step - loss: 0.5929 - val_loss: 7.1166\n",
            "Epoch 740/1000\n",
            "16/16 [==============================] - 0s 832us/step - loss: 0.5895 - val_loss: 7.0768\n",
            "Epoch 741/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5844 - val_loss: 7.0957\n",
            "Epoch 742/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5802 - val_loss: 7.0749\n",
            "Epoch 743/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5759 - val_loss: 6.9385\n",
            "Epoch 744/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5724 - val_loss: 6.8341\n",
            "Epoch 745/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5679 - val_loss: 6.7931\n",
            "Epoch 746/1000\n",
            "16/16 [==============================] - 0s 876us/step - loss: 0.5636 - val_loss: 6.7797\n",
            "Epoch 747/1000\n",
            "16/16 [==============================] - 0s 868us/step - loss: 0.5598 - val_loss: 6.7727\n",
            "Epoch 748/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 0.5563 - val_loss: 6.7621\n",
            "Epoch 749/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5537 - val_loss: 6.8004\n",
            "Epoch 750/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 6.7743\n",
            "Epoch 751/1000\n",
            "16/16 [==============================] - 0s 923us/step - loss: 0.5459 - val_loss: 6.6935\n",
            "Epoch 752/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 6.5988\n",
            "Epoch 753/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5378 - val_loss: 6.5584\n",
            "Epoch 754/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5341 - val_loss: 6.5484\n",
            "Epoch 755/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 6.4920\n",
            "Epoch 756/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5271 - val_loss: 6.4285\n",
            "Epoch 757/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 6.3667\n",
            "Epoch 758/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 6.2952\n",
            "Epoch 759/1000\n",
            "16/16 [==============================] - 0s 942us/step - loss: 0.5168 - val_loss: 6.1975\n",
            "Epoch 760/1000\n",
            "16/16 [==============================] - 0s 945us/step - loss: 0.5148 - val_loss: 6.1032\n",
            "Epoch 761/1000\n",
            "16/16 [==============================] - 0s 925us/step - loss: 0.5107 - val_loss: 6.0740\n",
            "Epoch 762/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5075 - val_loss: 6.0807\n",
            "Epoch 763/1000\n",
            "16/16 [==============================] - 0s 956us/step - loss: 0.5043 - val_loss: 6.0916\n",
            "Epoch 764/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5017 - val_loss: 6.0966\n",
            "Epoch 765/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4993 - val_loss: 6.0732\n",
            "Epoch 766/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4960 - val_loss: 6.0527\n",
            "Epoch 767/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4935 - val_loss: 6.0190\n",
            "Epoch 768/1000\n",
            "16/16 [==============================] - 0s 927us/step - loss: 0.4908 - val_loss: 5.9828\n",
            "Epoch 769/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 5.9216\n",
            "Epoch 770/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4849 - val_loss: 5.8818\n",
            "Epoch 771/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4817 - val_loss: 5.8993\n",
            "Epoch 772/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4789 - val_loss: 5.9063\n",
            "Epoch 773/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4767 - val_loss: 5.8755\n",
            "Epoch 774/1000\n",
            "16/16 [==============================] - 0s 872us/step - loss: 0.4734 - val_loss: 5.8632\n",
            "Epoch 775/1000\n",
            "16/16 [==============================] - 0s 768us/step - loss: 0.4702 - val_loss: 5.8759\n",
            "Epoch 776/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4682 - val_loss: 5.8542\n",
            "Epoch 777/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4652 - val_loss: 5.8291\n",
            "Epoch 778/1000\n",
            "16/16 [==============================] - 0s 981us/step - loss: 0.4631 - val_loss: 5.7953\n",
            "Epoch 779/1000\n",
            "16/16 [==============================] - 0s 866us/step - loss: 0.4606 - val_loss: 5.7982\n",
            "Epoch 780/1000\n",
            "16/16 [==============================] - 0s 877us/step - loss: 0.4591 - val_loss: 5.8019\n",
            "Epoch 781/1000\n",
            "16/16 [==============================] - 0s 918us/step - loss: 0.4566 - val_loss: 5.7673\n",
            "Epoch 782/1000\n",
            "16/16 [==============================] - 0s 935us/step - loss: 0.4544 - val_loss: 5.7097\n",
            "Epoch 783/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 0.4521 - val_loss: 5.6554\n",
            "Epoch 784/1000\n",
            "16/16 [==============================] - 0s 899us/step - loss: 0.4496 - val_loss: 5.6199\n",
            "Epoch 785/1000\n",
            "16/16 [==============================] - 0s 945us/step - loss: 0.4476 - val_loss: 5.5584\n",
            "Epoch 786/1000\n",
            "16/16 [==============================] - 0s 828us/step - loss: 0.4449 - val_loss: 5.4854\n",
            "Epoch 787/1000\n",
            "16/16 [==============================] - 0s 855us/step - loss: 0.4418 - val_loss: 5.4395\n",
            "Epoch 788/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 5.4268\n",
            "Epoch 789/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 5.4343\n",
            "Epoch 790/1000\n",
            "16/16 [==============================] - 0s 848us/step - loss: 0.4355 - val_loss: 5.4170\n",
            "Epoch 791/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 5.4395\n",
            "Epoch 792/1000\n",
            "16/16 [==============================] - 0s 972us/step - loss: 0.4308 - val_loss: 5.4495\n",
            "Epoch 793/1000\n",
            "16/16 [==============================] - 0s 949us/step - loss: 0.4293 - val_loss: 5.4899\n",
            "Epoch 794/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 5.5089\n",
            "Epoch 795/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 5.4822\n",
            "Epoch 796/1000\n",
            "16/16 [==============================] - 0s 859us/step - loss: 0.4247 - val_loss: 5.4408\n",
            "Epoch 797/1000\n",
            "16/16 [==============================] - 0s 944us/step - loss: 0.4226 - val_loss: 5.3685\n",
            "Epoch 798/1000\n",
            "16/16 [==============================] - 0s 905us/step - loss: 0.4199 - val_loss: 5.2740\n",
            "Epoch 799/1000\n",
            "16/16 [==============================] - 0s 954us/step - loss: 0.4175 - val_loss: 5.1843\n",
            "Epoch 800/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 5.1152\n",
            "Epoch 801/1000\n",
            "16/16 [==============================] - 0s 884us/step - loss: 0.4125 - val_loss: 5.0550\n",
            "Epoch 802/1000\n",
            "16/16 [==============================] - 0s 911us/step - loss: 0.4109 - val_loss: 5.0048\n",
            "Epoch 803/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 4.9625\n",
            "Epoch 804/1000\n",
            "16/16 [==============================] - 0s 906us/step - loss: 0.4073 - val_loss: 4.9630\n",
            "Epoch 805/1000\n",
            "16/16 [==============================] - 0s 949us/step - loss: 0.4052 - val_loss: 4.9723\n",
            "Epoch 806/1000\n",
            "16/16 [==============================] - 0s 928us/step - loss: 0.4036 - val_loss: 5.0045\n",
            "Epoch 807/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 5.0193\n",
            "Epoch 808/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 5.0477\n",
            "Epoch 809/1000\n",
            "16/16 [==============================] - 0s 999us/step - loss: 0.3988 - val_loss: 5.0488\n",
            "Epoch 810/1000\n",
            "16/16 [==============================] - 0s 878us/step - loss: 0.3972 - val_loss: 5.0170\n",
            "Epoch 811/1000\n",
            "16/16 [==============================] - 0s 898us/step - loss: 0.3946 - val_loss: 4.9656\n",
            "Epoch 812/1000\n",
            "16/16 [==============================] - 0s 878us/step - loss: 0.3927 - val_loss: 4.8789\n",
            "Epoch 813/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 4.8244\n",
            "Epoch 814/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 4.7604\n",
            "Epoch 815/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 4.7232\n",
            "Epoch 816/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 4.6225\n",
            "Epoch 817/1000\n",
            "16/16 [==============================] - 0s 966us/step - loss: 0.3818 - val_loss: 4.5396\n",
            "Epoch 818/1000\n",
            "16/16 [==============================] - 0s 964us/step - loss: 0.3804 - val_loss: 4.4634\n",
            "Epoch 819/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3783 - val_loss: 4.4148\n",
            "Epoch 820/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 4.3873\n",
            "Epoch 821/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 4.3737\n",
            "Epoch 822/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 4.3525\n",
            "Epoch 823/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3710 - val_loss: 4.3217\n",
            "Epoch 824/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 4.2961\n",
            "Epoch 825/1000\n",
            "16/16 [==============================] - 0s 979us/step - loss: 0.3663 - val_loss: 4.3392\n",
            "Epoch 826/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 4.4198\n",
            "Epoch 827/1000\n",
            "16/16 [==============================] - 0s 881us/step - loss: 0.3615 - val_loss: 4.4497\n",
            "Epoch 828/1000\n",
            "16/16 [==============================] - 0s 997us/step - loss: 0.3596 - val_loss: 4.4556\n",
            "Epoch 829/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 4.4448\n",
            "Epoch 830/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 4.4201\n",
            "Epoch 831/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 4.3739\n",
            "Epoch 832/1000\n",
            "16/16 [==============================] - 0s 937us/step - loss: 0.3538 - val_loss: 4.3083\n",
            "Epoch 833/1000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.3506 - val_loss: 4.2670\n",
            "Epoch 834/1000\n",
            "16/16 [==============================] - 0s 927us/step - loss: 0.3486 - val_loss: 4.2405\n",
            "Epoch 835/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 4.2025\n",
            "Epoch 836/1000\n",
            "16/16 [==============================] - 0s 938us/step - loss: 0.3456 - val_loss: 4.1560\n",
            "Epoch 837/1000\n",
            "16/16 [==============================] - 0s 845us/step - loss: 0.3441 - val_loss: 4.1111\n",
            "Epoch 838/1000\n",
            "16/16 [==============================] - 0s 877us/step - loss: 0.3424 - val_loss: 4.0781\n",
            "Epoch 839/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 4.0690\n",
            "Epoch 840/1000\n",
            "16/16 [==============================] - 0s 917us/step - loss: 0.3388 - val_loss: 4.0536\n",
            "Epoch 841/1000\n",
            "16/16 [==============================] - 0s 805us/step - loss: 0.3368 - val_loss: 4.0585\n",
            "Epoch 842/1000\n",
            "16/16 [==============================] - 0s 923us/step - loss: 0.3343 - val_loss: 4.0729\n",
            "Epoch 843/1000\n",
            "16/16 [==============================] - 0s 967us/step - loss: 0.3327 - val_loss: 4.0834\n",
            "Epoch 844/1000\n",
            "16/16 [==============================] - 0s 837us/step - loss: 0.3308 - val_loss: 4.1127\n",
            "Epoch 845/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 4.1205\n",
            "Epoch 846/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3280 - val_loss: 4.0994\n",
            "Epoch 847/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 4.0937\n",
            "Epoch 848/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 4.0855\n",
            "Epoch 849/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 4.0734\n",
            "Epoch 850/1000\n",
            "16/16 [==============================] - 0s 995us/step - loss: 0.3227 - val_loss: 4.0740\n",
            "Epoch 851/1000\n",
            "16/16 [==============================] - 0s 910us/step - loss: 0.3217 - val_loss: 4.1194\n",
            "Epoch 852/1000\n",
            "16/16 [==============================] - 0s 946us/step - loss: 0.3207 - val_loss: 4.1391\n",
            "Epoch 853/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 4.1293\n",
            "Epoch 854/1000\n",
            "16/16 [==============================] - 0s 932us/step - loss: 0.3179 - val_loss: 4.0976\n",
            "Epoch 855/1000\n",
            "16/16 [==============================] - 0s 889us/step - loss: 0.3167 - val_loss: 4.0801\n",
            "Epoch 856/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3154 - val_loss: 4.0638\n",
            "Epoch 857/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 4.0384\n",
            "Epoch 858/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3131 - val_loss: 4.0148\n",
            "Epoch 859/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 3.9833\n",
            "Epoch 860/1000\n",
            "16/16 [==============================] - 0s 915us/step - loss: 0.3110 - val_loss: 3.9687\n",
            "Epoch 861/1000\n",
            "16/16 [==============================] - 0s 949us/step - loss: 0.3098 - val_loss: 3.9505\n",
            "Epoch 862/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3091 - val_loss: 3.9139\n",
            "Epoch 863/1000\n",
            "16/16 [==============================] - 0s 912us/step - loss: 0.3079 - val_loss: 3.9002\n",
            "Epoch 864/1000\n",
            "16/16 [==============================] - 0s 909us/step - loss: 0.3069 - val_loss: 3.8825\n",
            "Epoch 865/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3058 - val_loss: 3.8461\n",
            "Epoch 866/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 0.3048 - val_loss: 3.8146\n",
            "Epoch 867/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3042 - val_loss: 3.7959\n",
            "Epoch 868/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.3025 - val_loss: 3.8166\n",
            "Epoch 869/1000\n",
            "16/16 [==============================] - 0s 971us/step - loss: 0.3021 - val_loss: 3.8686\n",
            "Epoch 870/1000\n",
            "16/16 [==============================] - 0s 849us/step - loss: 0.2999 - val_loss: 3.8768\n",
            "Epoch 871/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2988 - val_loss: 3.8745\n",
            "Epoch 872/1000\n",
            "16/16 [==============================] - 0s 862us/step - loss: 0.2977 - val_loss: 3.8638\n",
            "Epoch 873/1000\n",
            "16/16 [==============================] - 0s 864us/step - loss: 0.2971 - val_loss: 3.8428\n",
            "Epoch 874/1000\n",
            "16/16 [==============================] - 0s 885us/step - loss: 0.2960 - val_loss: 3.8319\n",
            "Epoch 875/1000\n",
            "16/16 [==============================] - 0s 884us/step - loss: 0.2950 - val_loss: 3.8059\n",
            "Epoch 876/1000\n",
            "16/16 [==============================] - 0s 913us/step - loss: 0.2943 - val_loss: 3.8121\n",
            "Epoch 877/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2932 - val_loss: 3.8059\n",
            "Epoch 878/1000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.2922 - val_loss: 3.7848\n",
            "Epoch 879/1000\n",
            "16/16 [==============================] - 0s 803us/step - loss: 0.2912 - val_loss: 3.7579\n",
            "Epoch 880/1000\n",
            "16/16 [==============================] - 0s 843us/step - loss: 0.2903 - val_loss: 3.7325\n",
            "Epoch 881/1000\n",
            "16/16 [==============================] - 0s 822us/step - loss: 0.2890 - val_loss: 3.7216\n",
            "Epoch 882/1000\n",
            "16/16 [==============================] - 0s 879us/step - loss: 0.2881 - val_loss: 3.7109\n",
            "Epoch 883/1000\n",
            "16/16 [==============================] - 0s 908us/step - loss: 0.2876 - val_loss: 3.7075\n",
            "Epoch 884/1000\n",
            "16/16 [==============================] - 0s 971us/step - loss: 0.2865 - val_loss: 3.7071\n",
            "Epoch 885/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2856 - val_loss: 3.7043\n",
            "Epoch 886/1000\n",
            "16/16 [==============================] - 0s 904us/step - loss: 0.2852 - val_loss: 3.7360\n",
            "Epoch 887/1000\n",
            "16/16 [==============================] - 0s 838us/step - loss: 0.2837 - val_loss: 3.7286\n",
            "Epoch 888/1000\n",
            "16/16 [==============================] - 0s 867us/step - loss: 0.2819 - val_loss: 3.6988\n",
            "Epoch 889/1000\n",
            "16/16 [==============================] - 0s 868us/step - loss: 0.2809 - val_loss: 3.6468\n",
            "Epoch 890/1000\n",
            "16/16 [==============================] - 0s 949us/step - loss: 0.2801 - val_loss: 3.6011\n",
            "Epoch 891/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2795 - val_loss: 3.5409\n",
            "Epoch 892/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2783 - val_loss: 3.4982\n",
            "Epoch 893/1000\n",
            "16/16 [==============================] - 0s 814us/step - loss: 0.2779 - val_loss: 3.4574\n",
            "Epoch 894/1000\n",
            "16/16 [==============================] - 0s 876us/step - loss: 0.2772 - val_loss: 3.4286\n",
            "Epoch 895/1000\n",
            "16/16 [==============================] - 0s 904us/step - loss: 0.2767 - val_loss: 3.4179\n",
            "Epoch 896/1000\n",
            "16/16 [==============================] - 0s 891us/step - loss: 0.2757 - val_loss: 3.4614\n",
            "Epoch 897/1000\n",
            "16/16 [==============================] - 0s 861us/step - loss: 0.2737 - val_loss: 3.4867\n",
            "Epoch 898/1000\n",
            "16/16 [==============================] - 0s 929us/step - loss: 0.2727 - val_loss: 3.4929\n",
            "Epoch 899/1000\n",
            "16/16 [==============================] - 0s 906us/step - loss: 0.2715 - val_loss: 3.5072\n",
            "Epoch 900/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 0.2705 - val_loss: 3.5065\n",
            "Epoch 901/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 0.2696 - val_loss: 3.5143\n",
            "Epoch 902/1000\n",
            "16/16 [==============================] - 0s 981us/step - loss: 0.2689 - val_loss: 3.5222\n",
            "Epoch 903/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2681 - val_loss: 3.5182\n",
            "Epoch 904/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2672 - val_loss: 3.5177\n",
            "Epoch 905/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2670 - val_loss: 3.5594\n",
            "Epoch 906/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2659 - val_loss: 3.5547\n",
            "Epoch 907/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2650 - val_loss: 3.5267\n",
            "Epoch 908/1000\n",
            "16/16 [==============================] - 0s 972us/step - loss: 0.2646 - val_loss: 3.5284\n",
            "Epoch 909/1000\n",
            "16/16 [==============================] - 0s 962us/step - loss: 0.2631 - val_loss: 3.5006\n",
            "Epoch 910/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2623 - val_loss: 3.4525\n",
            "Epoch 911/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2610 - val_loss: 3.4104\n",
            "Epoch 912/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2601 - val_loss: 3.3874\n",
            "Epoch 913/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2593 - val_loss: 3.3806\n",
            "Epoch 914/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2584 - val_loss: 3.3646\n",
            "Epoch 915/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2576 - val_loss: 3.3659\n",
            "Epoch 916/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2570 - val_loss: 3.4012\n",
            "Epoch 917/1000\n",
            "16/16 [==============================] - 0s 904us/step - loss: 0.2554 - val_loss: 3.4159\n",
            "Epoch 918/1000\n",
            "16/16 [==============================] - 0s 818us/step - loss: 0.2547 - val_loss: 3.4173\n",
            "Epoch 919/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2532 - val_loss: 3.4276\n",
            "Epoch 920/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2525 - val_loss: 3.4419\n",
            "Epoch 921/1000\n",
            "16/16 [==============================] - 0s 979us/step - loss: 0.2515 - val_loss: 3.4548\n",
            "Epoch 922/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2506 - val_loss: 3.4789\n",
            "Epoch 923/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2506 - val_loss: 3.5042\n",
            "Epoch 924/1000\n",
            "16/16 [==============================] - 0s 893us/step - loss: 0.2497 - val_loss: 3.5022\n",
            "Epoch 925/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2490 - val_loss: 3.5419\n",
            "Epoch 926/1000\n",
            "16/16 [==============================] - 0s 989us/step - loss: 0.2478 - val_loss: 3.5774\n",
            "Epoch 927/1000\n",
            "16/16 [==============================] - 0s 830us/step - loss: 0.2473 - val_loss: 3.6203\n",
            "Epoch 928/1000\n",
            "16/16 [==============================] - 0s 911us/step - loss: 0.2474 - val_loss: 3.6169\n",
            "Epoch 929/1000\n",
            "16/16 [==============================] - 0s 838us/step - loss: 0.2466 - val_loss: 3.5793\n",
            "Epoch 930/1000\n",
            "16/16 [==============================] - 0s 963us/step - loss: 0.2450 - val_loss: 3.6011\n",
            "Epoch 931/1000\n",
            "16/16 [==============================] - 0s 879us/step - loss: 0.2445 - val_loss: 3.6362\n",
            "Epoch 932/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2439 - val_loss: 3.6358\n",
            "Epoch 933/1000\n",
            "16/16 [==============================] - 0s 966us/step - loss: 0.2433 - val_loss: 3.6230\n",
            "Epoch 934/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2420 - val_loss: 3.6093\n",
            "Epoch 935/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2411 - val_loss: 3.5750\n",
            "Epoch 936/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2402 - val_loss: 3.5031\n",
            "Epoch 937/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2392 - val_loss: 3.4436\n",
            "Epoch 938/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2377 - val_loss: 3.4261\n",
            "Epoch 939/1000\n",
            "16/16 [==============================] - 0s 941us/step - loss: 0.2367 - val_loss: 3.4464\n",
            "Epoch 940/1000\n",
            "16/16 [==============================] - 0s 964us/step - loss: 0.2356 - val_loss: 3.4995\n",
            "Epoch 941/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2355 - val_loss: 3.5577\n",
            "Epoch 942/1000\n",
            "16/16 [==============================] - 0s 938us/step - loss: 0.2346 - val_loss: 3.5526\n",
            "Epoch 943/1000\n",
            "16/16 [==============================] - 0s 993us/step - loss: 0.2338 - val_loss: 3.5182\n",
            "Epoch 944/1000\n",
            "16/16 [==============================] - 0s 991us/step - loss: 0.2332 - val_loss: 3.4763\n",
            "Epoch 945/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2321 - val_loss: 3.4419\n",
            "Epoch 946/1000\n",
            "16/16 [==============================] - 0s 835us/step - loss: 0.2314 - val_loss: 3.4299\n",
            "Epoch 947/1000\n",
            "16/16 [==============================] - 0s 931us/step - loss: 0.2304 - val_loss: 3.4673\n",
            "Epoch 948/1000\n",
            "16/16 [==============================] - 0s 922us/step - loss: 0.2309 - val_loss: 3.5395\n",
            "Epoch 949/1000\n",
            "16/16 [==============================] - 0s 972us/step - loss: 0.2304 - val_loss: 3.5554\n",
            "Epoch 950/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2297 - val_loss: 3.5332\n",
            "Epoch 951/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2289 - val_loss: 3.4809\n",
            "Epoch 952/1000\n",
            "16/16 [==============================] - 0s 983us/step - loss: 0.2273 - val_loss: 3.4454\n",
            "Epoch 953/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2269 - val_loss: 3.4139\n",
            "Epoch 954/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2264 - val_loss: 3.4032\n",
            "Epoch 955/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2252 - val_loss: 3.4427\n",
            "Epoch 956/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2247 - val_loss: 3.4526\n",
            "Epoch 957/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2243 - val_loss: 3.4381\n",
            "Epoch 958/1000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.2233 - val_loss: 3.4524\n",
            "Epoch 959/1000\n",
            "16/16 [==============================] - 0s 971us/step - loss: 0.2224 - val_loss: 3.4198\n",
            "Epoch 960/1000\n",
            "16/16 [==============================] - 0s 980us/step - loss: 0.2210 - val_loss: 3.3761\n",
            "Epoch 961/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2196 - val_loss: 3.3266\n",
            "Epoch 962/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2192 - val_loss: 3.2864\n",
            "Epoch 963/1000\n",
            "16/16 [==============================] - 0s 927us/step - loss: 0.2184 - val_loss: 3.2598\n",
            "Epoch 964/1000\n",
            "16/16 [==============================] - 0s 942us/step - loss: 0.2182 - val_loss: 3.2478\n",
            "Epoch 965/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2176 - val_loss: 3.2383\n",
            "Epoch 966/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2175 - val_loss: 3.2255\n",
            "Epoch 967/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2161 - val_loss: 3.2651\n",
            "Epoch 968/1000\n",
            "16/16 [==============================] - 0s 991us/step - loss: 0.2145 - val_loss: 3.3028\n",
            "Epoch 969/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2140 - val_loss: 3.3859\n",
            "Epoch 970/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2126 - val_loss: 3.4318\n",
            "Epoch 971/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2124 - val_loss: 3.4629\n",
            "Epoch 972/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2121 - val_loss: 3.4698\n",
            "Epoch 973/1000\n",
            "16/16 [==============================] - 0s 961us/step - loss: 0.2117 - val_loss: 3.4706\n",
            "Epoch 974/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2109 - val_loss: 3.4641\n",
            "Epoch 975/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2106 - val_loss: 3.5056\n",
            "Epoch 976/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2106 - val_loss: 3.5343\n",
            "Epoch 977/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2104 - val_loss: 3.5182\n",
            "Epoch 978/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2095 - val_loss: 3.5084\n",
            "Epoch 979/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2088 - val_loss: 3.5313\n",
            "Epoch 980/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2083 - val_loss: 3.5179\n",
            "Epoch 981/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2076 - val_loss: 3.5024\n",
            "Epoch 982/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2070 - val_loss: 3.5167\n",
            "Epoch 983/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2065 - val_loss: 3.5005\n",
            "Epoch 984/1000\n",
            "16/16 [==============================] - 0s 995us/step - loss: 0.2058 - val_loss: 3.5211\n",
            "Epoch 985/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2049 - val_loss: 3.4860\n",
            "Epoch 986/1000\n",
            "16/16 [==============================] - 0s 946us/step - loss: 0.2034 - val_loss: 3.4239\n",
            "Epoch 987/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2031 - val_loss: 3.3532\n",
            "Epoch 988/1000\n",
            "16/16 [==============================] - 0s 953us/step - loss: 0.2028 - val_loss: 3.2959\n",
            "Epoch 989/1000\n",
            "16/16 [==============================] - 0s 960us/step - loss: 0.2017 - val_loss: 3.2738\n",
            "Epoch 990/1000\n",
            "16/16 [==============================] - 0s 923us/step - loss: 0.2011 - val_loss: 3.2603\n",
            "Epoch 991/1000\n",
            "16/16 [==============================] - 0s 839us/step - loss: 0.2007 - val_loss: 3.2395\n",
            "Epoch 992/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2009 - val_loss: 3.2063\n",
            "Epoch 993/1000\n",
            "16/16 [==============================] - 0s 836us/step - loss: 0.2004 - val_loss: 3.1879\n",
            "Epoch 994/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2004 - val_loss: 3.1661\n",
            "Epoch 995/1000\n",
            "16/16 [==============================] - 0s 952us/step - loss: 0.1997 - val_loss: 3.1760\n",
            "Epoch 996/1000\n",
            "16/16 [==============================] - 0s 890us/step - loss: 0.1984 - val_loss: 3.2040\n",
            "Epoch 997/1000\n",
            "16/16 [==============================] - 0s 951us/step - loss: 0.1977 - val_loss: 3.2213\n",
            "Epoch 998/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1964 - val_loss: 3.2299\n",
            "Epoch 999/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1954 - val_loss: 3.2355\n",
            "Epoch 1000/1000\n",
            "16/16 [==============================] - 0s 889us/step - loss: 0.1949 - val_loss: 3.2394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f99ffca0ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br5IX6u3XdPm",
        "colab_type": "code",
        "outputId": "f1681862-17b7-4099-f8c3-7f3b28832f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_input = array([30])\n",
        "test_input = test_input.reshape((1, 1, 1))\n",
        "test_output = model.predict(test_input, verbose=0)\n",
        "print(test_output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[291.2891]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}