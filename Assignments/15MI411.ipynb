{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyMm4AWBO3SE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymYAOnxLQD2Q"
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "a = 11\n",
    "X = []\n",
    "for i in range(1,100):X.append(a*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICw9iZa8bo19"
   },
   "outputs": [],
   "source": [
    "Y = X[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RukbeQHryjGQ",
    "outputId": "c044c715-69b8-40df-8e2f-2c7729ae5bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]+X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mqiYv_dk1AZg",
    "outputId": "c8dff11f-5bf1-4ebf-9963-5f1cfdf0ff66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1]+X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EFIm1S_izes"
   },
   "outputs": [],
   "source": [
    "Dataset = np.column_stack((X, Y))\n",
    "target_data = [X+Y for X,Y in zip(X,Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NIN9ZCxaf3v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1089\n",
      "22 1078\n",
      "33 1067\n",
      "44 1056\n",
      "55 1045\n",
      "66 1034\n",
      "77 1023\n",
      "88 1012\n",
      "99 1001\n",
      "110 990\n",
      "121 979\n",
      "132 968\n",
      "143 957\n",
      "154 946\n",
      "165 935\n",
      "176 924\n",
      "187 913\n",
      "198 902\n",
      "209 891\n",
      "220 880\n",
      "231 869\n",
      "242 858\n",
      "253 847\n",
      "264 836\n",
      "275 825\n",
      "286 814\n",
      "297 803\n",
      "308 792\n",
      "319 781\n",
      "330 770\n",
      "341 759\n",
      "352 748\n",
      "363 737\n",
      "374 726\n",
      "385 715\n",
      "396 704\n",
      "407 693\n",
      "418 682\n",
      "429 671\n",
      "440 660\n",
      "451 649\n",
      "462 638\n",
      "473 627\n",
      "484 616\n",
      "495 605\n",
      "506 594\n",
      "517 583\n",
      "528 572\n",
      "539 561\n",
      "550 550\n",
      "561 539\n",
      "572 528\n",
      "583 517\n",
      "594 506\n",
      "605 495\n",
      "616 484\n",
      "627 473\n",
      "638 462\n",
      "649 451\n",
      "660 440\n",
      "671 429\n",
      "682 418\n",
      "693 407\n",
      "704 396\n",
      "715 385\n",
      "726 374\n",
      "737 363\n",
      "748 352\n",
      "759 341\n",
      "770 330\n",
      "781 319\n",
      "792 308\n",
      "803 297\n",
      "814 286\n",
      "825 275\n",
      "836 264\n",
      "847 253\n",
      "858 242\n",
      "869 231\n",
      "880 220\n",
      "891 209\n",
      "902 198\n",
      "913 187\n",
      "924 176\n",
      "935 165\n",
      "946 154\n",
      "957 143\n",
      "968 132\n",
      "979 121\n",
      "990 110\n",
      "1001 99\n",
      "1012 88\n",
      "1023 77\n",
      "1034 66\n",
      "1045 55\n",
      "1056 44\n",
      "1067 33\n",
      "1078 22\n",
      "1089 11\n"
     ]
    }
   ],
   "source": [
    " for i in range(99):\n",
    "    print(X[i],Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaxy1iDUhPKP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    " for i in range(99):\n",
    "    print(X[i]+Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fJqX7OVj_QV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11, 1089],\n",
       "       [  22, 1078],\n",
       "       [  33, 1067],\n",
       "       [  44, 1056],\n",
       "       [  55, 1045],\n",
       "       [  66, 1034],\n",
       "       [  77, 1023],\n",
       "       [  88, 1012],\n",
       "       [  99, 1001],\n",
       "       [ 110,  990],\n",
       "       [ 121,  979],\n",
       "       [ 132,  968],\n",
       "       [ 143,  957],\n",
       "       [ 154,  946],\n",
       "       [ 165,  935],\n",
       "       [ 176,  924],\n",
       "       [ 187,  913],\n",
       "       [ 198,  902],\n",
       "       [ 209,  891],\n",
       "       [ 220,  880],\n",
       "       [ 231,  869],\n",
       "       [ 242,  858],\n",
       "       [ 253,  847],\n",
       "       [ 264,  836],\n",
       "       [ 275,  825],\n",
       "       [ 286,  814],\n",
       "       [ 297,  803],\n",
       "       [ 308,  792],\n",
       "       [ 319,  781],\n",
       "       [ 330,  770],\n",
       "       [ 341,  759],\n",
       "       [ 352,  748],\n",
       "       [ 363,  737],\n",
       "       [ 374,  726],\n",
       "       [ 385,  715],\n",
       "       [ 396,  704],\n",
       "       [ 407,  693],\n",
       "       [ 418,  682],\n",
       "       [ 429,  671],\n",
       "       [ 440,  660],\n",
       "       [ 451,  649],\n",
       "       [ 462,  638],\n",
       "       [ 473,  627],\n",
       "       [ 484,  616],\n",
       "       [ 495,  605],\n",
       "       [ 506,  594],\n",
       "       [ 517,  583],\n",
       "       [ 528,  572],\n",
       "       [ 539,  561],\n",
       "       [ 550,  550],\n",
       "       [ 561,  539],\n",
       "       [ 572,  528],\n",
       "       [ 583,  517],\n",
       "       [ 594,  506],\n",
       "       [ 605,  495],\n",
       "       [ 616,  484],\n",
       "       [ 627,  473],\n",
       "       [ 638,  462],\n",
       "       [ 649,  451],\n",
       "       [ 660,  440],\n",
       "       [ 671,  429],\n",
       "       [ 682,  418],\n",
       "       [ 693,  407],\n",
       "       [ 704,  396],\n",
       "       [ 715,  385],\n",
       "       [ 726,  374],\n",
       "       [ 737,  363],\n",
       "       [ 748,  352],\n",
       "       [ 759,  341],\n",
       "       [ 770,  330],\n",
       "       [ 781,  319],\n",
       "       [ 792,  308],\n",
       "       [ 803,  297],\n",
       "       [ 814,  286],\n",
       "       [ 825,  275],\n",
       "       [ 836,  264],\n",
       "       [ 847,  253],\n",
       "       [ 858,  242],\n",
       "       [ 869,  231],\n",
       "       [ 880,  220],\n",
       "       [ 891,  209],\n",
       "       [ 902,  198],\n",
       "       [ 913,  187],\n",
       "       [ 924,  176],\n",
       "       [ 935,  165],\n",
       "       [ 946,  154],\n",
       "       [ 957,  143],\n",
       "       [ 968,  132],\n",
       "       [ 979,  121],\n",
       "       [ 990,  110],\n",
       "       [1001,   99],\n",
       "       [1012,   88],\n",
       "       [1023,   77],\n",
       "       [1034,   66],\n",
       "       [1045,   55],\n",
       "       [1056,   44],\n",
       "       [1067,   33],\n",
       "       [1078,   22],\n",
       "       [1089,   11]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDYEZiYokE-P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100,\n",
       " 1100]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-whHOrCNkTSb"
   },
   "outputs": [],
   "source": [
    "Dataset=np.array(Dataset, dtype=\"float\")\n",
    "target_data=np.array(target_data, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ua28ezZRFGS",
    "outputId": "a83e4309-b171-4627-ef2d-048f09cb529f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 2, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset = np.array(Dataset).reshape(99, 2,1)\n",
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CJeoWr1QRleE",
    "outputId": "a1f6f224-5fbb-4ad4-f210-c52e73a82085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H67cjNOolfub"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(Dataset,target_data,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "colab_type": "code",
    "id": "hIHbtZkBSCV0",
    "outputId": "f0c77110-534a-4dc8-fb0b-fb783357c791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 2, 200)            161600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 2, 100)            120400    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 2, 50)             30200     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 25)                7600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                520       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 320,541\n",
      "Trainable params: 320,541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(2,1)))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(25, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UrdLR33XSW5w",
    "outputId": "9de15a1b-af5f-4a26-b2e2-ce7686d9de1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DarkLord\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 79 samples, validate on 20 samples\n",
      "Epoch 1/500\n",
      "79/79 [==============================] - 5s 68ms/step - loss: 1210553.3386 - val_loss: 1210130.6250\n",
      "Epoch 2/500\n",
      "79/79 [==============================] - 0s 853us/step - loss: 1210047.4731 - val_loss: 1209953.8750\n",
      "Epoch 3/500\n",
      "79/79 [==============================] - 0s 776us/step - loss: 1209941.0095 - val_loss: 1209847.7500\n",
      "Epoch 4/500\n",
      "79/79 [==============================] - 0s 874us/step - loss: 1209839.5854 - val_loss: 1209670.2500\n",
      "Epoch 5/500\n",
      "79/79 [==============================] - 0s 865us/step - loss: 1209648.6551 - val_loss: 1209278.2500\n",
      "Epoch 6/500\n",
      "79/79 [==============================] - 0s 925us/step - loss: 1209104.2627 - val_loss: 1208103.2500\n",
      "Epoch 7/500\n",
      "79/79 [==============================] - 0s 884us/step - loss: 1207553.5934 - val_loss: 1205732.1250\n",
      "Epoch 8/500\n",
      "79/79 [==============================] - 0s 861us/step - loss: 1204024.9335 - val_loss: 1199748.3750\n",
      "Epoch 9/500\n",
      "79/79 [==============================] - 0s 873us/step - loss: 1196054.7848 - val_loss: 1189529.7500\n",
      "Epoch 10/500\n",
      "79/79 [==============================] - 0s 877us/step - loss: 1183279.3528 - val_loss: 1173537.2500\n",
      "Epoch 11/500\n",
      "79/79 [==============================] - 0s 872us/step - loss: 1162130.9794 - val_loss: 1141366.6250\n",
      "Epoch 12/500\n",
      "79/79 [==============================] - 0s 875us/step - loss: 1123579.6551 - val_loss: 1084902.0000\n",
      "Epoch 13/500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 1052383.8695 - val_loss: 984812.6250\n",
      "Epoch 14/500\n",
      "79/79 [==============================] - 0s 897us/step - loss: 935569.2366 - val_loss: 846376.6875\n",
      "Epoch 15/500\n",
      "79/79 [==============================] - 0s 881us/step - loss: 756583.3275 - val_loss: 630954.3750\n",
      "Epoch 16/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 515323.4964 - val_loss: 355312.3750\n",
      "Epoch 17/500\n",
      "79/79 [==============================] - 0s 853us/step - loss: 234373.3119 - val_loss: 180163.4062\n",
      "Epoch 18/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 178055.0943 - val_loss: 218664.8750\n",
      "Epoch 19/500\n",
      "79/79 [==============================] - 0s 972us/step - loss: 199076.2644 - val_loss: 163442.4219\n",
      "Epoch 20/500\n",
      "79/79 [==============================] - 0s 856us/step - loss: 131709.0374 - val_loss: 143222.7031\n",
      "Epoch 21/500\n",
      "79/79 [==============================] - 0s 917us/step - loss: 114050.4312 - val_loss: 144547.5000\n",
      "Epoch 22/500\n",
      "79/79 [==============================] - 0s 929us/step - loss: 109376.7594 - val_loss: 126295.7969\n",
      "Epoch 23/500\n",
      "79/79 [==============================] - 0s 990us/step - loss: 93107.9435 - val_loss: 100570.8047\n",
      "Epoch 24/500\n",
      "79/79 [==============================] - 0s 857us/step - loss: 74314.8268 - val_loss: 69957.1016\n",
      "Epoch 25/500\n",
      "79/79 [==============================] - 0s 931us/step - loss: 57389.4157 - val_loss: 49015.8711\n",
      "Epoch 26/500\n",
      "79/79 [==============================] - 0s 854us/step - loss: 43794.5410 - val_loss: 36652.5391\n",
      "Epoch 27/500\n",
      "79/79 [==============================] - 0s 896us/step - loss: 34598.4333 - val_loss: 27340.5469\n",
      "Epoch 28/500\n",
      "79/79 [==============================] - 0s 883us/step - loss: 27147.0437 - val_loss: 18941.5488\n",
      "Epoch 29/500\n",
      "79/79 [==============================] - 0s 729us/step - loss: 22734.4675 - val_loss: 14815.5059\n",
      "Epoch 30/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 16505.5513 - val_loss: 10888.8408\n",
      "Epoch 31/500\n",
      "79/79 [==============================] - 0s 926us/step - loss: 13073.4153 - val_loss: 8458.9453\n",
      "Epoch 32/500\n",
      "79/79 [==============================] - 0s 920us/step - loss: 10124.5515 - val_loss: 4684.0327\n",
      "Epoch 33/500\n",
      "79/79 [==============================] - 0s 815us/step - loss: 7906.8618 - val_loss: 6924.2197\n",
      "Epoch 34/500\n",
      "79/79 [==============================] - 0s 842us/step - loss: 7803.2495 - val_loss: 3251.2427\n",
      "Epoch 35/500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 6560.2097 - val_loss: 2686.1401\n",
      "Epoch 36/500\n",
      "79/79 [==============================] - 0s 903us/step - loss: 5280.6045 - val_loss: 1710.1810\n",
      "Epoch 37/500\n",
      "79/79 [==============================] - 0s 918us/step - loss: 4368.5969 - val_loss: 673.3427\n",
      "Epoch 38/500\n",
      "79/79 [==============================] - 0s 890us/step - loss: 3546.5212 - val_loss: 2286.7632\n",
      "Epoch 39/500\n",
      "79/79 [==============================] - 0s 916us/step - loss: 3843.4441 - val_loss: 455.8655\n",
      "Epoch 40/500\n",
      "79/79 [==============================] - 0s 893us/step - loss: 4071.6533 - val_loss: 860.7677\n",
      "Epoch 41/500\n",
      "79/79 [==============================] - 0s 959us/step - loss: 2739.3349 - val_loss: 869.1048\n",
      "Epoch 42/500\n",
      "79/79 [==============================] - 0s 906us/step - loss: 2573.7461 - val_loss: 672.4052\n",
      "Epoch 43/500\n",
      "79/79 [==============================] - 0s 974us/step - loss: 1931.6299 - val_loss: 348.0046\n",
      "Epoch 44/500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 1832.9723 - val_loss: 693.4904\n",
      "Epoch 45/500\n",
      "79/79 [==============================] - 0s 931us/step - loss: 1733.1240 - val_loss: 216.5290\n",
      "Epoch 46/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1313.7834 - val_loss: 298.5662\n",
      "Epoch 47/500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 1457.9375 - val_loss: 190.5057\n",
      "Epoch 48/500\n",
      "79/79 [==============================] - 0s 808us/step - loss: 1455.2912 - val_loss: 1149.6194\n",
      "Epoch 49/500\n",
      "79/79 [==============================] - 0s 953us/step - loss: 1540.4093 - val_loss: 386.3415\n",
      "Epoch 50/500\n",
      "79/79 [==============================] - 0s 758us/step - loss: 1263.6817 - val_loss: 466.6923\n",
      "Epoch 51/500\n",
      "79/79 [==============================] - 0s 888us/step - loss: 973.4593 - val_loss: 277.4796\n",
      "Epoch 52/500\n",
      "79/79 [==============================] - 0s 891us/step - loss: 928.4632 - val_loss: 771.1437\n",
      "Epoch 53/500\n",
      "79/79 [==============================] - 0s 893us/step - loss: 920.2728 - val_loss: 256.4205\n",
      "Epoch 54/500\n",
      "79/79 [==============================] - 0s 894us/step - loss: 795.0108 - val_loss: 135.5924\n",
      "Epoch 55/500\n",
      "79/79 [==============================] - 0s 893us/step - loss: 687.2326 - val_loss: 183.7829\n",
      "Epoch 56/500\n",
      "79/79 [==============================] - 0s 896us/step - loss: 691.5808 - val_loss: 49.6819\n",
      "Epoch 57/500\n",
      "79/79 [==============================] - 0s 903us/step - loss: 633.2507 - val_loss: 196.3267\n",
      "Epoch 58/500\n",
      "79/79 [==============================] - 0s 882us/step - loss: 629.9923 - val_loss: 283.3205\n",
      "Epoch 59/500\n",
      "79/79 [==============================] - 0s 908us/step - loss: 500.2448 - val_loss: 78.1278\n",
      "Epoch 60/500\n",
      "79/79 [==============================] - 0s 867us/step - loss: 426.1155 - val_loss: 155.1307\n",
      "Epoch 61/500\n",
      "79/79 [==============================] - 0s 908us/step - loss: 371.9335 - val_loss: 121.8540\n",
      "Epoch 62/500\n",
      "79/79 [==============================] - 0s 875us/step - loss: 343.5152 - val_loss: 139.3603\n",
      "Epoch 63/500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 310.8386 - val_loss: 80.8919\n",
      "Epoch 64/500\n",
      "79/79 [==============================] - 0s 875us/step - loss: 274.9185 - val_loss: 108.0878\n",
      "Epoch 65/500\n",
      "79/79 [==============================] - 0s 927us/step - loss: 246.8392 - val_loss: 50.1364\n",
      "Epoch 66/500\n",
      "79/79 [==============================] - 0s 949us/step - loss: 163.1482 - val_loss: 96.8882\n",
      "Epoch 67/500\n",
      "79/79 [==============================] - 0s 904us/step - loss: 204.9717 - val_loss: 43.4729\n",
      "Epoch 68/500\n",
      "79/79 [==============================] - 0s 880us/step - loss: 160.3566 - val_loss: 94.0625\n",
      "Epoch 69/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 940us/step - loss: 174.1375 - val_loss: 57.3635\n",
      "Epoch 70/500\n",
      "79/79 [==============================] - ETA: 0s - loss: 161.364 - 0s 757us/step - loss: 183.4746 - val_loss: 208.7318\n",
      "Epoch 71/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 231.2499 - val_loss: 176.3157\n",
      "Epoch 72/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 221.3527 - val_loss: 109.7344\n",
      "Epoch 73/500\n",
      "79/79 [==============================] - 0s 947us/step - loss: 217.6042 - val_loss: 59.8799\n",
      "Epoch 74/500\n",
      "79/79 [==============================] - 0s 934us/step - loss: 133.1485 - val_loss: 24.8516\n",
      "Epoch 75/500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 107.2483 - val_loss: 44.7153\n",
      "Epoch 76/500\n",
      "79/79 [==============================] - 0s 817us/step - loss: 106.2677 - val_loss: 49.3373\n",
      "Epoch 77/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 103.8869 - val_loss: 38.5505\n",
      "Epoch 78/500\n",
      "79/79 [==============================] - 0s 848us/step - loss: 87.5964 - val_loss: 43.2033\n",
      "Epoch 79/500\n",
      "79/79 [==============================] - 0s 867us/step - loss: 83.3800 - val_loss: 107.2632\n",
      "Epoch 80/500\n",
      "79/79 [==============================] - 0s 881us/step - loss: 113.5170 - val_loss: 58.9325\n",
      "Epoch 81/500\n",
      "79/79 [==============================] - 0s 897us/step - loss: 101.0161 - val_loss: 72.0165\n",
      "Epoch 82/500\n",
      "79/79 [==============================] - 0s 841us/step - loss: 88.9897 - val_loss: 50.9944\n",
      "Epoch 83/500\n",
      "79/79 [==============================] - 0s 819us/step - loss: 67.9566 - val_loss: 40.4142\n",
      "Epoch 84/500\n",
      "79/79 [==============================] - 0s 866us/step - loss: 56.5250 - val_loss: 41.6854\n",
      "Epoch 85/500\n",
      "79/79 [==============================] - 0s 871us/step - loss: 57.1088 - val_loss: 44.8840\n",
      "Epoch 86/500\n",
      "79/79 [==============================] - 0s 807us/step - loss: 47.9742 - val_loss: 53.5524\n",
      "Epoch 87/500\n",
      "79/79 [==============================] - 0s 856us/step - loss: 71.0773 - val_loss: 74.8864\n",
      "Epoch 88/500\n",
      "79/79 [==============================] - 0s 856us/step - loss: 68.1347 - val_loss: 70.7560\n",
      "Epoch 89/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 63.3909 - val_loss: 31.9705\n",
      "Epoch 90/500\n",
      "79/79 [==============================] - 0s 845us/step - loss: 39.4639 - val_loss: 45.9616\n",
      "Epoch 91/500\n",
      "79/79 [==============================] - 0s 887us/step - loss: 44.7056 - val_loss: 43.9319\n",
      "Epoch 92/500\n",
      "79/79 [==============================] - 0s 896us/step - loss: 31.4726 - val_loss: 40.7798\n",
      "Epoch 93/500\n",
      "79/79 [==============================] - 0s 876us/step - loss: 38.8772 - val_loss: 70.8064\n",
      "Epoch 94/500\n",
      "79/79 [==============================] - 0s 891us/step - loss: 46.3752 - val_loss: 46.8438\n",
      "Epoch 95/500\n",
      "79/79 [==============================] - 0s 849us/step - loss: 34.6226 - val_loss: 39.7320\n",
      "Epoch 96/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 29.3167 - val_loss: 32.5297\n",
      "Epoch 97/500\n",
      "79/79 [==============================] - 0s 892us/step - loss: 18.1483 - val_loss: 56.2124\n",
      "Epoch 98/500\n",
      "79/79 [==============================] - 0s 854us/step - loss: 20.4509 - val_loss: 61.3753\n",
      "Epoch 99/500\n",
      "79/79 [==============================] - 0s 842us/step - loss: 25.5261 - val_loss: 45.4579\n",
      "Epoch 100/500\n",
      "79/79 [==============================] - 0s 865us/step - loss: 30.7062 - val_loss: 44.3968\n",
      "Epoch 101/500\n",
      "79/79 [==============================] - 0s 935us/step - loss: 27.0660 - val_loss: 36.1413\n",
      "Epoch 102/500\n",
      "79/79 [==============================] - 0s 907us/step - loss: 24.1385 - val_loss: 30.8380\n",
      "Epoch 103/500\n",
      "79/79 [==============================] - 0s 947us/step - loss: 39.8425 - val_loss: 47.7992\n",
      "Epoch 104/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 30.4948 - val_loss: 65.4667\n",
      "Epoch 105/500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 19.8499 - val_loss: 68.8378\n",
      "Epoch 106/500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 67.5000 - val_loss: 32.8616\n",
      "Epoch 107/500\n",
      "79/79 [==============================] - 0s 880us/step - loss: 26.1880 - val_loss: 116.3467\n",
      "Epoch 108/500\n",
      "79/79 [==============================] - 0s 957us/step - loss: 88.6878 - val_loss: 69.9814\n",
      "Epoch 109/500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 29.4852 - val_loss: 22.8517\n",
      "Epoch 110/500\n",
      "79/79 [==============================] - 0s 891us/step - loss: 22.2666 - val_loss: 38.7574\n",
      "Epoch 111/500\n",
      "79/79 [==============================] - 0s 992us/step - loss: 34.7450 - val_loss: 41.4043\n",
      "Epoch 112/500\n",
      "79/79 [==============================] - 0s 898us/step - loss: 32.9673 - val_loss: 29.5558\n",
      "Epoch 113/500\n",
      "79/79 [==============================] - 0s 757us/step - loss: 28.7687 - val_loss: 37.8741\n",
      "Epoch 114/500\n",
      "79/79 [==============================] - 0s 858us/step - loss: 26.1556 - val_loss: 31.7349\n",
      "Epoch 115/500\n",
      "79/79 [==============================] - 0s 820us/step - loss: 36.7706 - val_loss: 29.5404\n",
      "Epoch 116/500\n",
      "79/79 [==============================] - 0s 857us/step - loss: 27.0031 - val_loss: 27.5125\n",
      "Epoch 117/500\n",
      "79/79 [==============================] - 0s 927us/step - loss: 29.7573 - val_loss: 30.4170\n",
      "Epoch 118/500\n",
      "79/79 [==============================] - 0s 871us/step - loss: 27.4776 - val_loss: 26.4396\n",
      "Epoch 119/500\n",
      "79/79 [==============================] - 0s 919us/step - loss: 22.6622 - val_loss: 67.1961\n",
      "Epoch 120/500\n",
      "79/79 [==============================] - 0s 831us/step - loss: 41.8872 - val_loss: 49.9034\n",
      "Epoch 121/500\n",
      "79/79 [==============================] - 0s 811us/step - loss: 38.8495 - val_loss: 68.8337\n",
      "Epoch 122/500\n",
      "79/79 [==============================] - 0s 831us/step - loss: 75.6795 - val_loss: 36.0992\n",
      "Epoch 123/500\n",
      "79/79 [==============================] - 0s 823us/step - loss: 50.2773 - val_loss: 18.7260\n",
      "Epoch 124/500\n",
      "79/79 [==============================] - 0s 838us/step - loss: 33.8581 - val_loss: 36.4146\n",
      "Epoch 125/500\n",
      "79/79 [==============================] - 0s 794us/step - loss: 57.1150 - val_loss: 47.6005\n",
      "Epoch 126/500\n",
      "79/79 [==============================] - 0s 792us/step - loss: 33.8595 - val_loss: 31.1757\n",
      "Epoch 127/500\n",
      "79/79 [==============================] - 0s 790us/step - loss: 37.2176 - val_loss: 25.6817\n",
      "Epoch 128/500\n",
      "79/79 [==============================] - 0s 840us/step - loss: 22.7647 - val_loss: 18.3972\n",
      "Epoch 129/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 35.2080 - val_loss: 45.6800\n",
      "Epoch 130/500\n",
      "79/79 [==============================] - 0s 812us/step - loss: 43.0950 - val_loss: 59.1528\n",
      "Epoch 131/500\n",
      "79/79 [==============================] - 0s 808us/step - loss: 54.2008 - val_loss: 97.7305\n",
      "Epoch 132/500\n",
      "79/79 [==============================] - 0s 837us/step - loss: 61.9369 - val_loss: 28.3016\n",
      "Epoch 133/500\n",
      "79/79 [==============================] - 0s 797us/step - loss: 93.9872 - val_loss: 20.6173\n",
      "Epoch 134/500\n",
      "79/79 [==============================] - 0s 788us/step - loss: 117.1542 - val_loss: 269.5509\n",
      "Epoch 135/500\n",
      "79/79 [==============================] - 0s 781us/step - loss: 324.1542 - val_loss: 557.2357\n",
      "Epoch 136/500\n",
      "79/79 [==============================] - 0s 781us/step - loss: 483.7693 - val_loss: 362.5524\n",
      "Epoch 137/500\n",
      "79/79 [==============================] - 0s 819us/step - loss: 368.2654 - val_loss: 1777.9729\n",
      "Epoch 138/500\n",
      "79/79 [==============================] - 0s 823us/step - loss: 1095.7699 - val_loss: 1591.0950\n",
      "Epoch 139/500\n",
      "79/79 [==============================] - 0s 782us/step - loss: 1418.7396 - val_loss: 610.8902\n",
      "Epoch 140/500\n",
      "79/79 [==============================] - 0s 808us/step - loss: 841.2193 - val_loss: 256.3198\n",
      "Epoch 141/500\n",
      "79/79 [==============================] - 0s 763us/step - loss: 585.3995 - val_loss: 435.7620\n",
      "Epoch 142/500\n",
      "79/79 [==============================] - 0s 790us/step - loss: 489.7640 - val_loss: 282.9854\n",
      "Epoch 143/500\n",
      "79/79 [==============================] - 0s 788us/step - loss: 699.3493 - val_loss: 771.4821\n",
      "Epoch 144/500\n",
      "79/79 [==============================] - 0s 787us/step - loss: 548.6058 - val_loss: 370.5799\n",
      "Epoch 145/500\n",
      "79/79 [==============================] - 0s 784us/step - loss: 659.3060 - val_loss: 353.3106\n",
      "Epoch 146/500\n",
      "79/79 [==============================] - 0s 780us/step - loss: 384.3788 - val_loss: 306.3173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/500\n",
      "79/79 [==============================] - 0s 825us/step - loss: 370.2893 - val_loss: 357.6513\n",
      "Epoch 148/500\n",
      "79/79 [==============================] - 0s 794us/step - loss: 453.8498 - val_loss: 291.7947\n",
      "Epoch 149/500\n",
      "79/79 [==============================] - 0s 781us/step - loss: 279.7707 - val_loss: 257.9805\n",
      "Epoch 150/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 237.8283 - val_loss: 376.4442\n",
      "Epoch 151/500\n",
      "79/79 [==============================] - 0s 793us/step - loss: 299.4517 - val_loss: 156.5151\n",
      "Epoch 152/500\n",
      "79/79 [==============================] - 0s 765us/step - loss: 252.1510 - val_loss: 254.7124\n",
      "Epoch 153/500\n",
      "79/79 [==============================] - 0s 831us/step - loss: 256.3059 - val_loss: 327.9179\n",
      "Epoch 154/500\n",
      "79/79 [==============================] - 0s 814us/step - loss: 378.9217 - val_loss: 123.4551\n",
      "Epoch 155/500\n",
      "79/79 [==============================] - 0s 781us/step - loss: 175.5328 - val_loss: 113.8706\n",
      "Epoch 156/500\n",
      "79/79 [==============================] - 0s 821us/step - loss: 105.9748 - val_loss: 92.7564\n",
      "Epoch 157/500\n",
      "79/79 [==============================] - 0s 821us/step - loss: 172.1729 - val_loss: 137.8030\n",
      "Epoch 158/500\n",
      "79/79 [==============================] - 0s 787us/step - loss: 124.6392 - val_loss: 145.7980\n",
      "Epoch 159/500\n",
      "79/79 [==============================] - 0s 838us/step - loss: 109.6793 - val_loss: 134.8580\n",
      "Epoch 160/500\n",
      "79/79 [==============================] - 0s 826us/step - loss: 114.3795 - val_loss: 110.2989\n",
      "Epoch 161/500\n",
      "79/79 [==============================] - 0s 829us/step - loss: 73.5039 - val_loss: 42.5867\n",
      "Epoch 162/500\n",
      "79/79 [==============================] - 0s 796us/step - loss: 62.2285 - val_loss: 114.2570\n",
      "Epoch 163/500\n",
      "79/79 [==============================] - 0s 764us/step - loss: 68.6166 - val_loss: 143.2614\n",
      "Epoch 164/500\n",
      "79/79 [==============================] - 0s 780us/step - loss: 57.0560 - val_loss: 236.4759\n",
      "Epoch 165/500\n",
      "79/79 [==============================] - 0s 782us/step - loss: 43.7347 - val_loss: 224.8613\n",
      "Epoch 166/500\n",
      "79/79 [==============================] - 0s 813us/step - loss: 33.1625 - val_loss: 199.3178\n",
      "Epoch 167/500\n",
      "79/79 [==============================] - 0s 840us/step - loss: 38.5471 - val_loss: 202.2043\n",
      "Epoch 168/500\n",
      "79/79 [==============================] - 0s 815us/step - loss: 30.8693 - val_loss: 209.4883\n",
      "Epoch 169/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 31.7762 - val_loss: 207.9314\n",
      "Epoch 170/500\n",
      "79/79 [==============================] - 0s 818us/step - loss: 28.1755 - val_loss: 190.6124\n",
      "Epoch 171/500\n",
      "79/79 [==============================] - 0s 853us/step - loss: 24.5979 - val_loss: 161.0284\n",
      "Epoch 172/500\n",
      "79/79 [==============================] - 0s 817us/step - loss: 25.8869 - val_loss: 124.8870\n",
      "Epoch 173/500\n",
      "79/79 [==============================] - 0s 854us/step - loss: 29.1856 - val_loss: 90.9430\n",
      "Epoch 174/500\n",
      "79/79 [==============================] - 0s 874us/step - loss: 25.8060 - val_loss: 67.8679\n",
      "Epoch 175/500\n",
      "79/79 [==============================] - 0s 789us/step - loss: 21.5963 - val_loss: 54.2619\n",
      "Epoch 176/500\n",
      "79/79 [==============================] - 0s 850us/step - loss: 22.0599 - val_loss: 40.5880\n",
      "Epoch 177/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 16.6234 - val_loss: 39.7846\n",
      "Epoch 178/500\n",
      "79/79 [==============================] - 0s 839us/step - loss: 16.5504 - val_loss: 33.4390\n",
      "Epoch 179/500\n",
      "79/79 [==============================] - 0s 871us/step - loss: 12.7152 - val_loss: 36.2255\n",
      "Epoch 180/500\n",
      "79/79 [==============================] - 0s 814us/step - loss: 12.3342 - val_loss: 28.7753\n",
      "Epoch 181/500\n",
      "79/79 [==============================] - 0s 837us/step - loss: 10.7095 - val_loss: 29.0780\n",
      "Epoch 182/500\n",
      "79/79 [==============================] - 0s 826us/step - loss: 11.9618 - val_loss: 25.3906\n",
      "Epoch 183/500\n",
      "79/79 [==============================] - 0s 804us/step - loss: 9.5192 - val_loss: 26.1950\n",
      "Epoch 184/500\n",
      "79/79 [==============================] - 0s 784us/step - loss: 9.7690 - val_loss: 21.3665\n",
      "Epoch 185/500\n",
      "79/79 [==============================] - 0s 819us/step - loss: 9.8073 - val_loss: 24.6409\n",
      "Epoch 186/500\n",
      "79/79 [==============================] - 0s 774us/step - loss: 9.1631 - val_loss: 22.1677\n",
      "Epoch 187/500\n",
      "79/79 [==============================] - 0s 790us/step - loss: 8.5211 - val_loss: 20.5917\n",
      "Epoch 188/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 8.4729 - val_loss: 26.8421\n",
      "Epoch 189/500\n",
      "79/79 [==============================] - 0s 874us/step - loss: 8.2110 - val_loss: 27.6238\n",
      "Epoch 190/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 8.3243 - val_loss: 23.8995\n",
      "Epoch 191/500\n",
      "79/79 [==============================] - 0s 777us/step - loss: 7.9327 - val_loss: 21.1896\n",
      "Epoch 192/500\n",
      "79/79 [==============================] - 0s 786us/step - loss: 8.6807 - val_loss: 37.6224\n",
      "Epoch 193/500\n",
      "79/79 [==============================] - 0s 829us/step - loss: 10.9089 - val_loss: 25.6236\n",
      "Epoch 194/500\n",
      "79/79 [==============================] - 0s 855us/step - loss: 12.7019 - val_loss: 30.4395\n",
      "Epoch 195/500\n",
      "79/79 [==============================] - 0s 805us/step - loss: 10.4314 - val_loss: 21.7836\n",
      "Epoch 196/500\n",
      "79/79 [==============================] - 0s 776us/step - loss: 9.8084 - val_loss: 26.3807\n",
      "Epoch 197/500\n",
      "79/79 [==============================] - 0s 855us/step - loss: 8.4664 - val_loss: 23.0265\n",
      "Epoch 198/500\n",
      "79/79 [==============================] - 0s 899us/step - loss: 6.3626 - val_loss: 24.5953\n",
      "Epoch 199/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 7.1806 - val_loss: 23.9723\n",
      "Epoch 200/500\n",
      "79/79 [==============================] - 0s 850us/step - loss: 7.1288 - val_loss: 24.3468\n",
      "Epoch 201/500\n",
      "79/79 [==============================] - 0s 887us/step - loss: 6.0730 - val_loss: 20.2567\n",
      "Epoch 202/500\n",
      "79/79 [==============================] - 0s 872us/step - loss: 6.2942 - val_loss: 19.9207\n",
      "Epoch 203/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 6.2048 - val_loss: 25.2946\n",
      "Epoch 204/500\n",
      "79/79 [==============================] - 0s 839us/step - loss: 7.8879 - val_loss: 19.3949\n",
      "Epoch 205/500\n",
      "79/79 [==============================] - 0s 814us/step - loss: 6.6610 - val_loss: 22.2672\n",
      "Epoch 206/500\n",
      "79/79 [==============================] - 0s 840us/step - loss: 7.0963 - val_loss: 22.0369\n",
      "Epoch 207/500\n",
      "79/79 [==============================] - 0s 793us/step - loss: 8.7738 - val_loss: 20.3369\n",
      "Epoch 208/500\n",
      "79/79 [==============================] - 0s 814us/step - loss: 8.4730 - val_loss: 18.2987\n",
      "Epoch 209/500\n",
      "79/79 [==============================] - 0s 810us/step - loss: 6.1250 - val_loss: 23.0903\n",
      "Epoch 210/500\n",
      "79/79 [==============================] - 0s 818us/step - loss: 5.3856 - val_loss: 22.8851\n",
      "Epoch 211/500\n",
      "79/79 [==============================] - 0s 816us/step - loss: 4.8698 - val_loss: 21.2138\n",
      "Epoch 212/500\n",
      "79/79 [==============================] - 0s 896us/step - loss: 4.7293 - val_loss: 20.7328\n",
      "Epoch 213/500\n",
      "79/79 [==============================] - 0s 800us/step - loss: 4.6194 - val_loss: 21.0196\n",
      "Epoch 214/500\n",
      "79/79 [==============================] - 0s 828us/step - loss: 4.7197 - val_loss: 17.3925\n",
      "Epoch 215/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 4.9095 - val_loss: 17.7425\n",
      "Epoch 216/500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 5.1956 - val_loss: 19.8143\n",
      "Epoch 217/500\n",
      "79/79 [==============================] - 0s 887us/step - loss: 4.7139 - val_loss: 19.2116\n",
      "Epoch 218/500\n",
      "79/79 [==============================] - 0s 770us/step - loss: 4.8007 - val_loss: 19.9283\n",
      "Epoch 219/500\n",
      "79/79 [==============================] - 0s 844us/step - loss: 4.6963 - val_loss: 18.0551\n",
      "Epoch 220/500\n",
      "79/79 [==============================] - 0s 849us/step - loss: 4.6255 - val_loss: 17.7096\n",
      "Epoch 221/500\n",
      "79/79 [==============================] - 0s 919us/step - loss: 4.2616 - val_loss: 16.8746\n",
      "Epoch 222/500\n",
      "79/79 [==============================] - 0s 795us/step - loss: 5.5496 - val_loss: 17.3816\n",
      "Epoch 223/500\n",
      "79/79 [==============================] - 0s 899us/step - loss: 4.6323 - val_loss: 19.2158\n",
      "Epoch 224/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 4.8916 - val_loss: 20.1697\n",
      "Epoch 225/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 852us/step - loss: 6.2547 - val_loss: 19.4022\n",
      "Epoch 226/500\n",
      "79/79 [==============================] - 0s 852us/step - loss: 4.2004 - val_loss: 14.9258\n",
      "Epoch 227/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 4.5577 - val_loss: 15.9441\n",
      "Epoch 228/500\n",
      "79/79 [==============================] - 0s 801us/step - loss: 4.4144 - val_loss: 17.1442\n",
      "Epoch 229/500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 4.3174 - val_loss: 17.9168\n",
      "Epoch 230/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 4.5017 - val_loss: 19.2413\n",
      "Epoch 231/500\n",
      "79/79 [==============================] - 0s 838us/step - loss: 4.5135 - val_loss: 19.7751\n",
      "Epoch 232/500\n",
      "79/79 [==============================] - 0s 840us/step - loss: 4.6342 - val_loss: 14.6258\n",
      "Epoch 233/500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 4.2005 - val_loss: 14.6506\n",
      "Epoch 234/500\n",
      "79/79 [==============================] - 0s 838us/step - loss: 3.8913 - val_loss: 19.5091\n",
      "Epoch 235/500\n",
      "79/79 [==============================] - 0s 832us/step - loss: 4.4537 - val_loss: 17.7218\n",
      "Epoch 236/500\n",
      "79/79 [==============================] - 0s 849us/step - loss: 3.9588 - val_loss: 17.8855\n",
      "Epoch 237/500\n",
      "79/79 [==============================] - 0s 821us/step - loss: 3.9920 - val_loss: 18.3737\n",
      "Epoch 238/500\n",
      "79/79 [==============================] - 0s 786us/step - loss: 6.4469 - val_loss: 16.9674\n",
      "Epoch 239/500\n",
      "79/79 [==============================] - 0s 853us/step - loss: 5.1582 - val_loss: 17.4989\n",
      "Epoch 240/500\n",
      "79/79 [==============================] - 0s 901us/step - loss: 5.7169 - val_loss: 18.0644\n",
      "Epoch 241/500\n",
      "79/79 [==============================] - 0s 866us/step - loss: 4.6316 - val_loss: 18.4345\n",
      "Epoch 242/500\n",
      "79/79 [==============================] - 0s 818us/step - loss: 4.1862 - val_loss: 14.3589\n",
      "Epoch 243/500\n",
      "79/79 [==============================] - 0s 880us/step - loss: 3.8343 - val_loss: 14.1938\n",
      "Epoch 244/500\n",
      "79/79 [==============================] - 0s 812us/step - loss: 3.3093 - val_loss: 18.1529\n",
      "Epoch 245/500\n",
      "79/79 [==============================] - 0s 706us/step - loss: 3.6874 - val_loss: 18.6195\n",
      "Epoch 246/500\n",
      "79/79 [==============================] - 0s 757us/step - loss: 2.7742 - val_loss: 14.6406\n",
      "Epoch 247/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 3.8726 - val_loss: 14.0730\n",
      "Epoch 248/500\n",
      "79/79 [==============================] - 0s 841us/step - loss: 3.1777 - val_loss: 19.7406\n",
      "Epoch 249/500\n",
      "79/79 [==============================] - 0s 813us/step - loss: 3.5464 - val_loss: 15.5321\n",
      "Epoch 250/500\n",
      "79/79 [==============================] - 0s 853us/step - loss: 3.8036 - val_loss: 14.6567\n",
      "Epoch 251/500\n",
      "79/79 [==============================] - 0s 921us/step - loss: 3.9659 - val_loss: 16.3507\n",
      "Epoch 252/500\n",
      "79/79 [==============================] - 0s 806us/step - loss: 4.3201 - val_loss: 18.3302\n",
      "Epoch 253/500\n",
      "79/79 [==============================] - 0s 833us/step - loss: 5.0288 - val_loss: 17.0985\n",
      "Epoch 254/500\n",
      "79/79 [==============================] - 0s 813us/step - loss: 3.1488 - val_loss: 14.3479\n",
      "Epoch 255/500\n",
      "79/79 [==============================] - 0s 801us/step - loss: 4.5344 - val_loss: 15.1897\n",
      "Epoch 256/500\n",
      "79/79 [==============================] - 0s 825us/step - loss: 5.5087 - val_loss: 25.1930\n",
      "Epoch 257/500\n",
      "79/79 [==============================] - 0s 843us/step - loss: 9.9441 - val_loss: 17.1451\n",
      "Epoch 258/500\n",
      "79/79 [==============================] - 0s 842us/step - loss: 9.8135 - val_loss: 15.5734\n",
      "Epoch 259/500\n",
      "79/79 [==============================] - 0s 833us/step - loss: 6.0685 - val_loss: 22.6640\n",
      "Epoch 260/500\n",
      "79/79 [==============================] - 0s 838us/step - loss: 5.8024 - val_loss: 19.1590\n",
      "Epoch 261/500\n",
      "79/79 [==============================] - 0s 794us/step - loss: 5.1276 - val_loss: 22.3821\n",
      "Epoch 262/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 7.2908 - val_loss: 15.0813\n",
      "Epoch 263/500\n",
      "79/79 [==============================] - 0s 826us/step - loss: 6.2840 - val_loss: 21.6762\n",
      "Epoch 264/500\n",
      "79/79 [==============================] - 0s 826us/step - loss: 4.4939 - val_loss: 21.6461\n",
      "Epoch 265/500\n",
      "79/79 [==============================] - 0s 811us/step - loss: 4.9810 - val_loss: 16.0401\n",
      "Epoch 266/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 6.3861 - val_loss: 13.0812\n",
      "Epoch 267/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 4.6779 - val_loss: 12.8260\n",
      "Epoch 268/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 3.3545 - val_loss: 15.2366\n",
      "Epoch 269/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 5.5745 - val_loss: 16.4300\n",
      "Epoch 270/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 4.8648 - val_loss: 15.7960\n",
      "Epoch 271/500\n",
      "79/79 [==============================] - 0s 740us/step - loss: 4.4523 - val_loss: 13.4342\n",
      "Epoch 272/500\n",
      "79/79 [==============================] - 0s 669us/step - loss: 4.0466 - val_loss: 17.0376\n",
      "Epoch 273/500\n",
      "79/79 [==============================] - 0s 831us/step - loss: 4.2125 - val_loss: 15.2817\n",
      "Epoch 274/500\n",
      "79/79 [==============================] - 0s 833us/step - loss: 5.0947 - val_loss: 13.2127\n",
      "Epoch 275/500\n",
      "79/79 [==============================] - 0s 799us/step - loss: 3.2564 - val_loss: 14.2217\n",
      "Epoch 276/500\n",
      "79/79 [==============================] - 0s 816us/step - loss: 3.4865 - val_loss: 13.2196\n",
      "Epoch 277/500\n",
      "79/79 [==============================] - 0s 842us/step - loss: 4.5540 - val_loss: 14.6217\n",
      "Epoch 278/500\n",
      "79/79 [==============================] - 0s 807us/step - loss: 4.5293 - val_loss: 28.5501\n",
      "Epoch 279/500\n",
      "79/79 [==============================] - 0s 839us/step - loss: 8.2810 - val_loss: 14.4126\n",
      "Epoch 280/500\n",
      "79/79 [==============================] - 0s 851us/step - loss: 7.6967 - val_loss: 15.7332\n",
      "Epoch 281/500\n",
      "79/79 [==============================] - 0s 852us/step - loss: 10.8988 - val_loss: 35.6721\n",
      "Epoch 282/500\n",
      "79/79 [==============================] - 0s 986us/step - loss: 16.3185 - val_loss: 19.2584\n",
      "Epoch 283/500\n",
      "79/79 [==============================] - 0s 744us/step - loss: 22.0064 - val_loss: 14.7995\n",
      "Epoch 284/500\n",
      "79/79 [==============================] - 0s 797us/step - loss: 14.8035 - val_loss: 52.0658\n",
      "Epoch 285/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 30.4626 - val_loss: 65.7749\n",
      "Epoch 286/500\n",
      "79/79 [==============================] - 0s 835us/step - loss: 34.6584 - val_loss: 48.3703\n",
      "Epoch 287/500\n",
      "79/79 [==============================] - 0s 790us/step - loss: 48.3957 - val_loss: 116.1184\n",
      "Epoch 288/500\n",
      "79/79 [==============================] - 0s 852us/step - loss: 113.3699 - val_loss: 178.5539\n",
      "Epoch 289/500\n",
      "79/79 [==============================] - 0s 846us/step - loss: 127.2037 - val_loss: 149.7455\n",
      "Epoch 290/500\n",
      "79/79 [==============================] - 0s 848us/step - loss: 107.0262 - val_loss: 84.7750\n",
      "Epoch 291/500\n",
      "79/79 [==============================] - 0s 777us/step - loss: 122.2109 - val_loss: 120.7727\n",
      "Epoch 292/500\n",
      "79/79 [==============================] - 0s 807us/step - loss: 96.3068 - val_loss: 194.0907\n",
      "Epoch 293/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 113.1601 - val_loss: 84.8651\n",
      "Epoch 294/500\n",
      "79/79 [==============================] - 0s 804us/step - loss: 66.3604 - val_loss: 54.9999\n",
      "Epoch 295/500\n",
      "79/79 [==============================] - 0s 850us/step - loss: 41.2724 - val_loss: 32.6038\n",
      "Epoch 296/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 29.8048 - val_loss: 25.3735\n",
      "Epoch 297/500\n",
      "79/79 [==============================] - 0s 817us/step - loss: 49.9760 - val_loss: 51.7317\n",
      "Epoch 298/500\n",
      "79/79 [==============================] - 0s 800us/step - loss: 39.8059 - val_loss: 41.2667\n",
      "Epoch 299/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 39.1829 - val_loss: 39.4229\n",
      "Epoch 300/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 73.9426 - val_loss: 54.5121\n",
      "Epoch 301/500\n",
      "79/79 [==============================] - 0s 802us/step - loss: 38.8237 - val_loss: 85.7948\n",
      "Epoch 302/500\n",
      "79/79 [==============================] - 0s 870us/step - loss: 57.2777 - val_loss: 85.1501\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 823us/step - loss: 69.1589 - val_loss: 49.6169\n",
      "Epoch 304/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 47.8901 - val_loss: 37.2053\n",
      "Epoch 305/500\n",
      "79/79 [==============================] - 0s 813us/step - loss: 48.7027 - val_loss: 45.1855\n",
      "Epoch 306/500\n",
      "79/79 [==============================] - 0s 839us/step - loss: 47.2309 - val_loss: 53.7742\n",
      "Epoch 307/500\n",
      "79/79 [==============================] - 0s 785us/step - loss: 50.8856 - val_loss: 47.7296\n",
      "Epoch 308/500\n",
      "79/79 [==============================] - 0s 874us/step - loss: 56.3139 - val_loss: 122.3247\n",
      "Epoch 309/500\n",
      "79/79 [==============================] - 0s 840us/step - loss: 70.0410 - val_loss: 45.1013\n",
      "Epoch 310/500\n",
      "79/79 [==============================] - 0s 882us/step - loss: 81.8072 - val_loss: 59.6772\n",
      "Epoch 311/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 107.2321 - val_loss: 199.9544\n",
      "Epoch 312/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 131.6463 - val_loss: 164.9365\n",
      "Epoch 313/500\n",
      "79/79 [==============================] - 0s 830us/step - loss: 117.1856 - val_loss: 206.3765\n",
      "Epoch 314/500\n",
      "79/79 [==============================] - 0s 844us/step - loss: 214.5617 - val_loss: 107.0491\n",
      "Epoch 315/500\n",
      "79/79 [==============================] - 0s 826us/step - loss: 201.1660 - val_loss: 176.5976\n",
      "Epoch 316/500\n",
      "79/79 [==============================] - 0s 855us/step - loss: 114.2817 - val_loss: 122.3110\n",
      "Epoch 317/500\n",
      "79/79 [==============================] - 0s 835us/step - loss: 87.5364 - val_loss: 80.4377\n",
      "Epoch 318/500\n",
      "79/79 [==============================] - 0s 846us/step - loss: 73.8225 - val_loss: 108.4065\n",
      "Epoch 319/500\n",
      "79/79 [==============================] - 0s 861us/step - loss: 104.6470 - val_loss: 52.1830\n",
      "Epoch 320/500\n",
      "79/79 [==============================] - 0s 819us/step - loss: 61.4229 - val_loss: 94.1955\n",
      "Epoch 321/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 67.2531 - val_loss: 171.0872\n",
      "Epoch 322/500\n",
      "79/79 [==============================] - 0s 861us/step - loss: 109.5910 - val_loss: 161.8110\n",
      "Epoch 323/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 100.6693 - val_loss: 152.9200\n",
      "Epoch 324/500\n",
      "79/79 [==============================] - 0s 879us/step - loss: 122.0119 - val_loss: 38.8458\n",
      "Epoch 325/500\n",
      "79/79 [==============================] - 0s 807us/step - loss: 78.4976 - val_loss: 102.7883\n",
      "Epoch 326/500\n",
      "79/79 [==============================] - 0s 806us/step - loss: 49.2753 - val_loss: 132.9713\n",
      "Epoch 327/500\n",
      "79/79 [==============================] - 0s 803us/step - loss: 35.8615 - val_loss: 62.4226\n",
      "Epoch 328/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 24.5289 - val_loss: 96.4864\n",
      "Epoch 329/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 28.2300 - val_loss: 63.8047\n",
      "Epoch 330/500\n",
      "79/79 [==============================] - 0s 862us/step - loss: 15.9837 - val_loss: 63.6407\n",
      "Epoch 331/500\n",
      "79/79 [==============================] - 0s 794us/step - loss: 21.1169 - val_loss: 94.6184\n",
      "Epoch 332/500\n",
      "79/79 [==============================] - 0s 824us/step - loss: 25.6100 - val_loss: 49.1382\n",
      "Epoch 333/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 24.8041 - val_loss: 69.9500\n",
      "Epoch 334/500\n",
      "79/79 [==============================] - 0s 848us/step - loss: 25.4624 - val_loss: 64.7424\n",
      "Epoch 335/500\n",
      "79/79 [==============================] - 0s 871us/step - loss: 51.0675 - val_loss: 52.8147\n",
      "Epoch 336/500\n",
      "79/79 [==============================] - 0s 829us/step - loss: 28.5405 - val_loss: 27.9933\n",
      "Epoch 337/500\n",
      "79/79 [==============================] - 0s 916us/step - loss: 24.4816 - val_loss: 46.3340\n",
      "Epoch 338/500\n",
      "79/79 [==============================] - 0s 879us/step - loss: 22.8563 - val_loss: 62.4636\n",
      "Epoch 339/500\n",
      "79/79 [==============================] - 0s 854us/step - loss: 35.1357 - val_loss: 67.9281\n",
      "Epoch 340/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 41.6202 - val_loss: 87.2217\n",
      "Epoch 341/500\n",
      "79/79 [==============================] - 0s 850us/step - loss: 37.3399 - val_loss: 39.5448\n",
      "Epoch 342/500\n",
      "79/79 [==============================] - 0s 835us/step - loss: 20.9901 - val_loss: 81.0740\n",
      "Epoch 343/500\n",
      "79/79 [==============================] - 0s 845us/step - loss: 21.6938 - val_loss: 66.3449\n",
      "Epoch 344/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 26.5799 - val_loss: 54.7544\n",
      "Epoch 345/500\n",
      "79/79 [==============================] - 0s 817us/step - loss: 60.2020 - val_loss: 121.3536\n",
      "Epoch 346/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 40.9690 - val_loss: 48.8830\n",
      "Epoch 347/500\n",
      "79/79 [==============================] - 0s 819us/step - loss: 47.6867 - val_loss: 47.7478\n",
      "Epoch 348/500\n",
      "79/79 [==============================] - 0s 803us/step - loss: 37.4086 - val_loss: 67.9346\n",
      "Epoch 349/500\n",
      "79/79 [==============================] - 0s 861us/step - loss: 47.3635 - val_loss: 64.5198\n",
      "Epoch 350/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 68.1848 - val_loss: 146.6279\n",
      "Epoch 351/500\n",
      "79/79 [==============================] - 0s 850us/step - loss: 83.0312 - val_loss: 51.5318\n",
      "Epoch 352/500\n",
      "79/79 [==============================] - 0s 812us/step - loss: 45.6273 - val_loss: 38.2523\n",
      "Epoch 353/500\n",
      "79/79 [==============================] - 0s 791us/step - loss: 50.2712 - val_loss: 57.2697\n",
      "Epoch 354/500\n",
      "79/79 [==============================] - 0s 844us/step - loss: 45.1454 - val_loss: 33.4411\n",
      "Epoch 355/500\n",
      "79/79 [==============================] - 0s 807us/step - loss: 24.6057 - val_loss: 80.1477\n",
      "Epoch 356/500\n",
      "79/79 [==============================] - 0s 818us/step - loss: 35.0807 - val_loss: 30.5667\n",
      "Epoch 357/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 27.4613 - val_loss: 27.3808\n",
      "Epoch 358/500\n",
      "79/79 [==============================] - 0s 800us/step - loss: 30.1247 - val_loss: 94.8527\n",
      "Epoch 359/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 44.9848 - val_loss: 31.6553\n",
      "Epoch 360/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 27.7964 - val_loss: 36.6057\n",
      "Epoch 361/500\n",
      "79/79 [==============================] - 0s 835us/step - loss: 18.4254 - val_loss: 41.7433\n",
      "Epoch 362/500\n",
      "79/79 [==============================] - 0s 824us/step - loss: 20.5440 - val_loss: 99.0886\n",
      "Epoch 363/500\n",
      "79/79 [==============================] - 0s 846us/step - loss: 35.0645 - val_loss: 68.8470\n",
      "Epoch 364/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 28.5102 - val_loss: 62.4660\n",
      "Epoch 365/500\n",
      "79/79 [==============================] - 0s 842us/step - loss: 51.0336 - val_loss: 68.2300\n",
      "Epoch 366/500\n",
      "79/79 [==============================] - 0s 897us/step - loss: 45.0376 - val_loss: 51.6871\n",
      "Epoch 367/500\n",
      "79/79 [==============================] - 0s 812us/step - loss: 39.1391 - val_loss: 80.9635\n",
      "Epoch 368/500\n",
      "79/79 [==============================] - 0s 848us/step - loss: 38.0825 - val_loss: 35.6079\n",
      "Epoch 369/500\n",
      "79/79 [==============================] - 0s 874us/step - loss: 25.0671 - val_loss: 88.5458\n",
      "Epoch 370/500\n",
      "79/79 [==============================] - 0s 761us/step - loss: 26.4147 - val_loss: 87.2319\n",
      "Epoch 371/500\n",
      "79/79 [==============================] - 0s 647us/step - loss: 53.2947 - val_loss: 68.5517\n",
      "Epoch 372/500\n",
      "79/79 [==============================] - 0s 825us/step - loss: 45.0772 - val_loss: 75.2769\n",
      "Epoch 373/500\n",
      "79/79 [==============================] - 0s 863us/step - loss: 46.2004 - val_loss: 37.0096\n",
      "Epoch 374/500\n",
      "79/79 [==============================] - 0s 824us/step - loss: 29.8790 - val_loss: 67.4722\n",
      "Epoch 375/500\n",
      "79/79 [==============================] - 0s 857us/step - loss: 27.3100 - val_loss: 29.7525\n",
      "Epoch 376/500\n",
      "79/79 [==============================] - 0s 829us/step - loss: 23.1582 - val_loss: 22.6396\n",
      "Epoch 377/500\n",
      "79/79 [==============================] - 0s 844us/step - loss: 11.2687 - val_loss: 37.6027\n",
      "Epoch 378/500\n",
      "79/79 [==============================] - 0s 872us/step - loss: 12.7993 - val_loss: 25.5408\n",
      "Epoch 379/500\n",
      "79/79 [==============================] - 0s 844us/step - loss: 11.0974 - val_loss: 26.9843\n",
      "Epoch 380/500\n",
      "79/79 [==============================] - 0s 867us/step - loss: 10.1275 - val_loss: 47.0347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/500\n",
      "79/79 [==============================] - 0s 892us/step - loss: 13.1130 - val_loss: 27.8586\n",
      "Epoch 382/500\n",
      "79/79 [==============================] - 0s 988us/step - loss: 11.9484 - val_loss: 20.8093\n",
      "Epoch 383/500\n",
      "79/79 [==============================] - 0s 804us/step - loss: 12.2800 - val_loss: 27.0278\n",
      "Epoch 384/500\n",
      "79/79 [==============================] - 0s 855us/step - loss: 11.0331 - val_loss: 30.2773\n",
      "Epoch 385/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 9.3283 - val_loss: 29.9609\n",
      "Epoch 386/500\n",
      "79/79 [==============================] - 0s 828us/step - loss: 7.2279 - val_loss: 31.0202\n",
      "Epoch 387/500\n",
      "79/79 [==============================] - 0s 813us/step - loss: 7.0434 - val_loss: 24.5169\n",
      "Epoch 388/500\n",
      "79/79 [==============================] - 0s 815us/step - loss: 14.7678 - val_loss: 47.2424\n",
      "Epoch 389/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 14.4599 - val_loss: 29.2186\n",
      "Epoch 390/500\n",
      "79/79 [==============================] - 0s 868us/step - loss: 15.7847 - val_loss: 33.4082\n",
      "Epoch 391/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 13.2894 - val_loss: 31.9348\n",
      "Epoch 392/500\n",
      "79/79 [==============================] - 0s 862us/step - loss: 12.4813 - val_loss: 39.3288\n",
      "Epoch 393/500\n",
      "79/79 [==============================] - 0s 836us/step - loss: 15.2329 - val_loss: 37.6426\n",
      "Epoch 394/500\n",
      "79/79 [==============================] - 0s 871us/step - loss: 14.6335 - val_loss: 23.9492\n",
      "Epoch 395/500\n",
      "79/79 [==============================] - 0s 811us/step - loss: 9.9381 - val_loss: 21.1398\n",
      "Epoch 396/500\n",
      "79/79 [==============================] - 0s 869us/step - loss: 7.5762 - val_loss: 26.4485\n",
      "Epoch 397/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 14.6786 - val_loss: 21.5479\n",
      "Epoch 398/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 6.3504 - val_loss: 20.3720\n",
      "Epoch 399/500\n",
      "79/79 [==============================] - 0s 857us/step - loss: 6.2078 - val_loss: 25.8640\n",
      "Epoch 400/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 6.9501 - val_loss: 25.6220\n",
      "Epoch 401/500\n",
      "79/79 [==============================] - 0s 840us/step - loss: 4.2286 - val_loss: 20.4132\n",
      "Epoch 402/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 3.2086 - val_loss: 22.6538\n",
      "Epoch 403/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 4.8852 - val_loss: 21.0830\n",
      "Epoch 404/500\n",
      "79/79 [==============================] - 0s 815us/step - loss: 4.8697 - val_loss: 22.3914\n",
      "Epoch 405/500\n",
      "79/79 [==============================] - 0s 880us/step - loss: 2.9225 - val_loss: 19.0009\n",
      "Epoch 406/500\n",
      "79/79 [==============================] - 0s 862us/step - loss: 3.9964 - val_loss: 22.5072\n",
      "Epoch 407/500\n",
      "79/79 [==============================] - 0s 902us/step - loss: 2.2448 - val_loss: 25.5180\n",
      "Epoch 408/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 2.6097 - val_loss: 21.3416\n",
      "Epoch 409/500\n",
      "79/79 [==============================] - 0s 876us/step - loss: 3.0464 - val_loss: 18.5915\n",
      "Epoch 410/500\n",
      "79/79 [==============================] - 0s 777us/step - loss: 2.0403 - val_loss: 17.3178\n",
      "Epoch 411/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 1.9656 - val_loss: 18.0445\n",
      "Epoch 412/500\n",
      "79/79 [==============================] - 0s 879us/step - loss: 2.0495 - val_loss: 19.2420\n",
      "Epoch 413/500\n",
      "79/79 [==============================] - 0s 867us/step - loss: 2.6755 - val_loss: 19.3786\n",
      "Epoch 414/500\n",
      "79/79 [==============================] - 0s 837us/step - loss: 1.5858 - val_loss: 19.6017\n",
      "Epoch 415/500\n",
      "79/79 [==============================] - 0s 873us/step - loss: 1.6021 - val_loss: 19.1502\n",
      "Epoch 416/500\n",
      "79/79 [==============================] - 0s 886us/step - loss: 2.2006 - val_loss: 18.8881\n",
      "Epoch 417/500\n",
      "79/79 [==============================] - 0s 829us/step - loss: 1.8496 - val_loss: 19.2986\n",
      "Epoch 418/500\n",
      "79/79 [==============================] - 0s 843us/step - loss: 1.8700 - val_loss: 19.9579\n",
      "Epoch 419/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 1.9645 - val_loss: 17.9219\n",
      "Epoch 420/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 1.4325 - val_loss: 17.7785\n",
      "Epoch 421/500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 1.8718 - val_loss: 19.3554\n",
      "Epoch 422/500\n",
      "79/79 [==============================] - 0s 824us/step - loss: 1.9521 - val_loss: 21.9314\n",
      "Epoch 423/500\n",
      "79/79 [==============================] - 0s 846us/step - loss: 1.6498 - val_loss: 20.4223\n",
      "Epoch 424/500\n",
      "79/79 [==============================] - 0s 867us/step - loss: 1.5540 - val_loss: 19.1488\n",
      "Epoch 425/500\n",
      "79/79 [==============================] - 0s 693us/step - loss: 2.1719 - val_loss: 21.0362\n",
      "Epoch 426/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 2.2374 - val_loss: 19.8962\n",
      "Epoch 427/500\n",
      "79/79 [==============================] - 0s 854us/step - loss: 1.8727 - val_loss: 18.5449\n",
      "Epoch 428/500\n",
      "79/79 [==============================] - 0s 865us/step - loss: 1.8185 - val_loss: 20.0590\n",
      "Epoch 429/500\n",
      "79/79 [==============================] - 0s 816us/step - loss: 2.3032 - val_loss: 19.6713\n",
      "Epoch 430/500\n",
      "79/79 [==============================] - 0s 848us/step - loss: 2.1754 - val_loss: 19.9960\n",
      "Epoch 431/500\n",
      "79/79 [==============================] - 0s 820us/step - loss: 1.8916 - val_loss: 19.4035\n",
      "Epoch 432/500\n",
      "79/79 [==============================] - 0s 842us/step - loss: 1.4427 - val_loss: 20.7188\n",
      "Epoch 433/500\n",
      "79/79 [==============================] - 0s 865us/step - loss: 1.4138 - val_loss: 19.3772\n",
      "Epoch 434/500\n",
      "79/79 [==============================] - 0s 864us/step - loss: 1.6832 - val_loss: 18.7858\n",
      "Epoch 435/500\n",
      "79/79 [==============================] - 0s 820us/step - loss: 1.8695 - val_loss: 18.9655\n",
      "Epoch 436/500\n",
      "79/79 [==============================] - 0s 835us/step - loss: 1.0541 - val_loss: 18.6130\n",
      "Epoch 437/500\n",
      "79/79 [==============================] - 0s 823us/step - loss: 1.5204 - val_loss: 20.7978\n",
      "Epoch 438/500\n",
      "79/79 [==============================] - 0s 839us/step - loss: 1.4500 - val_loss: 21.4972\n",
      "Epoch 439/500\n",
      "79/79 [==============================] - 0s 827us/step - loss: 1.1784 - val_loss: 18.5627\n",
      "Epoch 440/500\n",
      "79/79 [==============================] - 0s 866us/step - loss: 1.4319 - val_loss: 18.9906\n",
      "Epoch 441/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 1.4118 - val_loss: 19.2752\n",
      "Epoch 442/500\n",
      "79/79 [==============================] - 0s 860us/step - loss: 1.1401 - val_loss: 20.1371\n",
      "Epoch 443/500\n",
      "79/79 [==============================] - 0s 883us/step - loss: 1.0747 - val_loss: 20.1211\n",
      "Epoch 444/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 1.4766 - val_loss: 20.1233\n",
      "Epoch 445/500\n",
      "79/79 [==============================] - 0s 743us/step - loss: 2.2026 - val_loss: 19.2518\n",
      "Epoch 446/500\n",
      "79/79 [==============================] - 0s 861us/step - loss: 2.8624 - val_loss: 19.9262\n",
      "Epoch 447/500\n",
      "79/79 [==============================] - 0s 838us/step - loss: 3.8411 - val_loss: 21.9319\n",
      "Epoch 448/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 4.8687 - val_loss: 19.5271\n",
      "Epoch 449/500\n",
      "79/79 [==============================] - 0s 846us/step - loss: 4.9820 - val_loss: 20.7058\n",
      "Epoch 450/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 3.3826 - val_loss: 16.9102\n",
      "Epoch 451/500\n",
      "79/79 [==============================] - 0s 829us/step - loss: 3.0283 - val_loss: 21.7964\n",
      "Epoch 452/500\n",
      "79/79 [==============================] - 0s 824us/step - loss: 3.2055 - val_loss: 32.6004\n",
      "Epoch 453/500\n",
      "79/79 [==============================] - 0s 851us/step - loss: 2.5388 - val_loss: 30.4049\n",
      "Epoch 454/500\n",
      "79/79 [==============================] - 0s 793us/step - loss: 3.6232 - val_loss: 23.9041\n",
      "Epoch 455/500\n",
      "79/79 [==============================] - 0s 809us/step - loss: 3.2521 - val_loss: 23.3148\n",
      "Epoch 456/500\n",
      "79/79 [==============================] - 0s 805us/step - loss: 3.8100 - val_loss: 22.6512\n",
      "Epoch 457/500\n",
      "79/79 [==============================] - 0s 872us/step - loss: 1.9529 - val_loss: 21.5894\n",
      "Epoch 458/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 1.4219 - val_loss: 23.6079\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 864us/step - loss: 1.5795 - val_loss: 23.1652\n",
      "Epoch 460/500\n",
      "79/79 [==============================] - 0s 872us/step - loss: 1.4969 - val_loss: 22.5331\n",
      "Epoch 461/500\n",
      "79/79 [==============================] - 0s 859us/step - loss: 1.3300 - val_loss: 22.7641\n",
      "Epoch 462/500\n",
      "79/79 [==============================] - 0s 843us/step - loss: 1.5728 - val_loss: 20.9847\n",
      "Epoch 463/500\n",
      "79/79 [==============================] - 0s 780us/step - loss: 1.8308 - val_loss: 19.4749\n",
      "Epoch 464/500\n",
      "79/79 [==============================] - 0s 848us/step - loss: 1.8283 - val_loss: 19.2723\n",
      "Epoch 465/500\n",
      "79/79 [==============================] - 0s 839us/step - loss: 1.4228 - val_loss: 24.3073\n",
      "Epoch 466/500\n",
      "79/79 [==============================] - 0s 800us/step - loss: 2.5416 - val_loss: 21.8544\n",
      "Epoch 467/500\n",
      "79/79 [==============================] - 0s 837us/step - loss: 1.5792 - val_loss: 17.9031\n",
      "Epoch 468/500\n",
      "79/79 [==============================] - 0s 847us/step - loss: 1.6093 - val_loss: 17.5746\n",
      "Epoch 469/500\n",
      "79/79 [==============================] - 0s 822us/step - loss: 1.2518 - val_loss: 20.5146\n",
      "Epoch 470/500\n",
      "79/79 [==============================] - 0s 788us/step - loss: 1.8791 - val_loss: 21.1305\n",
      "Epoch 471/500\n",
      "79/79 [==============================] - 0s 817us/step - loss: 1.6242 - val_loss: 18.7419\n",
      "Epoch 472/500\n",
      "79/79 [==============================] - 0s 861us/step - loss: 1.3547 - val_loss: 20.2264\n",
      "Epoch 473/500\n",
      "79/79 [==============================] - 0s 871us/step - loss: 2.2428 - val_loss: 19.5729\n",
      "Epoch 474/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.9754 - val_loss: 20.9610\n",
      "Epoch 475/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.2595 - val_loss: 22.4229\n",
      "Epoch 476/500\n",
      "79/79 [==============================] - 0s 834us/step - loss: 1.1038 - val_loss: 21.0149\n",
      "Epoch 477/500\n",
      "79/79 [==============================] - 0s 902us/step - loss: 1.0843 - val_loss: 20.4358\n",
      "Epoch 478/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.9882 - val_loss: 21.4613\n",
      "Epoch 479/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 2.6098 - val_loss: 24.9482\n",
      "Epoch 480/500\n",
      "79/79 [==============================] - 0s 832us/step - loss: 3.1511 - val_loss: 21.5984\n",
      "Epoch 481/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.9160 - val_loss: 20.6018\n",
      "Epoch 482/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.5750 - val_loss: 21.6090\n",
      "Epoch 483/500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 1.4298 - val_loss: 22.0162\n",
      "Epoch 484/500\n",
      "79/79 [==============================] - 0s 816us/step - loss: 1.2297 - val_loss: 21.3042\n",
      "Epoch 485/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.9253 - val_loss: 22.1315\n",
      "Epoch 486/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.1866 - val_loss: 20.0364\n",
      "Epoch 487/500\n",
      "79/79 [==============================] - 0s 810us/step - loss: 0.8889 - val_loss: 20.1047\n",
      "Epoch 488/500\n",
      "79/79 [==============================] - 0s 900us/step - loss: 0.9662 - val_loss: 20.8889\n",
      "Epoch 489/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.9313 - val_loss: 21.6395\n",
      "Epoch 490/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.0250 - val_loss: 25.0364\n",
      "Epoch 491/500\n",
      "79/79 [==============================] - 0s 833us/step - loss: 1.5574 - val_loss: 21.3953\n",
      "Epoch 492/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.2797 - val_loss: 19.2512\n",
      "Epoch 493/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 2.6917 - val_loss: 28.0693\n",
      "Epoch 494/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 2.3724 - val_loss: 25.9593\n",
      "Epoch 495/500\n",
      "79/79 [==============================] - 0s 794us/step - loss: 2.5107 - val_loss: 26.1518\n",
      "Epoch 496/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 2.4276 - val_loss: 23.4943\n",
      "Epoch 497/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.9905 - val_loss: 20.2589\n",
      "Epoch 498/500\n",
      "79/79 [==============================] - 0s 880us/step - loss: 2.3759 - val_loss: 18.0264\n",
      "Epoch 499/500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 1.7540 - val_loss: 18.9196\n",
      "Epoch 500/500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 1.9290 - val_loss: 19.7499\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=500,validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPKizNRLWFPs"
   },
   "outputs": [],
   "source": [
    "results = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MJ36b2CcmjXY",
    "outputId": "0b6598ae-2c8c-49d3-ca97-dd300b309525"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ezulvCPMmjU-",
    "outputId": "0a05d811-1c47-43d5-8408-d7a1f79c39e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "0pwrsM6UmpMa",
    "outputId": "bd547e13-58ba-4052-876b-515998fc29fa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(20),results,c='r')\n",
    "plt.scatter(range(20),y_test,c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
