{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_seq414.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_IzHGQlLj6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, LSTM, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RufOo3wpLo_G",
        "colab_type": "code",
        "outputId": "27c53f21-26b7-41ce-935d-f58ac20f8f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "X = [[[(10**i)+j] for i in range (5)] for j in range(100)]\n",
        "print (X)\n",
        "Y = [(24+i) for i in range(100)]\n",
        "print (Y)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1], [10], [100], [1000], [10000]], [[2], [11], [101], [1001], [10001]], [[3], [12], [102], [1002], [10002]], [[4], [13], [103], [1003], [10003]], [[5], [14], [104], [1004], [10004]], [[6], [15], [105], [1005], [10005]], [[7], [16], [106], [1006], [10006]], [[8], [17], [107], [1007], [10007]], [[9], [18], [108], [1008], [10008]], [[10], [19], [109], [1009], [10009]], [[11], [20], [110], [1010], [10010]], [[12], [21], [111], [1011], [10011]], [[13], [22], [112], [1012], [10012]], [[14], [23], [113], [1013], [10013]], [[15], [24], [114], [1014], [10014]], [[16], [25], [115], [1015], [10015]], [[17], [26], [116], [1016], [10016]], [[18], [27], [117], [1017], [10017]], [[19], [28], [118], [1018], [10018]], [[20], [29], [119], [1019], [10019]], [[21], [30], [120], [1020], [10020]], [[22], [31], [121], [1021], [10021]], [[23], [32], [122], [1022], [10022]], [[24], [33], [123], [1023], [10023]], [[25], [34], [124], [1024], [10024]], [[26], [35], [125], [1025], [10025]], [[27], [36], [126], [1026], [10026]], [[28], [37], [127], [1027], [10027]], [[29], [38], [128], [1028], [10028]], [[30], [39], [129], [1029], [10029]], [[31], [40], [130], [1030], [10030]], [[32], [41], [131], [1031], [10031]], [[33], [42], [132], [1032], [10032]], [[34], [43], [133], [1033], [10033]], [[35], [44], [134], [1034], [10034]], [[36], [45], [135], [1035], [10035]], [[37], [46], [136], [1036], [10036]], [[38], [47], [137], [1037], [10037]], [[39], [48], [138], [1038], [10038]], [[40], [49], [139], [1039], [10039]], [[41], [50], [140], [1040], [10040]], [[42], [51], [141], [1041], [10041]], [[43], [52], [142], [1042], [10042]], [[44], [53], [143], [1043], [10043]], [[45], [54], [144], [1044], [10044]], [[46], [55], [145], [1045], [10045]], [[47], [56], [146], [1046], [10046]], [[48], [57], [147], [1047], [10047]], [[49], [58], [148], [1048], [10048]], [[50], [59], [149], [1049], [10049]], [[51], [60], [150], [1050], [10050]], [[52], [61], [151], [1051], [10051]], [[53], [62], [152], [1052], [10052]], [[54], [63], [153], [1053], [10053]], [[55], [64], [154], [1054], [10054]], [[56], [65], [155], [1055], [10055]], [[57], [66], [156], [1056], [10056]], [[58], [67], [157], [1057], [10057]], [[59], [68], [158], [1058], [10058]], [[60], [69], [159], [1059], [10059]], [[61], [70], [160], [1060], [10060]], [[62], [71], [161], [1061], [10061]], [[63], [72], [162], [1062], [10062]], [[64], [73], [163], [1063], [10063]], [[65], [74], [164], [1064], [10064]], [[66], [75], [165], [1065], [10065]], [[67], [76], [166], [1066], [10066]], [[68], [77], [167], [1067], [10067]], [[69], [78], [168], [1068], [10068]], [[70], [79], [169], [1069], [10069]], [[71], [80], [170], [1070], [10070]], [[72], [81], [171], [1071], [10071]], [[73], [82], [172], [1072], [10072]], [[74], [83], [173], [1073], [10073]], [[75], [84], [174], [1074], [10074]], [[76], [85], [175], [1075], [10075]], [[77], [86], [176], [1076], [10076]], [[78], [87], [177], [1077], [10077]], [[79], [88], [178], [1078], [10078]], [[80], [89], [179], [1079], [10079]], [[81], [90], [180], [1080], [10080]], [[82], [91], [181], [1081], [10081]], [[83], [92], [182], [1082], [10082]], [[84], [93], [183], [1083], [10083]], [[85], [94], [184], [1084], [10084]], [[86], [95], [185], [1085], [10085]], [[87], [96], [186], [1086], [10086]], [[88], [97], [187], [1087], [10087]], [[89], [98], [188], [1088], [10088]], [[90], [99], [189], [1089], [10089]], [[91], [100], [190], [1090], [10090]], [[92], [101], [191], [1091], [10091]], [[93], [102], [192], [1092], [10092]], [[94], [103], [193], [1093], [10093]], [[95], [104], [194], [1094], [10094]], [[96], [105], [195], [1095], [10095]], [[97], [106], [196], [1096], [10096]], [[98], [107], [197], [1097], [10097]], [[99], [108], [198], [1098], [10098]], [[100], [109], [199], [1099], [10099]]]\n",
            "[24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VLbQqMxLuoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(X, dtype=\"float32\")\n",
        "Y = np.array(Y, dtype=\"float32\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj5AUk6QLxuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X /= 500\n",
        "Y /= 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGdAKUnnL2GY",
        "colab_type": "code",
        "outputId": "cfbec353-4a0b-4d7e-9d7b-9f064a792f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NqttB53L4uy",
        "colab_type": "code",
        "outputId": "66d8ff57-968c-4404-8252-ee5f6f391039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP4L-9mwL8Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, \n",
        "                                                random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_xYT5ZbMB9W",
        "colab_type": "code",
        "outputId": "4c4498a3-4d47-40c7-dc77-6393bac11e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_train\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.9000e-01],\n",
              "        [2.0800e-01],\n",
              "        [3.8800e-01],\n",
              "        [2.1880e+00],\n",
              "        [2.0188e+01]],\n",
              "\n",
              "       [[1.1400e-01],\n",
              "        [1.3200e-01],\n",
              "        [3.1200e-01],\n",
              "        [2.1120e+00],\n",
              "        [2.0112e+01]],\n",
              "\n",
              "       [[4.6000e-02],\n",
              "        [6.4000e-02],\n",
              "        [2.4400e-01],\n",
              "        [2.0440e+00],\n",
              "        [2.0044e+01]],\n",
              "\n",
              "       [[8.0000e-02],\n",
              "        [9.8000e-02],\n",
              "        [2.7800e-01],\n",
              "        [2.0780e+00],\n",
              "        [2.0078e+01]],\n",
              "\n",
              "       [[5.0000e-02],\n",
              "        [6.8000e-02],\n",
              "        [2.4800e-01],\n",
              "        [2.0480e+00],\n",
              "        [2.0048e+01]],\n",
              "\n",
              "       [[2.8000e-02],\n",
              "        [4.6000e-02],\n",
              "        [2.2600e-01],\n",
              "        [2.0260e+00],\n",
              "        [2.0026e+01]],\n",
              "\n",
              "       [[1.2800e-01],\n",
              "        [1.4600e-01],\n",
              "        [3.2600e-01],\n",
              "        [2.1260e+00],\n",
              "        [2.0126e+01]],\n",
              "\n",
              "       [[1.4400e-01],\n",
              "        [1.6200e-01],\n",
              "        [3.4200e-01],\n",
              "        [2.1420e+00],\n",
              "        [2.0142e+01]],\n",
              "\n",
              "       [[1.1200e-01],\n",
              "        [1.3000e-01],\n",
              "        [3.1000e-01],\n",
              "        [2.1100e+00],\n",
              "        [2.0110e+01]],\n",
              "\n",
              "       [[1.7600e-01],\n",
              "        [1.9400e-01],\n",
              "        [3.7400e-01],\n",
              "        [2.1740e+00],\n",
              "        [2.0174e+01]],\n",
              "\n",
              "       [[1.4000e-02],\n",
              "        [3.2000e-02],\n",
              "        [2.1200e-01],\n",
              "        [2.0120e+00],\n",
              "        [2.0012e+01]],\n",
              "\n",
              "       [[1.7800e-01],\n",
              "        [1.9600e-01],\n",
              "        [3.7600e-01],\n",
              "        [2.1760e+00],\n",
              "        [2.0176e+01]],\n",
              "\n",
              "       [[1.3000e-01],\n",
              "        [1.4800e-01],\n",
              "        [3.2800e-01],\n",
              "        [2.1280e+00],\n",
              "        [2.0128e+01]],\n",
              "\n",
              "       [[5.4000e-02],\n",
              "        [7.2000e-02],\n",
              "        [2.5200e-01],\n",
              "        [2.0520e+00],\n",
              "        [2.0052e+01]],\n",
              "\n",
              "       [[9.8000e-02],\n",
              "        [1.1600e-01],\n",
              "        [2.9600e-01],\n",
              "        [2.0960e+00],\n",
              "        [2.0096e+01]],\n",
              "\n",
              "       [[1.0200e-01],\n",
              "        [1.2000e-01],\n",
              "        [3.0000e-01],\n",
              "        [2.1000e+00],\n",
              "        [2.0100e+01]],\n",
              "\n",
              "       [[1.4600e-01],\n",
              "        [1.6400e-01],\n",
              "        [3.4400e-01],\n",
              "        [2.1440e+00],\n",
              "        [2.0144e+01]],\n",
              "\n",
              "       [[1.1000e-01],\n",
              "        [1.2800e-01],\n",
              "        [3.0800e-01],\n",
              "        [2.1080e+00],\n",
              "        [2.0108e+01]],\n",
              "\n",
              "       [[4.4000e-02],\n",
              "        [6.2000e-02],\n",
              "        [2.4200e-01],\n",
              "        [2.0420e+00],\n",
              "        [2.0042e+01]],\n",
              "\n",
              "       [[5.2000e-02],\n",
              "        [7.0000e-02],\n",
              "        [2.5000e-01],\n",
              "        [2.0500e+00],\n",
              "        [2.0050e+01]],\n",
              "\n",
              "       [[6.8000e-02],\n",
              "        [8.6000e-02],\n",
              "        [2.6600e-01],\n",
              "        [2.0660e+00],\n",
              "        [2.0066e+01]],\n",
              "\n",
              "       [[6.0000e-02],\n",
              "        [7.8000e-02],\n",
              "        [2.5800e-01],\n",
              "        [2.0580e+00],\n",
              "        [2.0058e+01]],\n",
              "\n",
              "       [[1.0600e-01],\n",
              "        [1.2400e-01],\n",
              "        [3.0400e-01],\n",
              "        [2.1040e+00],\n",
              "        [2.0104e+01]],\n",
              "\n",
              "       [[3.0000e-02],\n",
              "        [4.8000e-02],\n",
              "        [2.2800e-01],\n",
              "        [2.0280e+00],\n",
              "        [2.0028e+01]],\n",
              "\n",
              "       [[1.7200e-01],\n",
              "        [1.9000e-01],\n",
              "        [3.7000e-01],\n",
              "        [2.1700e+00],\n",
              "        [2.0170e+01]],\n",
              "\n",
              "       [[1.6000e-01],\n",
              "        [1.7800e-01],\n",
              "        [3.5800e-01],\n",
              "        [2.1580e+00],\n",
              "        [2.0158e+01]],\n",
              "\n",
              "       [[1.9200e-01],\n",
              "        [2.1000e-01],\n",
              "        [3.9000e-01],\n",
              "        [2.1900e+00],\n",
              "        [2.0190e+01]],\n",
              "\n",
              "       [[8.0000e-03],\n",
              "        [2.6000e-02],\n",
              "        [2.0600e-01],\n",
              "        [2.0060e+00],\n",
              "        [2.0006e+01]],\n",
              "\n",
              "       [[8.8000e-02],\n",
              "        [1.0600e-01],\n",
              "        [2.8600e-01],\n",
              "        [2.0860e+00],\n",
              "        [2.0086e+01]],\n",
              "\n",
              "       [[2.4000e-02],\n",
              "        [4.2000e-02],\n",
              "        [2.2200e-01],\n",
              "        [2.0220e+00],\n",
              "        [2.0022e+01]],\n",
              "\n",
              "       [[1.9800e-01],\n",
              "        [2.1600e-01],\n",
              "        [3.9600e-01],\n",
              "        [2.1960e+00],\n",
              "        [2.0196e+01]],\n",
              "\n",
              "       [[9.2000e-02],\n",
              "        [1.1000e-01],\n",
              "        [2.9000e-01],\n",
              "        [2.0900e+00],\n",
              "        [2.0090e+01]],\n",
              "\n",
              "       [[1.6800e-01],\n",
              "        [1.8600e-01],\n",
              "        [3.6600e-01],\n",
              "        [2.1660e+00],\n",
              "        [2.0166e+01]],\n",
              "\n",
              "       [[1.3600e-01],\n",
              "        [1.5400e-01],\n",
              "        [3.3400e-01],\n",
              "        [2.1340e+00],\n",
              "        [2.0134e+01]],\n",
              "\n",
              "       [[1.8000e-01],\n",
              "        [1.9800e-01],\n",
              "        [3.7800e-01],\n",
              "        [2.1780e+00],\n",
              "        [2.0178e+01]],\n",
              "\n",
              "       [[1.7400e-01],\n",
              "        [1.9200e-01],\n",
              "        [3.7200e-01],\n",
              "        [2.1720e+00],\n",
              "        [2.0172e+01]],\n",
              "\n",
              "       [[1.0000e-01],\n",
              "        [1.1800e-01],\n",
              "        [2.9800e-01],\n",
              "        [2.0980e+00],\n",
              "        [2.0098e+01]],\n",
              "\n",
              "       [[1.6600e-01],\n",
              "        [1.8400e-01],\n",
              "        [3.6400e-01],\n",
              "        [2.1640e+00],\n",
              "        [2.0164e+01]],\n",
              "\n",
              "       [[1.2000e-01],\n",
              "        [1.3800e-01],\n",
              "        [3.1800e-01],\n",
              "        [2.1180e+00],\n",
              "        [2.0118e+01]],\n",
              "\n",
              "       [[4.0000e-02],\n",
              "        [5.8000e-02],\n",
              "        [2.3800e-01],\n",
              "        [2.0380e+00],\n",
              "        [2.0038e+01]],\n",
              "\n",
              "       [[1.6400e-01],\n",
              "        [1.8200e-01],\n",
              "        [3.6200e-01],\n",
              "        [2.1620e+00],\n",
              "        [2.0162e+01]],\n",
              "\n",
              "       [[7.8000e-02],\n",
              "        [9.6000e-02],\n",
              "        [2.7600e-01],\n",
              "        [2.0760e+00],\n",
              "        [2.0076e+01]],\n",
              "\n",
              "       [[1.3800e-01],\n",
              "        [1.5600e-01],\n",
              "        [3.3600e-01],\n",
              "        [2.1360e+00],\n",
              "        [2.0136e+01]],\n",
              "\n",
              "       [[6.0000e-03],\n",
              "        [2.4000e-02],\n",
              "        [2.0400e-01],\n",
              "        [2.0040e+00],\n",
              "        [2.0004e+01]],\n",
              "\n",
              "       [[6.4000e-02],\n",
              "        [8.2000e-02],\n",
              "        [2.6200e-01],\n",
              "        [2.0620e+00],\n",
              "        [2.0062e+01]],\n",
              "\n",
              "       [[1.7000e-01],\n",
              "        [1.8800e-01],\n",
              "        [3.6800e-01],\n",
              "        [2.1680e+00],\n",
              "        [2.0168e+01]],\n",
              "\n",
              "       [[1.0400e-01],\n",
              "        [1.2200e-01],\n",
              "        [3.0200e-01],\n",
              "        [2.1020e+00],\n",
              "        [2.0102e+01]],\n",
              "\n",
              "       [[7.4000e-02],\n",
              "        [9.2000e-02],\n",
              "        [2.7200e-01],\n",
              "        [2.0720e+00],\n",
              "        [2.0072e+01]],\n",
              "\n",
              "       [[1.0000e-02],\n",
              "        [2.8000e-02],\n",
              "        [2.0800e-01],\n",
              "        [2.0080e+00],\n",
              "        [2.0008e+01]],\n",
              "\n",
              "       [[2.0000e-03],\n",
              "        [2.0000e-02],\n",
              "        [2.0000e-01],\n",
              "        [2.0000e+00],\n",
              "        [2.0000e+01]],\n",
              "\n",
              "       [[1.1800e-01],\n",
              "        [1.3600e-01],\n",
              "        [3.1600e-01],\n",
              "        [2.1160e+00],\n",
              "        [2.0116e+01]],\n",
              "\n",
              "       [[1.2000e-02],\n",
              "        [3.0000e-02],\n",
              "        [2.1000e-01],\n",
              "        [2.0100e+00],\n",
              "        [2.0010e+01]],\n",
              "\n",
              "       [[1.9400e-01],\n",
              "        [2.1200e-01],\n",
              "        [3.9200e-01],\n",
              "        [2.1920e+00],\n",
              "        [2.0192e+01]],\n",
              "\n",
              "       [[4.0000e-03],\n",
              "        [2.2000e-02],\n",
              "        [2.0200e-01],\n",
              "        [2.0020e+00],\n",
              "        [2.0002e+01]],\n",
              "\n",
              "       [[1.8800e-01],\n",
              "        [2.0600e-01],\n",
              "        [3.8600e-01],\n",
              "        [2.1860e+00],\n",
              "        [2.0186e+01]],\n",
              "\n",
              "       [[8.4000e-02],\n",
              "        [1.0200e-01],\n",
              "        [2.8200e-01],\n",
              "        [2.0820e+00],\n",
              "        [2.0082e+01]],\n",
              "\n",
              "       [[2.0000e-02],\n",
              "        [3.8000e-02],\n",
              "        [2.1800e-01],\n",
              "        [2.0180e+00],\n",
              "        [2.0018e+01]],\n",
              "\n",
              "       [[3.8000e-02],\n",
              "        [5.6000e-02],\n",
              "        [2.3600e-01],\n",
              "        [2.0360e+00],\n",
              "        [2.0036e+01]],\n",
              "\n",
              "       [[1.8400e-01],\n",
              "        [2.0200e-01],\n",
              "        [3.8200e-01],\n",
              "        [2.1820e+00],\n",
              "        [2.0182e+01]],\n",
              "\n",
              "       [[9.6000e-02],\n",
              "        [1.1400e-01],\n",
              "        [2.9400e-01],\n",
              "        [2.0940e+00],\n",
              "        [2.0094e+01]],\n",
              "\n",
              "       [[1.3200e-01],\n",
              "        [1.5000e-01],\n",
              "        [3.3000e-01],\n",
              "        [2.1300e+00],\n",
              "        [2.0130e+01]],\n",
              "\n",
              "       [[1.5200e-01],\n",
              "        [1.7000e-01],\n",
              "        [3.5000e-01],\n",
              "        [2.1500e+00],\n",
              "        [2.0150e+01]],\n",
              "\n",
              "       [[1.5600e-01],\n",
              "        [1.7400e-01],\n",
              "        [3.5400e-01],\n",
              "        [2.1540e+00],\n",
              "        [2.0154e+01]],\n",
              "\n",
              "       [[9.0000e-02],\n",
              "        [1.0800e-01],\n",
              "        [2.8800e-01],\n",
              "        [2.0880e+00],\n",
              "        [2.0088e+01]],\n",
              "\n",
              "       [[1.8600e-01],\n",
              "        [2.0400e-01],\n",
              "        [3.8400e-01],\n",
              "        [2.1840e+00],\n",
              "        [2.0184e+01]],\n",
              "\n",
              "       [[1.8200e-01],\n",
              "        [2.0000e-01],\n",
              "        [3.8000e-01],\n",
              "        [2.1800e+00],\n",
              "        [2.0180e+01]],\n",
              "\n",
              "       [[1.0800e-01],\n",
              "        [1.2600e-01],\n",
              "        [3.0600e-01],\n",
              "        [2.1060e+00],\n",
              "        [2.0106e+01]],\n",
              "\n",
              "       [[3.2000e-02],\n",
              "        [5.0000e-02],\n",
              "        [2.3000e-01],\n",
              "        [2.0300e+00],\n",
              "        [2.0030e+01]],\n",
              "\n",
              "       [[1.5400e-01],\n",
              "        [1.7200e-01],\n",
              "        [3.5200e-01],\n",
              "        [2.1520e+00],\n",
              "        [2.0152e+01]],\n",
              "\n",
              "       [[1.6000e-02],\n",
              "        [3.4000e-02],\n",
              "        [2.1400e-01],\n",
              "        [2.0140e+00],\n",
              "        [2.0014e+01]],\n",
              "\n",
              "       [[1.6200e-01],\n",
              "        [1.8000e-01],\n",
              "        [3.6000e-01],\n",
              "        [2.1600e+00],\n",
              "        [2.0160e+01]],\n",
              "\n",
              "       [[6.2000e-02],\n",
              "        [8.0000e-02],\n",
              "        [2.6000e-01],\n",
              "        [2.0600e+00],\n",
              "        [2.0060e+01]],\n",
              "\n",
              "       [[5.6000e-02],\n",
              "        [7.4000e-02],\n",
              "        [2.5400e-01],\n",
              "        [2.0540e+00],\n",
              "        [2.0054e+01]],\n",
              "\n",
              "       [[1.2600e-01],\n",
              "        [1.4400e-01],\n",
              "        [3.2400e-01],\n",
              "        [2.1240e+00],\n",
              "        [2.0124e+01]],\n",
              "\n",
              "       [[1.8000e-02],\n",
              "        [3.6000e-02],\n",
              "        [2.1600e-01],\n",
              "        [2.0160e+00],\n",
              "        [2.0016e+01]],\n",
              "\n",
              "       [[1.4800e-01],\n",
              "        [1.6600e-01],\n",
              "        [3.4600e-01],\n",
              "        [2.1460e+00],\n",
              "        [2.0146e+01]],\n",
              "\n",
              "       [[3.4000e-02],\n",
              "        [5.2000e-02],\n",
              "        [2.3200e-01],\n",
              "        [2.0320e+00],\n",
              "        [2.0032e+01]],\n",
              "\n",
              "       [[1.2400e-01],\n",
              "        [1.4200e-01],\n",
              "        [3.2200e-01],\n",
              "        [2.1220e+00],\n",
              "        [2.0122e+01]],\n",
              "\n",
              "       [[1.5800e-01],\n",
              "        [1.7600e-01],\n",
              "        [3.5600e-01],\n",
              "        [2.1560e+00],\n",
              "        [2.0156e+01]],\n",
              "\n",
              "       [[2.0000e-01],\n",
              "        [2.1800e-01],\n",
              "        [3.9800e-01],\n",
              "        [2.1980e+00],\n",
              "        [2.0198e+01]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du1QRaqRMNcJ",
        "colab_type": "code",
        "outputId": "cfe6d0c5-245e-43ac-8dfd-d278382975ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.layers import SimpleRNN,LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM((2),input_shape=(5,1),return_sequences=True))\n",
        "model.add(LSTM((3),input_shape=(5,1),return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='relu'))\n",
        "#model.compile(optimizer='adam',loss='mae',metrics=['acc'])\n",
        "model.compile(optimizer='adam',loss='mae',metrics=['acc'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjpIUfGhMR7X",
        "colab_type": "code",
        "outputId": "a10fbad2-22db-4b73-bb38-ab8453283890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 5, 2)              32        \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 5, 3)              72        \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 120\n",
            "Trainable params: 120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQoeR66bM2wr",
        "colab_type": "code",
        "outputId": "c70b2ca3-e6b0-42ea-cfaa-4c86f7df3421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "80/80 [==============================] - 2s 29ms/step - loss: 0.1356 - acc: 0.0000e+00 - val_loss: 0.1170 - val_acc: 0.0000e+00\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.1315 - acc: 0.0000e+00 - val_loss: 0.1128 - val_acc: 0.0000e+00\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 0s 262us/step - loss: 0.1273 - acc: 0.0000e+00 - val_loss: 0.1086 - val_acc: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 0s 288us/step - loss: 0.1230 - acc: 0.0000e+00 - val_loss: 0.1042 - val_acc: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.1185 - acc: 0.0000e+00 - val_loss: 0.0996 - val_acc: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 0.1140 - acc: 0.0000e+00 - val_loss: 0.0950 - val_acc: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.1093 - acc: 0.0000e+00 - val_loss: 0.0902 - val_acc: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.1045 - acc: 0.0000e+00 - val_loss: 0.0853 - val_acc: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 0.0995 - acc: 0.0000e+00 - val_loss: 0.0802 - val_acc: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.0947 - acc: 0.0000e+00 - val_loss: 0.0750 - val_acc: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.0901 - acc: 0.0000e+00 - val_loss: 0.0698 - val_acc: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 0s 351us/step - loss: 0.0857 - acc: 0.0000e+00 - val_loss: 0.0645 - val_acc: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.0814 - acc: 0.0000e+00 - val_loss: 0.0599 - val_acc: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0776 - acc: 0.0000e+00 - val_loss: 0.0555 - val_acc: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 0s 385us/step - loss: 0.0736 - acc: 0.0000e+00 - val_loss: 0.0513 - val_acc: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 0s 394us/step - loss: 0.0703 - acc: 0.0000e+00 - val_loss: 0.0476 - val_acc: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.0668 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.0641 - acc: 0.0000e+00 - val_loss: 0.0415 - val_acc: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0610 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.0588 - acc: 0.0000e+00 - val_loss: 0.0370 - val_acc: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0566 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.0547 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0531 - acc: 0.0000e+00 - val_loss: 0.0355 - val_acc: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.0515 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.0504 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.0494 - acc: 0.0000e+00 - val_loss: 0.0388 - val_acc: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.0490 - acc: 0.0000e+00 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 0s 341us/step - loss: 0.0487 - acc: 0.0000e+00 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 0s 410us/step - loss: 0.0485 - acc: 0.0000e+00 - val_loss: 0.0419 - val_acc: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 0s 354us/step - loss: 0.0486 - acc: 0.0000e+00 - val_loss: 0.0427 - val_acc: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.0488 - acc: 0.0000e+00 - val_loss: 0.0434 - val_acc: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0489 - acc: 0.0000e+00 - val_loss: 0.0440 - val_acc: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.0491 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0492 - acc: 0.0000e+00 - val_loss: 0.0445 - val_acc: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 0s 353us/step - loss: 0.0492 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.0491 - acc: 0.0000e+00 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 0s 361us/step - loss: 0.0490 - acc: 0.0000e+00 - val_loss: 0.0436 - val_acc: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0489 - acc: 0.0000e+00 - val_loss: 0.0431 - val_acc: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.0487 - acc: 0.0000e+00 - val_loss: 0.0426 - val_acc: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 0s 385us/step - loss: 0.0486 - acc: 0.0000e+00 - val_loss: 0.0421 - val_acc: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.0484 - acc: 0.0000e+00 - val_loss: 0.0416 - val_acc: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.0483 - acc: 0.0000e+00 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 0s 370us/step - loss: 0.0482 - acc: 0.0000e+00 - val_loss: 0.0406 - val_acc: 0.0000e+00\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 0s 405us/step - loss: 0.0481 - acc: 0.0000e+00 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 0.0481 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 0s 430us/step - loss: 0.0482 - acc: 0.0000e+00 - val_loss: 0.0394 - val_acc: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.0483 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.0482 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0482 - acc: 0.0000e+00 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0481 - acc: 0.0000e+00 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 0s 282us/step - loss: 0.0481 - acc: 0.0000e+00 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 0.0481 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 0s 355us/step - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.0479 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 0s 441us/step - loss: 0.0478 - acc: 0.0000e+00 - val_loss: 0.0394 - val_acc: 0.0000e+00\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 0s 430us/step - loss: 0.0478 - acc: 0.0000e+00 - val_loss: 0.0396 - val_acc: 0.0000e+00\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 0s 416us/step - loss: 0.0477 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0476 - acc: 0.0000e+00 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0476 - acc: 0.0000e+00 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.0476 - acc: 0.0000e+00 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 0s 341us/step - loss: 0.0475 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.0475 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.0474 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 0s 401us/step - loss: 0.0474 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.0473 - acc: 0.0000e+00 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.0473 - acc: 0.0000e+00 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.0472 - acc: 0.0000e+00 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 0s 384us/step - loss: 0.0471 - acc: 0.0000e+00 - val_loss: 0.0395 - val_acc: 0.0000e+00\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 0.0471 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 0.0472 - acc: 0.0000e+00 - val_loss: 0.0388 - val_acc: 0.0000e+00\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.0471 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.0471 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0000e+00\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 0.0470 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0000e+00\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.0469 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 0.0468 - acc: 0.0000e+00 - val_loss: 0.0395 - val_acc: 0.0000e+00\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 0s 362us/step - loss: 0.0467 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 0s 360us/step - loss: 0.0466 - acc: 0.0000e+00 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 0.0466 - acc: 0.0000e+00 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 0s 367us/step - loss: 0.0465 - acc: 0.0000e+00 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.0464 - acc: 0.0000e+00 - val_loss: 0.0396 - val_acc: 0.0000e+00\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 0s 507us/step - loss: 0.0464 - acc: 0.0000e+00 - val_loss: 0.0394 - val_acc: 0.0000e+00\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 0s 378us/step - loss: 0.0463 - acc: 0.0000e+00 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 0s 370us/step - loss: 0.0462 - acc: 0.0000e+00 - val_loss: 0.0388 - val_acc: 0.0000e+00\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 0s 354us/step - loss: 0.0461 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 0s 394us/step - loss: 0.0461 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 0s 386us/step - loss: 0.0459 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0459 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.0458 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0457 - acc: 0.0000e+00 - val_loss: 0.0389 - val_acc: 0.0000e+00\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.0456 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 0s 372us/step - loss: 0.0455 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0000e+00\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.0454 - acc: 0.0000e+00 - val_loss: 0.0381 - val_acc: 0.0000e+00\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 0s 399us/step - loss: 0.0453 - acc: 0.0000e+00 - val_loss: 0.0378 - val_acc: 0.0000e+00\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.0453 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0452 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.0451 - acc: 0.0000e+00 - val_loss: 0.0376 - val_acc: 0.0000e+00\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.0449 - acc: 0.0000e+00 - val_loss: 0.0379 - val_acc: 0.0000e+00\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0448 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0000e+00\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0447 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0000e+00\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.0446 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0000e+00\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 0s 373us/step - loss: 0.0445 - acc: 0.0000e+00 - val_loss: 0.0386 - val_acc: 0.0000e+00\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 0.0444 - acc: 0.0000e+00 - val_loss: 0.0385 - val_acc: 0.0000e+00\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.0443 - acc: 0.0000e+00 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.0442 - acc: 0.0000e+00 - val_loss: 0.0380 - val_acc: 0.0000e+00\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0441 - acc: 0.0000e+00 - val_loss: 0.0378 - val_acc: 0.0000e+00\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.0439 - acc: 0.0000e+00 - val_loss: 0.0378 - val_acc: 0.0000e+00\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.0438 - acc: 0.0000e+00 - val_loss: 0.0377 - val_acc: 0.0000e+00\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.0437 - acc: 0.0000e+00 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 0s 370us/step - loss: 0.0435 - acc: 0.0000e+00 - val_loss: 0.0370 - val_acc: 0.0000e+00\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.0433 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0433 - acc: 0.0000e+00 - val_loss: 0.0356 - val_acc: 0.0000e+00\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0432 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 0.0431 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.0429 - acc: 0.0000e+00 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.0428 - acc: 0.0000e+00 - val_loss: 0.0352 - val_acc: 0.0000e+00\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 0s 343us/step - loss: 0.0426 - acc: 0.0000e+00 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.0423 - acc: 0.0000e+00 - val_loss: 0.0355 - val_acc: 0.0000e+00\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 0s 429us/step - loss: 0.0421 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.0419 - acc: 0.0000e+00 - val_loss: 0.0360 - val_acc: 0.0000e+00\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 0.0418 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 0.0417 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0416 - acc: 0.0000e+00 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0416 - acc: 0.0000e+00 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 0s 364us/step - loss: 0.0414 - acc: 0.0000e+00 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 0s 298us/step - loss: 0.0411 - acc: 0.0000e+00 - val_loss: 0.0362 - val_acc: 0.0000e+00\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.0409 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0405 - acc: 0.0000e+00 - val_loss: 0.0346 - val_acc: 0.0000e+00\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0402 - acc: 0.0000e+00 - val_loss: 0.0336 - val_acc: 0.0000e+00\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.0400 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0400 - acc: 0.0000e+00 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 0.0399 - acc: 0.0000e+00 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 0s 346us/step - loss: 0.0397 - acc: 0.0000e+00 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 0.0394 - acc: 0.0000e+00 - val_loss: 0.0322 - val_acc: 0.0000e+00\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0389 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0387 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0384 - acc: 0.0000e+00 - val_loss: 0.0330 - val_acc: 0.0000e+00\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 0s 361us/step - loss: 0.0381 - acc: 0.0000e+00 - val_loss: 0.0328 - val_acc: 0.0000e+00\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 0.0378 - acc: 0.0000e+00 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 0.0376 - acc: 0.0000e+00 - val_loss: 0.0323 - val_acc: 0.0000e+00\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.0372 - acc: 0.0000e+00 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 0s 443us/step - loss: 0.0370 - acc: 0.0000e+00 - val_loss: 0.0305 - val_acc: 0.0000e+00\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 0s 308us/step - loss: 0.0368 - acc: 0.0000e+00 - val_loss: 0.0300 - val_acc: 0.0000e+00\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 0.0364 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 0s 326us/step - loss: 0.0361 - acc: 0.0000e+00 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 0s 376us/step - loss: 0.0357 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0353 - acc: 0.0000e+00 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 0s 310us/step - loss: 0.0349 - acc: 0.0000e+00 - val_loss: 0.0303 - val_acc: 0.0000e+00\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 0.0347 - acc: 0.0000e+00 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0344 - acc: 0.0000e+00 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 0.0341 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0335 - acc: 0.0000e+00 - val_loss: 0.0285 - val_acc: 0.0000e+00\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0331 - acc: 0.0000e+00 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0326 - acc: 0.0000e+00 - val_loss: 0.0265 - val_acc: 0.0000e+00\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.0323 - acc: 0.0000e+00 - val_loss: 0.0257 - val_acc: 0.0000e+00\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 0s 374us/step - loss: 0.0320 - acc: 0.0000e+00 - val_loss: 0.0256 - val_acc: 0.0000e+00\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 0s 402us/step - loss: 0.0314 - acc: 0.0000e+00 - val_loss: 0.0258 - val_acc: 0.0000e+00\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 0s 384us/step - loss: 0.0307 - acc: 0.0000e+00 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 0s 351us/step - loss: 0.0302 - acc: 0.0000e+00 - val_loss: 0.0267 - val_acc: 0.0000e+00\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0299 - acc: 0.0000e+00 - val_loss: 0.0268 - val_acc: 0.0000e+00\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.0294 - acc: 0.0000e+00 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 0s 347us/step - loss: 0.0287 - acc: 0.0000e+00 - val_loss: 0.0248 - val_acc: 0.0000e+00\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 0s 337us/step - loss: 0.0280 - acc: 0.0000e+00 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 0s 367us/step - loss: 0.0273 - acc: 0.0000e+00 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 0.0268 - acc: 0.0000e+00 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 0.0267 - acc: 0.0000e+00 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 0s 367us/step - loss: 0.0265 - acc: 0.0000e+00 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0251 - acc: 0.0000e+00 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 0.0241 - acc: 0.0000e+00 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 0s 395us/step - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 0.0234 - acc: 0.0000e+00 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 0s 337us/step - loss: 0.0224 - acc: 0.0000e+00 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 0.0212 - acc: 0.0000e+00 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 0s 354us/step - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 0s 363us/step - loss: 0.0195 - acc: 0.0000e+00 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 0.0188 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.0131 - val_acc: 0.0000e+00\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.0130 - val_acc: 0.0000e+00\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0155 - acc: 0.0000e+00 - val_loss: 0.0127 - val_acc: 0.0000e+00\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 0.0144 - acc: 0.0000e+00 - val_loss: 0.0124 - val_acc: 0.0000e+00\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.0118 - val_acc: 0.0000e+00\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.0102 - val_acc: 0.0000e+00\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 0s 390us/step - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.0084 - val_acc: 0.0000e+00\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 0s 356us/step - loss: 0.0095 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 0s 354us/step - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 0s 320us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 0s 385us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 0s 448us/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 0s 376us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 0s 384us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 0s 336us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 0s 405us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 8.4202e-04 - val_acc: 0.0000e+00\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 8.6969e-04 - acc: 0.0000e+00 - val_loss: 8.6493e-04 - val_acc: 0.0000e+00\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 7.3711e-04 - acc: 0.0000e+00 - val_loss: 4.4859e-04 - val_acc: 0.0000e+00\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 0s 375us/step - loss: 7.4338e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 0s 368us/step - loss: 8.1386e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 0s 428us/step - loss: 9.9143e-04 - acc: 0.0000e+00 - val_loss: 7.2039e-04 - val_acc: 0.0000e+00\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 9.2308e-04 - val_acc: 0.0000e+00\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 4.9119e-04 - val_acc: 0.0000e+00\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 0s 383us/step - loss: 8.6474e-04 - acc: 0.0000e+00 - val_loss: 8.8119e-04 - val_acc: 0.0000e+00\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 0s 344us/step - loss: 7.3786e-04 - acc: 0.0000e+00 - val_loss: 4.3265e-04 - val_acc: 0.0000e+00\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 0s 408us/step - loss: 7.6395e-04 - acc: 0.0000e+00 - val_loss: 3.9759e-04 - val_acc: 0.0000e+00\n",
            "Epoch 221/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 6.0840e-04 - acc: 0.0000e+00 - val_loss: 6.6113e-04 - val_acc: 0.0000e+00\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 0s 320us/step - loss: 6.4682e-04 - acc: 0.0000e+00 - val_loss: 8.0343e-04 - val_acc: 0.0000e+00\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 0s 343us/step - loss: 7.0789e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 0s 501us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 0s 372us/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 0s 371us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 0s 390us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 9.0882e-04 - val_acc: 0.0000e+00\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 0s 294us/step - loss: 9.7509e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 0s 335us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 6.6734e-04 - val_acc: 0.0000e+00\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 9.7798e-04 - val_acc: 0.0000e+00\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 8.0974e-04 - acc: 0.0000e+00 - val_loss: 5.6322e-04 - val_acc: 0.0000e+00\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 7.6119e-04 - acc: 0.0000e+00 - val_loss: 4.3195e-04 - val_acc: 0.0000e+00\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 8.3356e-04 - acc: 0.0000e+00 - val_loss: 7.8953e-04 - val_acc: 0.0000e+00\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 6.9861e-04 - acc: 0.0000e+00 - val_loss: 3.7979e-04 - val_acc: 0.0000e+00\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 0s 340us/step - loss: 6.9007e-04 - acc: 0.0000e+00 - val_loss: 4.8868e-04 - val_acc: 0.0000e+00\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 5.4716e-04 - acc: 0.0000e+00 - val_loss: 3.5063e-04 - val_acc: 0.0000e+00\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 5.7020e-04 - acc: 0.0000e+00 - val_loss: 8.3590e-04 - val_acc: 0.0000e+00\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 8.0218e-04 - acc: 0.0000e+00 - val_loss: 3.5605e-04 - val_acc: 0.0000e+00\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 0s 320us/step - loss: 5.9390e-04 - acc: 0.0000e+00 - val_loss: 3.5226e-04 - val_acc: 0.0000e+00\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 5.8265e-04 - acc: 0.0000e+00 - val_loss: 5.9331e-04 - val_acc: 0.0000e+00\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 0s 367us/step - loss: 5.8589e-04 - acc: 0.0000e+00 - val_loss: 7.7670e-04 - val_acc: 0.0000e+00\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 6.9157e-04 - val_acc: 0.0000e+00\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 0s 309us/step - loss: 7.2421e-04 - acc: 0.0000e+00 - val_loss: 6.1449e-04 - val_acc: 0.0000e+00\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 0s 293us/step - loss: 5.8188e-04 - acc: 0.0000e+00 - val_loss: 3.5898e-04 - val_acc: 0.0000e+00\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 0s 273us/step - loss: 6.2402e-04 - acc: 0.0000e+00 - val_loss: 7.6419e-04 - val_acc: 0.0000e+00\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 0s 316us/step - loss: 6.6732e-04 - acc: 0.0000e+00 - val_loss: 4.4668e-04 - val_acc: 0.0000e+00\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 6.1838e-04 - acc: 0.0000e+00 - val_loss: 5.1890e-04 - val_acc: 0.0000e+00\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 0s 369us/step - loss: 5.4215e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 0s 350us/step - loss: 8.5107e-04 - acc: 0.0000e+00 - val_loss: 7.2633e-04 - val_acc: 0.0000e+00\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 0s 304us/step - loss: 9.6268e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 0s 351us/step - loss: 9.5731e-04 - acc: 0.0000e+00 - val_loss: 3.3781e-04 - val_acc: 0.0000e+00\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 0s 368us/step - loss: 5.9461e-04 - acc: 0.0000e+00 - val_loss: 8.8009e-04 - val_acc: 0.0000e+00\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 0s 347us/step - loss: 7.4333e-04 - acc: 0.0000e+00 - val_loss: 6.0817e-04 - val_acc: 0.0000e+00\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 0s 364us/step - loss: 8.7201e-04 - acc: 0.0000e+00 - val_loss: 8.7622e-04 - val_acc: 0.0000e+00\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 0s 337us/step - loss: 6.6910e-04 - acc: 0.0000e+00 - val_loss: 5.1809e-04 - val_acc: 0.0000e+00\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 8.6614e-04 - acc: 0.0000e+00 - val_loss: 5.9275e-04 - val_acc: 0.0000e+00\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 0s 379us/step - loss: 5.4771e-04 - acc: 0.0000e+00 - val_loss: 3.7494e-04 - val_acc: 0.0000e+00\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 0s 349us/step - loss: 5.4224e-04 - acc: 0.0000e+00 - val_loss: 6.4993e-04 - val_acc: 0.0000e+00\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 5.5334e-04 - acc: 0.0000e+00 - val_loss: 4.6292e-04 - val_acc: 0.0000e+00\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 4.9185e-04 - acc: 0.0000e+00 - val_loss: 3.2340e-04 - val_acc: 0.0000e+00\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 0s 448us/step - loss: 5.2502e-04 - acc: 0.0000e+00 - val_loss: 7.9277e-04 - val_acc: 0.0000e+00\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 0s 341us/step - loss: 7.0552e-04 - acc: 0.0000e+00 - val_loss: 6.3593e-04 - val_acc: 0.0000e+00\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 0s 285us/step - loss: 8.1162e-04 - acc: 0.0000e+00 - val_loss: 9.3366e-04 - val_acc: 0.0000e+00\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 8.2294e-04 - acc: 0.0000e+00 - val_loss: 3.3171e-04 - val_acc: 0.0000e+00\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 0s 346us/step - loss: 5.6494e-04 - acc: 0.0000e+00 - val_loss: 3.8244e-04 - val_acc: 0.0000e+00\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 5.0629e-04 - acc: 0.0000e+00 - val_loss: 8.6919e-04 - val_acc: 0.0000e+00\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 7.3806e-04 - acc: 0.0000e+00 - val_loss: 4.7831e-04 - val_acc: 0.0000e+00\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 0s 326us/step - loss: 6.9698e-04 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 8.5133e-04 - val_acc: 0.0000e+00\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 5.1005e-04 - val_acc: 0.0000e+00\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 0s 385us/step - loss: 6.1845e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 7.4433e-04 - acc: 0.0000e+00 - val_loss: 3.9398e-04 - val_acc: 0.0000e+00\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 6.7547e-04 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 0s 307us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 0s 357us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 9.5931e-04 - val_acc: 0.0000e+00\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 8.4069e-04 - acc: 0.0000e+00 - val_loss: 7.6596e-04 - val_acc: 0.0000e+00\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 0s 361us/step - loss: 8.0382e-04 - acc: 0.0000e+00 - val_loss: 7.4969e-04 - val_acc: 0.0000e+00\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 0s 350us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 0s 290us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 6.0417e-04 - val_acc: 0.0000e+00\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 0s 297us/step - loss: 6.1944e-04 - acc: 0.0000e+00 - val_loss: 5.0240e-04 - val_acc: 0.0000e+00\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 6.5659e-04 - acc: 0.0000e+00 - val_loss: 4.0392e-04 - val_acc: 0.0000e+00\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 0s 374us/step - loss: 8.4780e-04 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 0s 373us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 0s 416us/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 0s 301us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 3.5765e-04 - val_acc: 0.0000e+00\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 7.6877e-04 - acc: 0.0000e+00 - val_loss: 3.3525e-04 - val_acc: 0.0000e+00\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 6.4256e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 8.0693e-04 - acc: 0.0000e+00 - val_loss: 4.3611e-04 - val_acc: 0.0000e+00\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 0s 334us/step - loss: 7.6611e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 0s 317us/step - loss: 9.3328e-04 - acc: 0.0000e+00 - val_loss: 9.9201e-04 - val_acc: 0.0000e+00\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 0s 291us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 9.3175e-04 - acc: 0.0000e+00 - val_loss: 7.8728e-04 - val_acc: 0.0000e+00\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 0s 289us/step - loss: 7.4295e-04 - acc: 0.0000e+00 - val_loss: 4.0636e-04 - val_acc: 0.0000e+00\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 6.9101e-04 - acc: 0.0000e+00 - val_loss: 7.7798e-04 - val_acc: 0.0000e+00\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 6.1340e-04 - acc: 0.0000e+00 - val_loss: 3.5150e-04 - val_acc: 0.0000e+00\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 0s 313us/step - loss: 4.9282e-04 - acc: 0.0000e+00 - val_loss: 5.2491e-04 - val_acc: 0.0000e+00\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 0s 398us/step - loss: 4.7742e-04 - acc: 0.0000e+00 - val_loss: 3.0910e-04 - val_acc: 0.0000e+00\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 0s 394us/step - loss: 5.3060e-04 - acc: 0.0000e+00 - val_loss: 3.0850e-04 - val_acc: 0.0000e+00\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 0s 394us/step - loss: 5.1291e-04 - acc: 0.0000e+00 - val_loss: 5.5995e-04 - val_acc: 0.0000e+00\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 5.8944e-04 - acc: 0.0000e+00 - val_loss: 3.0075e-04 - val_acc: 0.0000e+00\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 4.2719e-04 - acc: 0.0000e+00 - val_loss: 2.7625e-04 - val_acc: 0.0000e+00\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 0s 292us/step - loss: 4.5839e-04 - acc: 0.0000e+00 - val_loss: 5.5346e-04 - val_acc: 0.0000e+00\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 0s 284us/step - loss: 4.7712e-04 - acc: 0.0000e+00 - val_loss: 7.2627e-04 - val_acc: 0.0000e+00\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 8.8085e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 0s 324us/step - loss: 9.7332e-04 - acc: 0.0000e+00 - val_loss: 6.8860e-04 - val_acc: 0.0000e+00\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 0s 351us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 7.2645e-04 - val_acc: 0.0000e+00\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 0s 311us/step - loss: 6.5517e-04 - acc: 0.0000e+00 - val_loss: 6.0225e-04 - val_acc: 0.0000e+00\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 0s 300us/step - loss: 5.3408e-04 - acc: 0.0000e+00 - val_loss: 3.2750e-04 - val_acc: 0.0000e+00\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 0s 465us/step - loss: 4.9088e-04 - acc: 0.0000e+00 - val_loss: 4.6515e-04 - val_acc: 0.0000e+00\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 0s 350us/step - loss: 4.6917e-04 - acc: 0.0000e+00 - val_loss: 3.1364e-04 - val_acc: 0.0000e+00\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 0s 303us/step - loss: 4.6262e-04 - acc: 0.0000e+00 - val_loss: 5.6068e-04 - val_acc: 0.0000e+00\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 5.2521e-04 - acc: 0.0000e+00 - val_loss: 3.0765e-04 - val_acc: 0.0000e+00\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 4.2154e-04 - acc: 0.0000e+00 - val_loss: 3.3704e-04 - val_acc: 0.0000e+00\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 5.4397e-04 - acc: 0.0000e+00 - val_loss: 7.5066e-04 - val_acc: 0.0000e+00\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 0s 299us/step - loss: 6.3950e-04 - acc: 0.0000e+00 - val_loss: 3.0110e-04 - val_acc: 0.0000e+00\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 0s 295us/step - loss: 3.9926e-04 - acc: 0.0000e+00 - val_loss: 3.2594e-04 - val_acc: 0.0000e+00\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 4.0239e-04 - acc: 0.0000e+00 - val_loss: 4.2248e-04 - val_acc: 0.0000e+00\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 4.6150e-04 - acc: 0.0000e+00 - val_loss: 5.1671e-04 - val_acc: 0.0000e+00\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 0s 330us/step - loss: 4.7202e-04 - acc: 0.0000e+00 - val_loss: 3.3049e-04 - val_acc: 0.0000e+00\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 0s 351us/step - loss: 5.8711e-04 - acc: 0.0000e+00 - val_loss: 2.7458e-04 - val_acc: 0.0000e+00\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 4.4012e-04 - acc: 0.0000e+00 - val_loss: 2.6913e-04 - val_acc: 0.0000e+00\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 0s 328us/step - loss: 4.0839e-04 - acc: 0.0000e+00 - val_loss: 7.7245e-04 - val_acc: 0.0000e+00\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 6.3676e-04 - acc: 0.0000e+00 - val_loss: 2.5974e-04 - val_acc: 0.0000e+00\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 0s 373us/step - loss: 4.4543e-04 - acc: 0.0000e+00 - val_loss: 4.1744e-04 - val_acc: 0.0000e+00\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 0s 337us/step - loss: 5.5748e-04 - acc: 0.0000e+00 - val_loss: 2.4878e-04 - val_acc: 0.0000e+00\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 0s 372us/step - loss: 4.8052e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 0s 347us/step - loss: 9.1249e-04 - acc: 0.0000e+00 - val_loss: 6.6621e-04 - val_acc: 0.0000e+00\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 0s 365us/step - loss: 7.8911e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 5.8495e-04 - val_acc: 0.0000e+00\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 0s 331us/step - loss: 8.8081e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 7.5653e-04 - acc: 0.0000e+00 - val_loss: 3.8807e-04 - val_acc: 0.0000e+00\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 0s 352us/step - loss: 7.2628e-04 - acc: 0.0000e+00 - val_loss: 7.4423e-04 - val_acc: 0.0000e+00\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 7.0181e-04 - acc: 0.0000e+00 - val_loss: 8.0736e-04 - val_acc: 0.0000e+00\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 0s 400us/step - loss: 6.0222e-04 - acc: 0.0000e+00 - val_loss: 4.0396e-04 - val_acc: 0.0000e+00\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 0s 332us/step - loss: 7.6081e-04 - acc: 0.0000e+00 - val_loss: 7.7878e-04 - val_acc: 0.0000e+00\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 0s 368us/step - loss: 6.0530e-04 - acc: 0.0000e+00 - val_loss: 3.2699e-04 - val_acc: 0.0000e+00\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 0s 325us/step - loss: 4.3498e-04 - acc: 0.0000e+00 - val_loss: 3.8366e-04 - val_acc: 0.0000e+00\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 0s 409us/step - loss: 5.4998e-04 - acc: 0.0000e+00 - val_loss: 6.0698e-04 - val_acc: 0.0000e+00\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 0s 380us/step - loss: 5.9820e-04 - acc: 0.0000e+00 - val_loss: 2.3689e-04 - val_acc: 0.0000e+00\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 0s 408us/step - loss: 4.1518e-04 - acc: 0.0000e+00 - val_loss: 2.6166e-04 - val_acc: 0.0000e+00\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 0s 378us/step - loss: 3.6146e-04 - acc: 0.0000e+00 - val_loss: 4.3704e-04 - val_acc: 0.0000e+00\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 0s 327us/step - loss: 4.8888e-04 - acc: 0.0000e+00 - val_loss: 4.7126e-04 - val_acc: 0.0000e+00\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 0s 406us/step - loss: 4.2094e-04 - acc: 0.0000e+00 - val_loss: 2.6353e-04 - val_acc: 0.0000e+00\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 0s 338us/step - loss: 4.6256e-04 - acc: 0.0000e+00 - val_loss: 5.8028e-04 - val_acc: 0.0000e+00\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 0s 278us/step - loss: 4.9819e-04 - acc: 0.0000e+00 - val_loss: 5.9681e-04 - val_acc: 0.0000e+00\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 0s 315us/step - loss: 4.5993e-04 - acc: 0.0000e+00 - val_loss: 3.1104e-04 - val_acc: 0.0000e+00\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 0s 339us/step - loss: 5.2997e-04 - acc: 0.0000e+00 - val_loss: 7.2765e-04 - val_acc: 0.0000e+00\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 0s 395us/step - loss: 5.7128e-04 - acc: 0.0000e+00 - val_loss: 3.7568e-04 - val_acc: 0.0000e+00\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 0s 408us/step - loss: 3.7533e-04 - acc: 0.0000e+00 - val_loss: 5.9778e-04 - val_acc: 0.0000e+00\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 5.0584e-04 - acc: 0.0000e+00 - val_loss: 2.5465e-04 - val_acc: 0.0000e+00\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 4.4821e-04 - acc: 0.0000e+00 - val_loss: 2.7439e-04 - val_acc: 0.0000e+00\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 0s 305us/step - loss: 3.5785e-04 - acc: 0.0000e+00 - val_loss: 2.3369e-04 - val_acc: 0.0000e+00\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 0s 333us/step - loss: 3.7277e-04 - acc: 0.0000e+00 - val_loss: 2.6170e-04 - val_acc: 0.0000e+00\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 4.4441e-04 - acc: 0.0000e+00 - val_loss: 3.2629e-04 - val_acc: 0.0000e+00\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 0s 348us/step - loss: 3.2989e-04 - acc: 0.0000e+00 - val_loss: 2.2757e-04 - val_acc: 0.0000e+00\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 0s 306us/step - loss: 3.6427e-04 - acc: 0.0000e+00 - val_loss: 4.3643e-04 - val_acc: 0.0000e+00\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 6.1661e-04 - acc: 0.0000e+00 - val_loss: 3.2363e-04 - val_acc: 0.0000e+00\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 0s 379us/step - loss: 3.5153e-04 - acc: 0.0000e+00 - val_loss: 5.3292e-04 - val_acc: 0.0000e+00\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 0s 322us/step - loss: 4.4604e-04 - acc: 0.0000e+00 - val_loss: 2.4119e-04 - val_acc: 0.0000e+00\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 0s 346us/step - loss: 3.7272e-04 - acc: 0.0000e+00 - val_loss: 7.0380e-04 - val_acc: 0.0000e+00\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 0s 355us/step - loss: 5.5421e-04 - acc: 0.0000e+00 - val_loss: 3.7011e-04 - val_acc: 0.0000e+00\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 0s 366us/step - loss: 5.8889e-04 - acc: 0.0000e+00 - val_loss: 5.0328e-04 - val_acc: 0.0000e+00\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 0s 318us/step - loss: 4.8005e-04 - acc: 0.0000e+00 - val_loss: 4.1913e-04 - val_acc: 0.0000e+00\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 0s 358us/step - loss: 7.7263e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 0s 312us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 0s 342us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 0s 329us/step - loss: 9.5096e-04 - acc: 0.0000e+00 - val_loss: 3.2637e-04 - val_acc: 0.0000e+00\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 0s 367us/step - loss: 4.9870e-04 - acc: 0.0000e+00 - val_loss: 2.8437e-04 - val_acc: 0.0000e+00\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 0s 321us/step - loss: 5.7406e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 0s 323us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 0s 359us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 2.2569e-04 - val_acc: 0.0000e+00\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 0s 319us/step - loss: 5.1101e-04 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
            "Epoch 380/1000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyzz-rKfM6_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyFPm30xPDT9",
        "colab_type": "code",
        "outputId": "9093dec8-bf8f-4ce4-e82d-10b94db4ec7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.round(y_predict*500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [2., 1., 2.],\n",
              "        [2., 1., 3.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 2.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 2.],\n",
              "        [2., 1., 2.],\n",
              "        [2., 1., 3.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.]],\n",
              "\n",
              "       [[1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [2., 1., 2.],\n",
              "        [2., 1., 3.],\n",
              "        [3., 2., 4.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 2.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 2.],\n",
              "        [2., 1., 2.],\n",
              "        [2., 1., 3.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 2.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [2., 1., 2.],\n",
              "        [2., 1., 2.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 2.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 2.],\n",
              "        [2., 1., 2.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 0., 1.]],\n",
              "\n",
              "       [[0., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 2.],\n",
              "        [2., 1., 2.],\n",
              "        [2., 1., 3.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu0tR-z8PIDa",
        "colab_type": "code",
        "outputId": "1954e5c0-930e-46d0-95a8-e15c5a510246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "np.round(y_test*500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 98.,  64.,  78.,  60., 106.,  55.,  42.,  52.,  49.,  67., 129.,\n",
              "        69., 102.,  72.,  92.,  66.,  74.,  89.,  44., 101.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTTJ31znPL9k",
        "colab_type": "code",
        "outputId": "3e6099b0-eebe-4b01-ef0a-4cfcdf7cbe3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "plt.scatter(range(20),y_predict,c = 'r')\n",
        "plt.scatter(range(20),y_test ,c='g')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-73d82157fc17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, data, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         verts=verts, edgecolors=edgecolors, **({\"data\": data} if data\n\u001b[0;32m-> 2862\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2863\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4180\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4182\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYfAlqQmDL0lNGHxJauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0iYmjxPQbL",
        "colab_type": "code",
        "outputId": "d2f5cdcf-36e4-4e33-fae1-0339ca84fef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "plt.plot(hist.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8463e856a4e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gui9A-cHPVo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}