{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15MI411.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyMm4AWBO3SE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edc38b7b-35b6-4483-824e-6d8527789ac5"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ymYAOnxLQD2Q",
        "colab": {}
      },
      "source": [
        "X=[]\n",
        "Y=[]\n",
        "a = 9\n",
        "X = []\n",
        "for i in range(1,100):X.append(a*i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ICw9iZa8bo19",
        "colab": {}
      },
      "source": [
        "Y = X[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9EFIm1S_izes",
        "colab": {}
      },
      "source": [
        "Data = np.column_stack((X, Y))\n",
        "trgt = [X+Y for X,Y in zip(X,Y)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NIN9ZCxaf3v7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb33d60b-d49b-490a-d464-7eec322f7cd6"
      },
      "source": [
        " for i in range(99):\n",
        "    print(X[i],Y[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 891\n",
            "18 882\n",
            "27 873\n",
            "36 864\n",
            "45 855\n",
            "54 846\n",
            "63 837\n",
            "72 828\n",
            "81 819\n",
            "90 810\n",
            "99 801\n",
            "108 792\n",
            "117 783\n",
            "126 774\n",
            "135 765\n",
            "144 756\n",
            "153 747\n",
            "162 738\n",
            "171 729\n",
            "180 720\n",
            "189 711\n",
            "198 702\n",
            "207 693\n",
            "216 684\n",
            "225 675\n",
            "234 666\n",
            "243 657\n",
            "252 648\n",
            "261 639\n",
            "270 630\n",
            "279 621\n",
            "288 612\n",
            "297 603\n",
            "306 594\n",
            "315 585\n",
            "324 576\n",
            "333 567\n",
            "342 558\n",
            "351 549\n",
            "360 540\n",
            "369 531\n",
            "378 522\n",
            "387 513\n",
            "396 504\n",
            "405 495\n",
            "414 486\n",
            "423 477\n",
            "432 468\n",
            "441 459\n",
            "450 450\n",
            "459 441\n",
            "468 432\n",
            "477 423\n",
            "486 414\n",
            "495 405\n",
            "504 396\n",
            "513 387\n",
            "522 378\n",
            "531 369\n",
            "540 360\n",
            "549 351\n",
            "558 342\n",
            "567 333\n",
            "576 324\n",
            "585 315\n",
            "594 306\n",
            "603 297\n",
            "612 288\n",
            "621 279\n",
            "630 270\n",
            "639 261\n",
            "648 252\n",
            "657 243\n",
            "666 234\n",
            "675 225\n",
            "684 216\n",
            "693 207\n",
            "702 198\n",
            "711 189\n",
            "720 180\n",
            "729 171\n",
            "738 162\n",
            "747 153\n",
            "756 144\n",
            "765 135\n",
            "774 126\n",
            "783 117\n",
            "792 108\n",
            "801 99\n",
            "810 90\n",
            "819 81\n",
            "828 72\n",
            "837 63\n",
            "846 54\n",
            "855 45\n",
            "864 36\n",
            "873 27\n",
            "882 18\n",
            "891 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xaxy1iDUhPKP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7abc95c8-bbf9-42dc-d748-b8675676ede9"
      },
      "source": [
        " for i in range(99):\n",
        "    print(X[i]+Y[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fJqX7OVj_QV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07d9fc19-66bd-42a7-c057-466c9dda2102"
      },
      "source": [
        "Data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  9, 891],\n",
              "       [ 18, 882],\n",
              "       [ 27, 873],\n",
              "       [ 36, 864],\n",
              "       [ 45, 855],\n",
              "       [ 54, 846],\n",
              "       [ 63, 837],\n",
              "       [ 72, 828],\n",
              "       [ 81, 819],\n",
              "       [ 90, 810],\n",
              "       [ 99, 801],\n",
              "       [108, 792],\n",
              "       [117, 783],\n",
              "       [126, 774],\n",
              "       [135, 765],\n",
              "       [144, 756],\n",
              "       [153, 747],\n",
              "       [162, 738],\n",
              "       [171, 729],\n",
              "       [180, 720],\n",
              "       [189, 711],\n",
              "       [198, 702],\n",
              "       [207, 693],\n",
              "       [216, 684],\n",
              "       [225, 675],\n",
              "       [234, 666],\n",
              "       [243, 657],\n",
              "       [252, 648],\n",
              "       [261, 639],\n",
              "       [270, 630],\n",
              "       [279, 621],\n",
              "       [288, 612],\n",
              "       [297, 603],\n",
              "       [306, 594],\n",
              "       [315, 585],\n",
              "       [324, 576],\n",
              "       [333, 567],\n",
              "       [342, 558],\n",
              "       [351, 549],\n",
              "       [360, 540],\n",
              "       [369, 531],\n",
              "       [378, 522],\n",
              "       [387, 513],\n",
              "       [396, 504],\n",
              "       [405, 495],\n",
              "       [414, 486],\n",
              "       [423, 477],\n",
              "       [432, 468],\n",
              "       [441, 459],\n",
              "       [450, 450],\n",
              "       [459, 441],\n",
              "       [468, 432],\n",
              "       [477, 423],\n",
              "       [486, 414],\n",
              "       [495, 405],\n",
              "       [504, 396],\n",
              "       [513, 387],\n",
              "       [522, 378],\n",
              "       [531, 369],\n",
              "       [540, 360],\n",
              "       [549, 351],\n",
              "       [558, 342],\n",
              "       [567, 333],\n",
              "       [576, 324],\n",
              "       [585, 315],\n",
              "       [594, 306],\n",
              "       [603, 297],\n",
              "       [612, 288],\n",
              "       [621, 279],\n",
              "       [630, 270],\n",
              "       [639, 261],\n",
              "       [648, 252],\n",
              "       [657, 243],\n",
              "       [666, 234],\n",
              "       [675, 225],\n",
              "       [684, 216],\n",
              "       [693, 207],\n",
              "       [702, 198],\n",
              "       [711, 189],\n",
              "       [720, 180],\n",
              "       [729, 171],\n",
              "       [738, 162],\n",
              "       [747, 153],\n",
              "       [756, 144],\n",
              "       [765, 135],\n",
              "       [774, 126],\n",
              "       [783, 117],\n",
              "       [792, 108],\n",
              "       [801,  99],\n",
              "       [810,  90],\n",
              "       [819,  81],\n",
              "       [828,  72],\n",
              "       [837,  63],\n",
              "       [846,  54],\n",
              "       [855,  45],\n",
              "       [864,  36],\n",
              "       [873,  27],\n",
              "       [882,  18],\n",
              "       [891,   9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hDYEZiYokE-P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37cafecc-b78f-4f8b-88e6-838ba6048866"
      },
      "source": [
        "trgt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900,\n",
              " 900]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-whHOrCNkTSb",
        "colab": {}
      },
      "source": [
        "Data=np.array(Data, dtype=\"float\")\n",
        "trgt=np.array(trgt, dtype=\"float\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ua28ezZRFGS",
        "outputId": "f46bdd55-687a-432e-8db6-4d348a5812e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Data = np.array(Data).reshape(99, 2,1)\n",
        "Data.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJeoWr1QRleE",
        "outputId": "c3dbcebf-b14c-4fba-9504-f3ac16c2cecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trgt.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H67cjNOolfub",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(Data,trgt,test_size=0.2,random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIHbtZkBSCV0",
        "outputId": "b84b532f-e380-498b-a1e0-534a71570f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(2,1)))\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(25, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 2, 200)            161600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 2, 100)            120400    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 2, 50)             30200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 25)                7600      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                520       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 320,541\n",
            "Trainable params: 320,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UrdLR33XSW5w",
        "outputId": "75535f98-2897-460c-8ebe-598df59a0dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs=500,validation_data=(x_test,y_test))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 79 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "79/79 [==============================] - 3s 40ms/step - loss: 809814.6044 - val_loss: 809110.6875\n",
            "Epoch 2/500\n",
            "79/79 [==============================] - 0s 992us/step - loss: 808856.0261 - val_loss: 807751.1250\n",
            "Epoch 3/500\n",
            "79/79 [==============================] - 0s 921us/step - loss: 807470.5095 - val_loss: 805491.9375\n",
            "Epoch 4/500\n",
            "79/79 [==============================] - 0s 957us/step - loss: 804314.4786 - val_loss: 800862.2500\n",
            "Epoch 5/500\n",
            "79/79 [==============================] - 0s 915us/step - loss: 798024.1535 - val_loss: 792453.3750\n",
            "Epoch 6/500\n",
            "79/79 [==============================] - 0s 874us/step - loss: 787210.0459 - val_loss: 776646.2500\n",
            "Epoch 7/500\n",
            "79/79 [==============================] - 0s 948us/step - loss: 768133.8758 - val_loss: 754010.3125\n",
            "Epoch 8/500\n",
            "79/79 [==============================] - 0s 930us/step - loss: 738110.3900 - val_loss: 709628.1250\n",
            "Epoch 9/500\n",
            "79/79 [==============================] - 0s 958us/step - loss: 685567.8576 - val_loss: 640033.3125\n",
            "Epoch 10/500\n",
            "79/79 [==============================] - 0s 943us/step - loss: 599882.0657 - val_loss: 542515.1250\n",
            "Epoch 11/500\n",
            "79/79 [==============================] - 0s 911us/step - loss: 490614.9498 - val_loss: 405915.6562\n",
            "Epoch 12/500\n",
            "79/79 [==============================] - 0s 978us/step - loss: 327609.6495 - val_loss: 236392.6562\n",
            "Epoch 13/500\n",
            "79/79 [==============================] - 0s 978us/step - loss: 159747.3533 - val_loss: 126396.7031\n",
            "Epoch 14/500\n",
            "79/79 [==============================] - 0s 921us/step - loss: 108069.3105 - val_loss: 121029.4141\n",
            "Epoch 15/500\n",
            "79/79 [==============================] - 0s 902us/step - loss: 107570.9072 - val_loss: 83909.5547\n",
            "Epoch 16/500\n",
            "79/79 [==============================] - 0s 933us/step - loss: 69968.5340 - val_loss: 69352.9219\n",
            "Epoch 17/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 51153.0498 - val_loss: 60332.2578\n",
            "Epoch 18/500\n",
            "79/79 [==============================] - 0s 946us/step - loss: 44846.2666 - val_loss: 39304.8086\n",
            "Epoch 19/500\n",
            "79/79 [==============================] - 0s 951us/step - loss: 40539.3846 - val_loss: 35528.2695\n",
            "Epoch 20/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 30792.0309 - val_loss: 21062.7090\n",
            "Epoch 21/500\n",
            "79/79 [==============================] - 0s 962us/step - loss: 21408.4748 - val_loss: 22109.5664\n",
            "Epoch 22/500\n",
            "79/79 [==============================] - 0s 956us/step - loss: 23990.5988 - val_loss: 21573.0410\n",
            "Epoch 23/500\n",
            "79/79 [==============================] - 0s 945us/step - loss: 19610.5814 - val_loss: 18197.4609\n",
            "Epoch 24/500\n",
            "79/79 [==============================] - 0s 939us/step - loss: 17717.7178 - val_loss: 18475.7754\n",
            "Epoch 25/500\n",
            "79/79 [==============================] - 0s 989us/step - loss: 13376.4871 - val_loss: 11381.7715\n",
            "Epoch 26/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 12469.7121 - val_loss: 10933.8301\n",
            "Epoch 27/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 10687.4603 - val_loss: 9968.3994\n",
            "Epoch 28/500\n",
            "79/79 [==============================] - 0s 961us/step - loss: 11886.2519 - val_loss: 15842.7627\n",
            "Epoch 29/500\n",
            "79/79 [==============================] - 0s 954us/step - loss: 26330.3401 - val_loss: 17754.8848\n",
            "Epoch 30/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 17440.6412 - val_loss: 12943.0498\n",
            "Epoch 31/500\n",
            "79/79 [==============================] - 0s 916us/step - loss: 14223.6953 - val_loss: 10979.3379\n",
            "Epoch 32/500\n",
            "79/79 [==============================] - 0s 926us/step - loss: 12663.0831 - val_loss: 7575.3423\n",
            "Epoch 33/500\n",
            "79/79 [==============================] - 0s 977us/step - loss: 9102.1328 - val_loss: 7074.7197\n",
            "Epoch 34/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 7908.3560 - val_loss: 7012.9258\n",
            "Epoch 35/500\n",
            "79/79 [==============================] - 0s 953us/step - loss: 6200.0675 - val_loss: 5497.6099\n",
            "Epoch 36/500\n",
            "79/79 [==============================] - 0s 943us/step - loss: 5304.9102 - val_loss: 4427.3330\n",
            "Epoch 37/500\n",
            "79/79 [==============================] - 0s 928us/step - loss: 4523.5635 - val_loss: 3113.7100\n",
            "Epoch 38/500\n",
            "79/79 [==============================] - 0s 946us/step - loss: 4078.1494 - val_loss: 2473.0510\n",
            "Epoch 39/500\n",
            "79/79 [==============================] - 0s 959us/step - loss: 3464.6681 - val_loss: 2329.4985\n",
            "Epoch 40/500\n",
            "79/79 [==============================] - 0s 901us/step - loss: 3782.7453 - val_loss: 1847.9789\n",
            "Epoch 41/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 3610.9709 - val_loss: 2424.4558\n",
            "Epoch 42/500\n",
            "79/79 [==============================] - 0s 987us/step - loss: 3075.6458 - val_loss: 1393.4213\n",
            "Epoch 43/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 2919.6387 - val_loss: 1925.5771\n",
            "Epoch 44/500\n",
            "79/79 [==============================] - 0s 964us/step - loss: 2786.9937 - val_loss: 1048.5491\n",
            "Epoch 45/500\n",
            "79/79 [==============================] - 0s 959us/step - loss: 2596.6219 - val_loss: 952.5125\n",
            "Epoch 46/500\n",
            "79/79 [==============================] - 0s 948us/step - loss: 2130.8652 - val_loss: 920.5355\n",
            "Epoch 47/500\n",
            "79/79 [==============================] - 0s 923us/step - loss: 2072.2156 - val_loss: 696.9458\n",
            "Epoch 48/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 2229.7933 - val_loss: 901.3832\n",
            "Epoch 49/500\n",
            "79/79 [==============================] - 0s 916us/step - loss: 2006.4081 - val_loss: 815.9608\n",
            "Epoch 50/500\n",
            "79/79 [==============================] - 0s 932us/step - loss: 1825.9170 - val_loss: 3614.0942\n",
            "Epoch 51/500\n",
            "79/79 [==============================] - 0s 962us/step - loss: 3109.3442 - val_loss: 1935.2623\n",
            "Epoch 52/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 1784.8703 - val_loss: 1786.8912\n",
            "Epoch 53/500\n",
            "79/79 [==============================] - 0s 988us/step - loss: 1677.7200 - val_loss: 1600.3636\n",
            "Epoch 54/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 1551.0171 - val_loss: 1497.3210\n",
            "Epoch 55/500\n",
            "79/79 [==============================] - 0s 975us/step - loss: 1477.5881 - val_loss: 1341.5497\n",
            "Epoch 56/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 3335.3531 - val_loss: 2237.6968\n",
            "Epoch 57/500\n",
            "79/79 [==============================] - 0s 983us/step - loss: 3218.0681 - val_loss: 2240.7339\n",
            "Epoch 58/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 2481.8244 - val_loss: 1338.0637\n",
            "Epoch 59/500\n",
            "79/79 [==============================] - 0s 919us/step - loss: 2132.4997 - val_loss: 1661.3678\n",
            "Epoch 60/500\n",
            "79/79 [==============================] - 0s 936us/step - loss: 2111.5172 - val_loss: 816.1060\n",
            "Epoch 61/500\n",
            "79/79 [==============================] - 0s 988us/step - loss: 1971.4004 - val_loss: 1087.0446\n",
            "Epoch 62/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 1608.1484 - val_loss: 922.8488\n",
            "Epoch 63/500\n",
            "79/79 [==============================] - 0s 970us/step - loss: 1700.5468 - val_loss: 620.5917\n",
            "Epoch 64/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 1353.4393 - val_loss: 410.7238\n",
            "Epoch 65/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 1175.5488 - val_loss: 475.4180\n",
            "Epoch 66/500\n",
            "79/79 [==============================] - 0s 928us/step - loss: 1197.5319 - val_loss: 404.2463\n",
            "Epoch 67/500\n",
            "79/79 [==============================] - 0s 982us/step - loss: 1045.8167 - val_loss: 226.5740\n",
            "Epoch 68/500\n",
            "79/79 [==============================] - 0s 974us/step - loss: 1087.4614 - val_loss: 272.1862\n",
            "Epoch 69/500\n",
            "79/79 [==============================] - 0s 986us/step - loss: 992.1794 - val_loss: 297.5214\n",
            "Epoch 70/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 801.2670 - val_loss: 518.8046\n",
            "Epoch 71/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 809.4791 - val_loss: 507.6036\n",
            "Epoch 72/500\n",
            "79/79 [==============================] - 0s 979us/step - loss: 737.8636 - val_loss: 652.2203\n",
            "Epoch 73/500\n",
            "79/79 [==============================] - 0s 989us/step - loss: 692.3532 - val_loss: 525.7740\n",
            "Epoch 74/500\n",
            "79/79 [==============================] - 0s 995us/step - loss: 573.7901 - val_loss: 393.6711\n",
            "Epoch 75/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 511.8808 - val_loss: 377.4933\n",
            "Epoch 76/500\n",
            "79/79 [==============================] - 0s 885us/step - loss: 553.4167 - val_loss: 269.7110\n",
            "Epoch 77/500\n",
            "79/79 [==============================] - 0s 984us/step - loss: 426.0523 - val_loss: 293.5887\n",
            "Epoch 78/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 348.3478 - val_loss: 227.2884\n",
            "Epoch 79/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 379.6351 - val_loss: 258.7427\n",
            "Epoch 80/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 336.2959 - val_loss: 445.2558\n",
            "Epoch 81/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 398.2241 - val_loss: 493.9136\n",
            "Epoch 82/500\n",
            "79/79 [==============================] - 0s 999us/step - loss: 296.1563 - val_loss: 506.7940\n",
            "Epoch 83/500\n",
            "79/79 [==============================] - 0s 981us/step - loss: 336.5384 - val_loss: 278.3030\n",
            "Epoch 84/500\n",
            "79/79 [==============================] - 0s 967us/step - loss: 265.7836 - val_loss: 152.6789\n",
            "Epoch 85/500\n",
            "79/79 [==============================] - 0s 982us/step - loss: 190.1224 - val_loss: 107.9242\n",
            "Epoch 86/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 206.5034 - val_loss: 166.2890\n",
            "Epoch 87/500\n",
            "79/79 [==============================] - 0s 937us/step - loss: 198.0990 - val_loss: 148.4878\n",
            "Epoch 88/500\n",
            "79/79 [==============================] - 0s 939us/step - loss: 153.9448 - val_loss: 153.8148\n",
            "Epoch 89/500\n",
            "79/79 [==============================] - 0s 985us/step - loss: 162.7783 - val_loss: 130.7386\n",
            "Epoch 90/500\n",
            "79/79 [==============================] - 0s 929us/step - loss: 136.0855 - val_loss: 80.0676\n",
            "Epoch 91/500\n",
            "79/79 [==============================] - 0s 851us/step - loss: 100.8048 - val_loss: 84.3318\n",
            "Epoch 92/500\n",
            "79/79 [==============================] - 0s 896us/step - loss: 94.1898 - val_loss: 78.6765\n",
            "Epoch 93/500\n",
            "79/79 [==============================] - 0s 918us/step - loss: 128.2387 - val_loss: 113.8204\n",
            "Epoch 94/500\n",
            "79/79 [==============================] - 0s 969us/step - loss: 135.2173 - val_loss: 296.4717\n",
            "Epoch 95/500\n",
            "79/79 [==============================] - 0s 982us/step - loss: 157.8352 - val_loss: 201.4558\n",
            "Epoch 96/500\n",
            "79/79 [==============================] - 0s 996us/step - loss: 308.6571 - val_loss: 188.9713\n",
            "Epoch 97/500\n",
            "79/79 [==============================] - 0s 971us/step - loss: 302.5935 - val_loss: 221.2683\n",
            "Epoch 98/500\n",
            "79/79 [==============================] - 0s 1000us/step - loss: 254.9011 - val_loss: 448.9367\n",
            "Epoch 99/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 256.8876 - val_loss: 458.5542\n",
            "Epoch 100/500\n",
            "79/79 [==============================] - 0s 896us/step - loss: 237.5607 - val_loss: 432.8518\n",
            "Epoch 101/500\n",
            "79/79 [==============================] - 0s 914us/step - loss: 251.7631 - val_loss: 225.5062\n",
            "Epoch 102/500\n",
            "79/79 [==============================] - 0s 919us/step - loss: 238.5306 - val_loss: 114.6121\n",
            "Epoch 103/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 82.8181 - val_loss: 82.9303\n",
            "Epoch 104/500\n",
            "79/79 [==============================] - 0s 906us/step - loss: 114.9721 - val_loss: 75.0791\n",
            "Epoch 105/500\n",
            "79/79 [==============================] - 0s 974us/step - loss: 107.0348 - val_loss: 73.3486\n",
            "Epoch 106/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 121.2743 - val_loss: 59.6068\n",
            "Epoch 107/500\n",
            "79/79 [==============================] - 0s 929us/step - loss: 116.5653 - val_loss: 369.8162\n",
            "Epoch 108/500\n",
            "79/79 [==============================] - 0s 903us/step - loss: 94.5867 - val_loss: 435.5035\n",
            "Epoch 109/500\n",
            "79/79 [==============================] - 0s 920us/step - loss: 88.0131 - val_loss: 188.4726\n",
            "Epoch 110/500\n",
            "79/79 [==============================] - 0s 884us/step - loss: 81.3490 - val_loss: 84.0451\n",
            "Epoch 111/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 77.6805 - val_loss: 80.4492\n",
            "Epoch 112/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 59.1015 - val_loss: 62.9099\n",
            "Epoch 113/500\n",
            "79/79 [==============================] - 0s 992us/step - loss: 54.2150 - val_loss: 68.8463\n",
            "Epoch 114/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 48.2638 - val_loss: 51.5881\n",
            "Epoch 115/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 51.9393 - val_loss: 44.7538\n",
            "Epoch 116/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 57.8091 - val_loss: 41.6379\n",
            "Epoch 117/500\n",
            "79/79 [==============================] - 0s 948us/step - loss: 43.2688 - val_loss: 72.2906\n",
            "Epoch 118/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 54.5646 - val_loss: 68.5075\n",
            "Epoch 119/500\n",
            "79/79 [==============================] - 0s 971us/step - loss: 51.0248 - val_loss: 68.9313\n",
            "Epoch 120/500\n",
            "79/79 [==============================] - 0s 980us/step - loss: 51.3511 - val_loss: 44.4037\n",
            "Epoch 121/500\n",
            "79/79 [==============================] - 0s 977us/step - loss: 36.3217 - val_loss: 60.9169\n",
            "Epoch 122/500\n",
            "79/79 [==============================] - 0s 939us/step - loss: 48.6025 - val_loss: 80.0588\n",
            "Epoch 123/500\n",
            "79/79 [==============================] - 0s 926us/step - loss: 45.0906 - val_loss: 35.8423\n",
            "Epoch 124/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 65.6108 - val_loss: 90.3989\n",
            "Epoch 125/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 59.7171 - val_loss: 152.9866\n",
            "Epoch 126/500\n",
            "79/79 [==============================] - 0s 957us/step - loss: 65.4281 - val_loss: 429.0032\n",
            "Epoch 127/500\n",
            "79/79 [==============================] - 0s 904us/step - loss: 93.7533 - val_loss: 304.7660\n",
            "Epoch 128/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 89.7755 - val_loss: 287.0467\n",
            "Epoch 129/500\n",
            "79/79 [==============================] - 0s 925us/step - loss: 64.3906 - val_loss: 273.5483\n",
            "Epoch 130/500\n",
            "79/79 [==============================] - 0s 918us/step - loss: 78.3695 - val_loss: 292.3663\n",
            "Epoch 131/500\n",
            "79/79 [==============================] - 0s 895us/step - loss: 119.2336 - val_loss: 356.3006\n",
            "Epoch 132/500\n",
            "79/79 [==============================] - 0s 908us/step - loss: 171.0717 - val_loss: 313.2486\n",
            "Epoch 133/500\n",
            "79/79 [==============================] - 0s 975us/step - loss: 228.6557 - val_loss: 250.4631\n",
            "Epoch 134/500\n",
            "79/79 [==============================] - 0s 983us/step - loss: 108.5266 - val_loss: 312.5745\n",
            "Epoch 135/500\n",
            "79/79 [==============================] - 0s 903us/step - loss: 176.0364 - val_loss: 285.6215\n",
            "Epoch 136/500\n",
            "79/79 [==============================] - 0s 902us/step - loss: 109.1926 - val_loss: 274.8678\n",
            "Epoch 137/500\n",
            "79/79 [==============================] - 0s 912us/step - loss: 110.5141 - val_loss: 333.6638\n",
            "Epoch 138/500\n",
            "79/79 [==============================] - 0s 880us/step - loss: 117.1175 - val_loss: 327.3199\n",
            "Epoch 139/500\n",
            "79/79 [==============================] - 0s 848us/step - loss: 88.1140 - val_loss: 336.6118\n",
            "Epoch 140/500\n",
            "79/79 [==============================] - 0s 917us/step - loss: 142.9941 - val_loss: 330.9042\n",
            "Epoch 141/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 105.4148 - val_loss: 213.3023\n",
            "Epoch 142/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 74.5394 - val_loss: 168.7147\n",
            "Epoch 143/500\n",
            "79/79 [==============================] - 0s 951us/step - loss: 77.1265 - val_loss: 62.2004\n",
            "Epoch 144/500\n",
            "79/79 [==============================] - 0s 935us/step - loss: 64.6705 - val_loss: 38.9279\n",
            "Epoch 145/500\n",
            "79/79 [==============================] - 0s 971us/step - loss: 53.7575 - val_loss: 64.4976\n",
            "Epoch 146/500\n",
            "79/79 [==============================] - 0s 963us/step - loss: 44.8127 - val_loss: 59.6512\n",
            "Epoch 147/500\n",
            "79/79 [==============================] - 0s 993us/step - loss: 48.3287 - val_loss: 36.7337\n",
            "Epoch 148/500\n",
            "79/79 [==============================] - 0s 930us/step - loss: 43.6942 - val_loss: 55.0359\n",
            "Epoch 149/500\n",
            "79/79 [==============================] - 0s 927us/step - loss: 50.7237 - val_loss: 66.9789\n",
            "Epoch 150/500\n",
            "79/79 [==============================] - 0s 985us/step - loss: 50.1097 - val_loss: 41.5218\n",
            "Epoch 151/500\n",
            "79/79 [==============================] - 0s 911us/step - loss: 44.5510 - val_loss: 51.7183\n",
            "Epoch 152/500\n",
            "79/79 [==============================] - 0s 889us/step - loss: 74.9242 - val_loss: 115.9195\n",
            "Epoch 153/500\n",
            "79/79 [==============================] - 0s 912us/step - loss: 75.0333 - val_loss: 231.7120\n",
            "Epoch 154/500\n",
            "79/79 [==============================] - 0s 931us/step - loss: 64.2003 - val_loss: 291.3118\n",
            "Epoch 155/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 64.3643 - val_loss: 258.2595\n",
            "Epoch 156/500\n",
            "79/79 [==============================] - 0s 976us/step - loss: 52.3836 - val_loss: 241.4031\n",
            "Epoch 157/500\n",
            "79/79 [==============================] - 0s 963us/step - loss: 48.7018 - val_loss: 131.4493\n",
            "Epoch 158/500\n",
            "79/79 [==============================] - 0s 885us/step - loss: 60.4043 - val_loss: 71.0369\n",
            "Epoch 159/500\n",
            "79/79 [==============================] - 0s 917us/step - loss: 58.7901 - val_loss: 67.1119\n",
            "Epoch 160/500\n",
            "79/79 [==============================] - 0s 989us/step - loss: 65.0778 - val_loss: 46.9388\n",
            "Epoch 161/500\n",
            "79/79 [==============================] - 0s 929us/step - loss: 40.0476 - val_loss: 33.0470\n",
            "Epoch 162/500\n",
            "79/79 [==============================] - 0s 922us/step - loss: 45.5306 - val_loss: 59.6505\n",
            "Epoch 163/500\n",
            "79/79 [==============================] - 0s 942us/step - loss: 44.8845 - val_loss: 29.7802\n",
            "Epoch 164/500\n",
            "79/79 [==============================] - 0s 916us/step - loss: 37.4025 - val_loss: 57.3159\n",
            "Epoch 165/500\n",
            "79/79 [==============================] - 0s 896us/step - loss: 32.6104 - val_loss: 51.4417\n",
            "Epoch 166/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 25.3958 - val_loss: 91.7923\n",
            "Epoch 167/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 45.9276 - val_loss: 56.1691\n",
            "Epoch 168/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 34.5714 - val_loss: 42.0592\n",
            "Epoch 169/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 26.3150 - val_loss: 58.0165\n",
            "Epoch 170/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 43.4467 - val_loss: 54.7010\n",
            "Epoch 171/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 43.1600 - val_loss: 63.2860\n",
            "Epoch 172/500\n",
            "79/79 [==============================] - 0s 997us/step - loss: 28.9877 - val_loss: 66.8495\n",
            "Epoch 173/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 32.0764 - val_loss: 54.2124\n",
            "Epoch 174/500\n",
            "79/79 [==============================] - 0s 987us/step - loss: 26.0559 - val_loss: 44.1532\n",
            "Epoch 175/500\n",
            "79/79 [==============================] - 0s 931us/step - loss: 28.6018 - val_loss: 27.2670\n",
            "Epoch 176/500\n",
            "79/79 [==============================] - 0s 958us/step - loss: 20.7837 - val_loss: 86.5757\n",
            "Epoch 177/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 50.9763 - val_loss: 57.9096\n",
            "Epoch 178/500\n",
            "79/79 [==============================] - 0s 984us/step - loss: 39.3233 - val_loss: 65.4097\n",
            "Epoch 179/500\n",
            "79/79 [==============================] - 0s 938us/step - loss: 45.2743 - val_loss: 108.4390\n",
            "Epoch 180/500\n",
            "79/79 [==============================] - 0s 910us/step - loss: 34.6644 - val_loss: 42.4342\n",
            "Epoch 181/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 29.1414 - val_loss: 58.6816\n",
            "Epoch 182/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 28.5829 - val_loss: 47.6745\n",
            "Epoch 183/500\n",
            "79/79 [==============================] - 0s 940us/step - loss: 38.9707 - val_loss: 56.6145\n",
            "Epoch 184/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 32.0685 - val_loss: 56.7455\n",
            "Epoch 185/500\n",
            "79/79 [==============================] - 0s 924us/step - loss: 27.6052 - val_loss: 72.2546\n",
            "Epoch 186/500\n",
            "79/79 [==============================] - 0s 927us/step - loss: 39.3591 - val_loss: 94.6485\n",
            "Epoch 187/500\n",
            "79/79 [==============================] - 0s 957us/step - loss: 28.5715 - val_loss: 84.9434\n",
            "Epoch 188/500\n",
            "79/79 [==============================] - 0s 933us/step - loss: 34.9061 - val_loss: 49.6986\n",
            "Epoch 189/500\n",
            "79/79 [==============================] - 0s 974us/step - loss: 30.2580 - val_loss: 74.3136\n",
            "Epoch 190/500\n",
            "79/79 [==============================] - 0s 905us/step - loss: 26.0409 - val_loss: 53.5398\n",
            "Epoch 191/500\n",
            "79/79 [==============================] - 0s 944us/step - loss: 29.1079 - val_loss: 69.8490\n",
            "Epoch 192/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 32.8066 - val_loss: 65.1195\n",
            "Epoch 193/500\n",
            "79/79 [==============================] - 0s 980us/step - loss: 38.2359 - val_loss: 43.5095\n",
            "Epoch 194/500\n",
            "79/79 [==============================] - 0s 915us/step - loss: 30.0391 - val_loss: 51.5282\n",
            "Epoch 195/500\n",
            "79/79 [==============================] - 0s 923us/step - loss: 28.1074 - val_loss: 47.6895\n",
            "Epoch 196/500\n",
            "79/79 [==============================] - 0s 943us/step - loss: 20.2645 - val_loss: 62.1159\n",
            "Epoch 197/500\n",
            "79/79 [==============================] - 0s 934us/step - loss: 13.2159 - val_loss: 81.8087\n",
            "Epoch 198/500\n",
            "79/79 [==============================] - 0s 905us/step - loss: 21.1620 - val_loss: 69.0109\n",
            "Epoch 199/500\n",
            "79/79 [==============================] - 0s 998us/step - loss: 14.4778 - val_loss: 41.3734\n",
            "Epoch 200/500\n",
            "79/79 [==============================] - 0s 916us/step - loss: 13.8737 - val_loss: 70.5269\n",
            "Epoch 201/500\n",
            "79/79 [==============================] - 0s 961us/step - loss: 22.3009 - val_loss: 53.4459\n",
            "Epoch 202/500\n",
            "79/79 [==============================] - 0s 976us/step - loss: 14.6949 - val_loss: 103.9231\n",
            "Epoch 203/500\n",
            "79/79 [==============================] - 0s 952us/step - loss: 43.2895 - val_loss: 165.0948\n",
            "Epoch 204/500\n",
            "79/79 [==============================] - 0s 920us/step - loss: 53.2623 - val_loss: 77.1919\n",
            "Epoch 205/500\n",
            "79/79 [==============================] - 0s 935us/step - loss: 40.3283 - val_loss: 163.5115\n",
            "Epoch 206/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 73.4639 - val_loss: 107.9842\n",
            "Epoch 207/500\n",
            "79/79 [==============================] - 0s 919us/step - loss: 77.3808 - val_loss: 62.1349\n",
            "Epoch 208/500\n",
            "79/79 [==============================] - 0s 936us/step - loss: 56.1844 - val_loss: 62.6902\n",
            "Epoch 209/500\n",
            "79/79 [==============================] - 0s 977us/step - loss: 64.3814 - val_loss: 140.1803\n",
            "Epoch 210/500\n",
            "79/79 [==============================] - 0s 906us/step - loss: 73.6742 - val_loss: 59.5474\n",
            "Epoch 211/500\n",
            "79/79 [==============================] - 0s 951us/step - loss: 37.7648 - val_loss: 248.9598\n",
            "Epoch 212/500\n",
            "79/79 [==============================] - 0s 981us/step - loss: 236.2719 - val_loss: 202.6459\n",
            "Epoch 213/500\n",
            "79/79 [==============================] - 0s 967us/step - loss: 79.5084 - val_loss: 229.2456\n",
            "Epoch 214/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 101.6063 - val_loss: 158.4725\n",
            "Epoch 215/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 40.2208 - val_loss: 100.8579\n",
            "Epoch 216/500\n",
            "79/79 [==============================] - 0s 976us/step - loss: 51.1819 - val_loss: 96.7430\n",
            "Epoch 217/500\n",
            "79/79 [==============================] - 0s 970us/step - loss: 69.5027 - val_loss: 100.9254\n",
            "Epoch 218/500\n",
            "79/79 [==============================] - 0s 971us/step - loss: 45.6266 - val_loss: 64.2277\n",
            "Epoch 219/500\n",
            "79/79 [==============================] - 0s 967us/step - loss: 51.8515 - val_loss: 98.4739\n",
            "Epoch 220/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 40.6641 - val_loss: 106.7124\n",
            "Epoch 221/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 41.9798 - val_loss: 70.1997\n",
            "Epoch 222/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 41.9680 - val_loss: 68.1096\n",
            "Epoch 223/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 59.7434 - val_loss: 108.2465\n",
            "Epoch 224/500\n",
            "79/79 [==============================] - 0s 982us/step - loss: 44.8385 - val_loss: 117.5452\n",
            "Epoch 225/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 53.1722 - val_loss: 47.8447\n",
            "Epoch 226/500\n",
            "79/79 [==============================] - 0s 978us/step - loss: 99.3436 - val_loss: 143.6240\n",
            "Epoch 227/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 90.9756 - val_loss: 71.6693\n",
            "Epoch 228/500\n",
            "79/79 [==============================] - 0s 963us/step - loss: 48.6333 - val_loss: 154.0918\n",
            "Epoch 229/500\n",
            "79/79 [==============================] - 0s 918us/step - loss: 148.8317 - val_loss: 66.8817\n",
            "Epoch 230/500\n",
            "79/79 [==============================] - 0s 927us/step - loss: 98.2339 - val_loss: 240.3785\n",
            "Epoch 231/500\n",
            "79/79 [==============================] - 0s 972us/step - loss: 126.1156 - val_loss: 136.2638\n",
            "Epoch 232/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 88.1597 - val_loss: 87.5613\n",
            "Epoch 233/500\n",
            "79/79 [==============================] - 0s 954us/step - loss: 61.2040 - val_loss: 91.0509\n",
            "Epoch 234/500\n",
            "79/79 [==============================] - 0s 914us/step - loss: 68.5727 - val_loss: 79.8913\n",
            "Epoch 235/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 57.1642 - val_loss: 74.1243\n",
            "Epoch 236/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 41.5084 - val_loss: 93.2281\n",
            "Epoch 237/500\n",
            "79/79 [==============================] - 0s 972us/step - loss: 54.3117 - val_loss: 77.6131\n",
            "Epoch 238/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 32.8590 - val_loss: 101.8593\n",
            "Epoch 239/500\n",
            "79/79 [==============================] - 0s 934us/step - loss: 37.9995 - val_loss: 95.7103\n",
            "Epoch 240/500\n",
            "79/79 [==============================] - 0s 978us/step - loss: 32.4088 - val_loss: 84.1855\n",
            "Epoch 241/500\n",
            "79/79 [==============================] - 0s 938us/step - loss: 22.0966 - val_loss: 80.8403\n",
            "Epoch 242/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 17.5752 - val_loss: 89.7607\n",
            "Epoch 243/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 17.8859 - val_loss: 78.6764\n",
            "Epoch 244/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 17.0882 - val_loss: 78.8749\n",
            "Epoch 245/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 18.5287 - val_loss: 91.2931\n",
            "Epoch 246/500\n",
            "79/79 [==============================] - 0s 940us/step - loss: 30.3338 - val_loss: 66.4710\n",
            "Epoch 247/500\n",
            "79/79 [==============================] - 0s 991us/step - loss: 31.0314 - val_loss: 82.6967\n",
            "Epoch 248/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 21.8332 - val_loss: 63.6242\n",
            "Epoch 249/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 15.1368 - val_loss: 92.2773\n",
            "Epoch 250/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 25.7843 - val_loss: 75.3628\n",
            "Epoch 251/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 19.6762 - val_loss: 130.8259\n",
            "Epoch 252/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 30.9614 - val_loss: 70.5683\n",
            "Epoch 253/500\n",
            "79/79 [==============================] - 0s 952us/step - loss: 34.8910 - val_loss: 66.2550\n",
            "Epoch 254/500\n",
            "79/79 [==============================] - 0s 990us/step - loss: 25.9234 - val_loss: 71.7624\n",
            "Epoch 255/500\n",
            "79/79 [==============================] - 0s 967us/step - loss: 17.3782 - val_loss: 73.6662\n",
            "Epoch 256/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 13.7010 - val_loss: 86.4698\n",
            "Epoch 257/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 16.2714 - val_loss: 80.6045\n",
            "Epoch 258/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 35.9687 - val_loss: 88.5815\n",
            "Epoch 259/500\n",
            "79/79 [==============================] - 0s 999us/step - loss: 22.2311 - val_loss: 92.1847\n",
            "Epoch 260/500\n",
            "79/79 [==============================] - 0s 977us/step - loss: 30.3695 - val_loss: 113.1455\n",
            "Epoch 261/500\n",
            "79/79 [==============================] - 0s 996us/step - loss: 30.9755 - val_loss: 71.9849\n",
            "Epoch 262/500\n",
            "79/79 [==============================] - 0s 953us/step - loss: 20.7609 - val_loss: 66.6720\n",
            "Epoch 263/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 15.6000 - val_loss: 79.9681\n",
            "Epoch 264/500\n",
            "79/79 [==============================] - 0s 970us/step - loss: 17.6199 - val_loss: 62.5002\n",
            "Epoch 265/500\n",
            "79/79 [==============================] - 0s 937us/step - loss: 13.5487 - val_loss: 70.0038\n",
            "Epoch 266/500\n",
            "79/79 [==============================] - 0s 959us/step - loss: 14.4906 - val_loss: 70.6081\n",
            "Epoch 267/500\n",
            "79/79 [==============================] - 0s 910us/step - loss: 10.0828 - val_loss: 61.4757\n",
            "Epoch 268/500\n",
            "79/79 [==============================] - 0s 912us/step - loss: 11.8231 - val_loss: 61.1260\n",
            "Epoch 269/500\n",
            "79/79 [==============================] - 0s 999us/step - loss: 14.5391 - val_loss: 64.1303\n",
            "Epoch 270/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 12.8789 - val_loss: 68.0803\n",
            "Epoch 271/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 18.9302 - val_loss: 54.5043\n",
            "Epoch 272/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 42.5638 - val_loss: 50.0665\n",
            "Epoch 273/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 46.9469 - val_loss: 90.9559\n",
            "Epoch 274/500\n",
            "79/79 [==============================] - 0s 970us/step - loss: 59.2144 - val_loss: 56.9148\n",
            "Epoch 275/500\n",
            "79/79 [==============================] - 0s 922us/step - loss: 50.0907 - val_loss: 79.9372\n",
            "Epoch 276/500\n",
            "79/79 [==============================] - 0s 958us/step - loss: 35.9950 - val_loss: 71.7379\n",
            "Epoch 277/500\n",
            "79/79 [==============================] - 0s 934us/step - loss: 30.8386 - val_loss: 78.7345\n",
            "Epoch 278/500\n",
            "79/79 [==============================] - 0s 887us/step - loss: 27.0787 - val_loss: 73.8329\n",
            "Epoch 279/500\n",
            "79/79 [==============================] - 0s 930us/step - loss: 25.9297 - val_loss: 99.3381\n",
            "Epoch 280/500\n",
            "79/79 [==============================] - 0s 937us/step - loss: 33.5644 - val_loss: 77.1484\n",
            "Epoch 281/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 20.5122 - val_loss: 122.6033\n",
            "Epoch 282/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 53.7002 - val_loss: 89.0188\n",
            "Epoch 283/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 33.3570 - val_loss: 80.5769\n",
            "Epoch 284/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 32.8508 - val_loss: 49.4946\n",
            "Epoch 285/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 39.5922 - val_loss: 58.6836\n",
            "Epoch 286/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 44.1656 - val_loss: 81.8244\n",
            "Epoch 287/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 52.1859 - val_loss: 56.8387\n",
            "Epoch 288/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 29.3568 - val_loss: 122.1906\n",
            "Epoch 289/500\n",
            "79/79 [==============================] - 0s 957us/step - loss: 65.4404 - val_loss: 48.8776\n",
            "Epoch 290/500\n",
            "79/79 [==============================] - 0s 907us/step - loss: 50.8896 - val_loss: 80.0997\n",
            "Epoch 291/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 59.6897 - val_loss: 51.3733\n",
            "Epoch 292/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 44.9861 - val_loss: 82.6548\n",
            "Epoch 293/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 65.9105 - val_loss: 72.3283\n",
            "Epoch 294/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 53.9190 - val_loss: 74.9600\n",
            "Epoch 295/500\n",
            "79/79 [==============================] - 0s 989us/step - loss: 30.6258 - val_loss: 59.5389\n",
            "Epoch 296/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 40.3666 - val_loss: 75.2830\n",
            "Epoch 297/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 35.6740 - val_loss: 59.1767\n",
            "Epoch 298/500\n",
            "79/79 [==============================] - 0s 972us/step - loss: 41.0137 - val_loss: 77.4485\n",
            "Epoch 299/500\n",
            "79/79 [==============================] - 0s 985us/step - loss: 17.0964 - val_loss: 68.8141\n",
            "Epoch 300/500\n",
            "79/79 [==============================] - 0s 964us/step - loss: 25.9844 - val_loss: 91.1752\n",
            "Epoch 301/500\n",
            "79/79 [==============================] - 0s 901us/step - loss: 32.0572 - val_loss: 85.2120\n",
            "Epoch 302/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 70.4579 - val_loss: 41.0106\n",
            "Epoch 303/500\n",
            "79/79 [==============================] - 0s 961us/step - loss: 23.0534 - val_loss: 52.0733\n",
            "Epoch 304/500\n",
            "79/79 [==============================] - 0s 963us/step - loss: 22.1770 - val_loss: 76.1940\n",
            "Epoch 305/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 26.3825 - val_loss: 84.9238\n",
            "Epoch 306/500\n",
            "79/79 [==============================] - 0s 965us/step - loss: 18.0256 - val_loss: 95.5786\n",
            "Epoch 307/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 17.6457 - val_loss: 175.2097\n",
            "Epoch 308/500\n",
            "79/79 [==============================] - 0s 929us/step - loss: 38.4191 - val_loss: 127.8716\n",
            "Epoch 309/500\n",
            "79/79 [==============================] - 0s 916us/step - loss: 19.0987 - val_loss: 134.0596\n",
            "Epoch 310/500\n",
            "79/79 [==============================] - 0s 996us/step - loss: 19.9508 - val_loss: 169.3615\n",
            "Epoch 311/500\n",
            "79/79 [==============================] - 0s 990us/step - loss: 22.2800 - val_loss: 191.8485\n",
            "Epoch 312/500\n",
            "79/79 [==============================] - 0s 948us/step - loss: 25.9749 - val_loss: 176.7476\n",
            "Epoch 313/500\n",
            "79/79 [==============================] - 0s 900us/step - loss: 36.0732 - val_loss: 164.9651\n",
            "Epoch 314/500\n",
            "79/79 [==============================] - 0s 968us/step - loss: 19.5994 - val_loss: 177.6413\n",
            "Epoch 315/500\n",
            "79/79 [==============================] - 0s 987us/step - loss: 15.6082 - val_loss: 177.0121\n",
            "Epoch 316/500\n",
            "79/79 [==============================] - 0s 914us/step - loss: 28.5683 - val_loss: 177.2522\n",
            "Epoch 317/500\n",
            "79/79 [==============================] - 0s 919us/step - loss: 29.8284 - val_loss: 199.0278\n",
            "Epoch 318/500\n",
            "79/79 [==============================] - 0s 912us/step - loss: 43.0553 - val_loss: 181.8317\n",
            "Epoch 319/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 52.8480 - val_loss: 201.8303\n",
            "Epoch 320/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 45.2329 - val_loss: 169.2339\n",
            "Epoch 321/500\n",
            "79/79 [==============================] - 0s 912us/step - loss: 17.2418 - val_loss: 149.1848\n",
            "Epoch 322/500\n",
            "79/79 [==============================] - 0s 908us/step - loss: 22.1871 - val_loss: 179.1505\n",
            "Epoch 323/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 27.2232 - val_loss: 173.7488\n",
            "Epoch 324/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 24.4932 - val_loss: 183.8217\n",
            "Epoch 325/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 38.9061 - val_loss: 224.0788\n",
            "Epoch 326/500\n",
            "79/79 [==============================] - 0s 993us/step - loss: 35.1214 - val_loss: 152.7553\n",
            "Epoch 327/500\n",
            "79/79 [==============================] - 0s 991us/step - loss: 51.1336 - val_loss: 169.1618\n",
            "Epoch 328/500\n",
            "79/79 [==============================] - 0s 909us/step - loss: 41.6947 - val_loss: 227.0228\n",
            "Epoch 329/500\n",
            "79/79 [==============================] - 0s 944us/step - loss: 104.8539 - val_loss: 184.9512\n",
            "Epoch 330/500\n",
            "79/79 [==============================] - 0s 994us/step - loss: 115.2269 - val_loss: 222.9933\n",
            "Epoch 331/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 240.8464 - val_loss: 337.0725\n",
            "Epoch 332/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 145.9303 - val_loss: 274.4803\n",
            "Epoch 333/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 107.1247 - val_loss: 205.3786\n",
            "Epoch 334/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 93.1514 - val_loss: 225.9460\n",
            "Epoch 335/500\n",
            "79/79 [==============================] - 0s 954us/step - loss: 114.4491 - val_loss: 320.5866\n",
            "Epoch 336/500\n",
            "79/79 [==============================] - 0s 933us/step - loss: 96.5495 - val_loss: 217.4310\n",
            "Epoch 337/500\n",
            "79/79 [==============================] - 0s 958us/step - loss: 205.6320 - val_loss: 278.2718\n",
            "Epoch 338/500\n",
            "79/79 [==============================] - 0s 955us/step - loss: 78.5114 - val_loss: 189.3174\n",
            "Epoch 339/500\n",
            "79/79 [==============================] - 0s 939us/step - loss: 87.9043 - val_loss: 155.7376\n",
            "Epoch 340/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 26.4309 - val_loss: 199.4092\n",
            "Epoch 341/500\n",
            "79/79 [==============================] - 0s 948us/step - loss: 65.6455 - val_loss: 250.9368\n",
            "Epoch 342/500\n",
            "79/79 [==============================] - 0s 959us/step - loss: 70.3244 - val_loss: 159.5310\n",
            "Epoch 343/500\n",
            "79/79 [==============================] - 0s 949us/step - loss: 41.7470 - val_loss: 282.1133\n",
            "Epoch 344/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 97.8394 - val_loss: 149.2552\n",
            "Epoch 345/500\n",
            "79/79 [==============================] - 0s 940us/step - loss: 99.8223 - val_loss: 157.9383\n",
            "Epoch 346/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 56.6491 - val_loss: 221.7613\n",
            "Epoch 347/500\n",
            "79/79 [==============================] - 0s 994us/step - loss: 69.5336 - val_loss: 222.5439\n",
            "Epoch 348/500\n",
            "79/79 [==============================] - 0s 984us/step - loss: 67.1641 - val_loss: 192.1878\n",
            "Epoch 349/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 77.6978 - val_loss: 180.5674\n",
            "Epoch 350/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 64.4773 - val_loss: 135.4207\n",
            "Epoch 351/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 48.3125 - val_loss: 128.6635\n",
            "Epoch 352/500\n",
            "79/79 [==============================] - 0s 998us/step - loss: 24.3846 - val_loss: 76.5100\n",
            "Epoch 353/500\n",
            "79/79 [==============================] - 0s 968us/step - loss: 32.3258 - val_loss: 87.5987\n",
            "Epoch 354/500\n",
            "79/79 [==============================] - 0s 975us/step - loss: 39.5689 - val_loss: 75.1713\n",
            "Epoch 355/500\n",
            "79/79 [==============================] - 0s 981us/step - loss: 20.0831 - val_loss: 45.3845\n",
            "Epoch 356/500\n",
            "79/79 [==============================] - 0s 984us/step - loss: 23.6971 - val_loss: 97.9314\n",
            "Epoch 357/500\n",
            "79/79 [==============================] - 0s 942us/step - loss: 30.8383 - val_loss: 62.5594\n",
            "Epoch 358/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 33.8609 - val_loss: 50.2264\n",
            "Epoch 359/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 17.0302 - val_loss: 57.3114\n",
            "Epoch 360/500\n",
            "79/79 [==============================] - 0s 999us/step - loss: 33.7056 - val_loss: 64.8660\n",
            "Epoch 361/500\n",
            "79/79 [==============================] - 0s 985us/step - loss: 25.5633 - val_loss: 38.1571\n",
            "Epoch 362/500\n",
            "79/79 [==============================] - 0s 922us/step - loss: 16.4937 - val_loss: 36.4387\n",
            "Epoch 363/500\n",
            "79/79 [==============================] - 0s 850us/step - loss: 27.5680 - val_loss: 51.8069\n",
            "Epoch 364/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 14.5481 - val_loss: 28.8410\n",
            "Epoch 365/500\n",
            "79/79 [==============================] - 0s 968us/step - loss: 19.9392 - val_loss: 79.7099\n",
            "Epoch 366/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 22.9577 - val_loss: 40.7905\n",
            "Epoch 367/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 21.9315 - val_loss: 73.9996\n",
            "Epoch 368/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 20.7351 - val_loss: 34.6269\n",
            "Epoch 369/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 16.2698 - val_loss: 45.1450\n",
            "Epoch 370/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 8.9245 - val_loss: 49.9463\n",
            "Epoch 371/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 14.0119 - val_loss: 52.6670\n",
            "Epoch 372/500\n",
            "79/79 [==============================] - 0s 963us/step - loss: 11.9605 - val_loss: 40.5603\n",
            "Epoch 373/500\n",
            "79/79 [==============================] - 0s 903us/step - loss: 11.5288 - val_loss: 37.9646\n",
            "Epoch 374/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 11.3797 - val_loss: 45.7638\n",
            "Epoch 375/500\n",
            "79/79 [==============================] - 0s 898us/step - loss: 10.5483 - val_loss: 50.3222\n",
            "Epoch 376/500\n",
            "79/79 [==============================] - 0s 904us/step - loss: 15.6834 - val_loss: 56.0116\n",
            "Epoch 377/500\n",
            "79/79 [==============================] - 0s 994us/step - loss: 16.3755 - val_loss: 36.8899\n",
            "Epoch 378/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 7.9385 - val_loss: 51.6471\n",
            "Epoch 379/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 11.2624 - val_loss: 43.0142\n",
            "Epoch 380/500\n",
            "79/79 [==============================] - 0s 903us/step - loss: 8.3097 - val_loss: 42.7847\n",
            "Epoch 381/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 6.4538 - val_loss: 57.8855\n",
            "Epoch 382/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 14.6711 - val_loss: 36.6148\n",
            "Epoch 383/500\n",
            "79/79 [==============================] - 0s 997us/step - loss: 12.5823 - val_loss: 55.4296\n",
            "Epoch 384/500\n",
            "79/79 [==============================] - 0s 936us/step - loss: 14.7772 - val_loss: 34.6394\n",
            "Epoch 385/500\n",
            "79/79 [==============================] - 0s 936us/step - loss: 6.8818 - val_loss: 45.3629\n",
            "Epoch 386/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 16.3127 - val_loss: 39.2700\n",
            "Epoch 387/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 15.6279 - val_loss: 45.2614\n",
            "Epoch 388/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 20.2235 - val_loss: 61.1649\n",
            "Epoch 389/500\n",
            "79/79 [==============================] - 0s 948us/step - loss: 11.5684 - val_loss: 43.2594\n",
            "Epoch 390/500\n",
            "79/79 [==============================] - 0s 953us/step - loss: 38.2796 - val_loss: 93.1844\n",
            "Epoch 391/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 21.8051 - val_loss: 37.4899\n",
            "Epoch 392/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 19.7901 - val_loss: 45.0032\n",
            "Epoch 393/500\n",
            "79/79 [==============================] - 0s 975us/step - loss: 15.6737 - val_loss: 46.3176\n",
            "Epoch 394/500\n",
            "79/79 [==============================] - 0s 928us/step - loss: 21.7113 - val_loss: 41.0342\n",
            "Epoch 395/500\n",
            "79/79 [==============================] - 0s 899us/step - loss: 10.2320 - val_loss: 51.9168\n",
            "Epoch 396/500\n",
            "79/79 [==============================] - 0s 965us/step - loss: 17.0136 - val_loss: 50.6713\n",
            "Epoch 397/500\n",
            "79/79 [==============================] - 0s 949us/step - loss: 23.7351 - val_loss: 77.6308\n",
            "Epoch 398/500\n",
            "79/79 [==============================] - 0s 951us/step - loss: 13.8193 - val_loss: 106.9608\n",
            "Epoch 399/500\n",
            "79/79 [==============================] - 0s 976us/step - loss: 20.7254 - val_loss: 90.5947\n",
            "Epoch 400/500\n",
            "79/79 [==============================] - 0s 959us/step - loss: 14.5698 - val_loss: 64.2257\n",
            "Epoch 401/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 13.0226 - val_loss: 45.7710\n",
            "Epoch 402/500\n",
            "79/79 [==============================] - 0s 941us/step - loss: 11.6596 - val_loss: 48.6848\n",
            "Epoch 403/500\n",
            "79/79 [==============================] - 0s 966us/step - loss: 11.4564 - val_loss: 62.3391\n",
            "Epoch 404/500\n",
            "79/79 [==============================] - 0s 946us/step - loss: 13.5519 - val_loss: 57.6275\n",
            "Epoch 405/500\n",
            "79/79 [==============================] - 0s 898us/step - loss: 6.8988 - val_loss: 43.5585\n",
            "Epoch 406/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 5.8904 - val_loss: 34.7780\n",
            "Epoch 407/500\n",
            "79/79 [==============================] - 0s 960us/step - loss: 7.0772 - val_loss: 30.2373\n",
            "Epoch 408/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 7.6747 - val_loss: 36.5860\n",
            "Epoch 409/500\n",
            "79/79 [==============================] - 0s 993us/step - loss: 7.4384 - val_loss: 36.4556\n",
            "Epoch 410/500\n",
            "79/79 [==============================] - 0s 920us/step - loss: 5.6225 - val_loss: 44.2247\n",
            "Epoch 411/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 4.7826 - val_loss: 43.6061\n",
            "Epoch 412/500\n",
            "79/79 [==============================] - 0s 997us/step - loss: 4.0219 - val_loss: 43.8603\n",
            "Epoch 413/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 2.8864 - val_loss: 42.4016\n",
            "Epoch 414/500\n",
            "79/79 [==============================] - 0s 972us/step - loss: 2.8820 - val_loss: 39.2528\n",
            "Epoch 415/500\n",
            "79/79 [==============================] - 0s 945us/step - loss: 3.0759 - val_loss: 35.9925\n",
            "Epoch 416/500\n",
            "79/79 [==============================] - 0s 940us/step - loss: 4.6034 - val_loss: 51.8479\n",
            "Epoch 417/500\n",
            "79/79 [==============================] - 0s 917us/step - loss: 4.6259 - val_loss: 39.8474\n",
            "Epoch 418/500\n",
            "79/79 [==============================] - 0s 930us/step - loss: 5.2221 - val_loss: 60.9629\n",
            "Epoch 419/500\n",
            "79/79 [==============================] - 0s 992us/step - loss: 12.0868 - val_loss: 38.3681\n",
            "Epoch 420/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 12.6258 - val_loss: 50.5664\n",
            "Epoch 421/500\n",
            "79/79 [==============================] - 0s 921us/step - loss: 8.4208 - val_loss: 43.9496\n",
            "Epoch 422/500\n",
            "79/79 [==============================] - 0s 916us/step - loss: 14.8882 - val_loss: 38.2639\n",
            "Epoch 423/500\n",
            "79/79 [==============================] - 0s 975us/step - loss: 7.9658 - val_loss: 40.1367\n",
            "Epoch 424/500\n",
            "79/79 [==============================] - 0s 954us/step - loss: 10.5454 - val_loss: 48.4193\n",
            "Epoch 425/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 11.9940 - val_loss: 76.5775\n",
            "Epoch 426/500\n",
            "79/79 [==============================] - 0s 980us/step - loss: 11.8057 - val_loss: 80.2301\n",
            "Epoch 427/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 12.3277 - val_loss: 96.7610\n",
            "Epoch 428/500\n",
            "79/79 [==============================] - 0s 951us/step - loss: 12.0428 - val_loss: 86.5187\n",
            "Epoch 429/500\n",
            "79/79 [==============================] - 0s 984us/step - loss: 4.3506 - val_loss: 97.2775\n",
            "Epoch 430/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 3.5992 - val_loss: 107.2012\n",
            "Epoch 431/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 3.3682 - val_loss: 111.3104\n",
            "Epoch 432/500\n",
            "79/79 [==============================] - 0s 926us/step - loss: 4.2012 - val_loss: 104.2344\n",
            "Epoch 433/500\n",
            "79/79 [==============================] - 0s 986us/step - loss: 5.1029 - val_loss: 101.8418\n",
            "Epoch 434/500\n",
            "79/79 [==============================] - 0s 988us/step - loss: 3.6958 - val_loss: 112.3419\n",
            "Epoch 435/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 4.1539 - val_loss: 108.2702\n",
            "Epoch 436/500\n",
            "79/79 [==============================] - 0s 973us/step - loss: 3.5137 - val_loss: 113.9683\n",
            "Epoch 437/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 4.7797 - val_loss: 110.8969\n",
            "Epoch 438/500\n",
            "79/79 [==============================] - 0s 932us/step - loss: 3.9687 - val_loss: 105.6051\n",
            "Epoch 439/500\n",
            "79/79 [==============================] - 0s 906us/step - loss: 3.5568 - val_loss: 110.5312\n",
            "Epoch 440/500\n",
            "79/79 [==============================] - 0s 969us/step - loss: 4.9834 - val_loss: 103.7832\n",
            "Epoch 441/500\n",
            "79/79 [==============================] - 0s 927us/step - loss: 10.2640 - val_loss: 116.2134\n",
            "Epoch 442/500\n",
            "79/79 [==============================] - 0s 917us/step - loss: 5.6953 - val_loss: 96.7617\n",
            "Epoch 443/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 6.8889 - val_loss: 105.8852\n",
            "Epoch 444/500\n",
            "79/79 [==============================] - 0s 909us/step - loss: 7.7154 - val_loss: 97.8688\n",
            "Epoch 445/500\n",
            "79/79 [==============================] - 0s 989us/step - loss: 8.3565 - val_loss: 108.1200\n",
            "Epoch 446/500\n",
            "79/79 [==============================] - 0s 928us/step - loss: 7.1390 - val_loss: 99.0568\n",
            "Epoch 447/500\n",
            "79/79 [==============================] - 0s 975us/step - loss: 8.3968 - val_loss: 111.8859\n",
            "Epoch 448/500\n",
            "79/79 [==============================] - 0s 964us/step - loss: 8.0220 - val_loss: 112.2423\n",
            "Epoch 449/500\n",
            "79/79 [==============================] - 0s 990us/step - loss: 8.3878 - val_loss: 109.0497\n",
            "Epoch 450/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 5.8938 - val_loss: 106.8186\n",
            "Epoch 451/500\n",
            "79/79 [==============================] - 0s 932us/step - loss: 5.3624 - val_loss: 102.6791\n",
            "Epoch 452/500\n",
            "79/79 [==============================] - 0s 971us/step - loss: 5.6626 - val_loss: 110.8014\n",
            "Epoch 453/500\n",
            "79/79 [==============================] - 0s 996us/step - loss: 5.7482 - val_loss: 90.1231\n",
            "Epoch 454/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 8.0846 - val_loss: 92.6093\n",
            "Epoch 455/500\n",
            "79/79 [==============================] - 0s 946us/step - loss: 7.5410 - val_loss: 102.7382\n",
            "Epoch 456/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 7.5877 - val_loss: 93.3996\n",
            "Epoch 457/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 8.2459 - val_loss: 107.3722\n",
            "Epoch 458/500\n",
            "79/79 [==============================] - 0s 947us/step - loss: 6.3611 - val_loss: 88.2486\n",
            "Epoch 459/500\n",
            "79/79 [==============================] - 0s 958us/step - loss: 6.7023 - val_loss: 99.7096\n",
            "Epoch 460/500\n",
            "79/79 [==============================] - 0s 997us/step - loss: 6.2717 - val_loss: 92.4526\n",
            "Epoch 461/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 8.5775 - val_loss: 88.4594\n",
            "Epoch 462/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 19.4639 - val_loss: 128.5150\n",
            "Epoch 463/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 15.3743 - val_loss: 78.1464\n",
            "Epoch 464/500\n",
            "79/79 [==============================] - 0s 981us/step - loss: 16.1939 - val_loss: 90.7247\n",
            "Epoch 465/500\n",
            "79/79 [==============================] - 0s 936us/step - loss: 7.1008 - val_loss: 79.0929\n",
            "Epoch 466/500\n",
            "79/79 [==============================] - 0s 994us/step - loss: 9.4047 - val_loss: 112.6027\n",
            "Epoch 467/500\n",
            "79/79 [==============================] - 0s 952us/step - loss: 8.4775 - val_loss: 111.7512\n",
            "Epoch 468/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 12.1864 - val_loss: 91.3047\n",
            "Epoch 469/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 24.0325 - val_loss: 150.1651\n",
            "Epoch 470/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 23.3493 - val_loss: 100.1697\n",
            "Epoch 471/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 37.3933 - val_loss: 100.9052\n",
            "Epoch 472/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 27.0028 - val_loss: 161.4942\n",
            "Epoch 473/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 41.7076 - val_loss: 102.3479\n",
            "Epoch 474/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 42.4191 - val_loss: 170.9056\n",
            "Epoch 475/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 43.3975 - val_loss: 87.4721\n",
            "Epoch 476/500\n",
            "79/79 [==============================] - 0s 993us/step - loss: 36.3695 - val_loss: 78.4617\n",
            "Epoch 477/500\n",
            "79/79 [==============================] - 0s 923us/step - loss: 44.9712 - val_loss: 121.3921\n",
            "Epoch 478/500\n",
            "79/79 [==============================] - 0s 992us/step - loss: 44.2562 - val_loss: 98.2531\n",
            "Epoch 479/500\n",
            "79/79 [==============================] - 0s 942us/step - loss: 39.2767 - val_loss: 135.6523\n",
            "Epoch 480/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 59.6634 - val_loss: 109.3098\n",
            "Epoch 481/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 41.4411 - val_loss: 73.9842\n",
            "Epoch 482/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 24.4583 - val_loss: 98.6412\n",
            "Epoch 483/500\n",
            "79/79 [==============================] - 0s 931us/step - loss: 43.3643 - val_loss: 96.3101\n",
            "Epoch 484/500\n",
            "79/79 [==============================] - 0s 964us/step - loss: 42.4754 - val_loss: 151.8083\n",
            "Epoch 485/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 49.3692 - val_loss: 99.9848\n",
            "Epoch 486/500\n",
            "79/79 [==============================] - 0s 956us/step - loss: 29.4163 - val_loss: 124.8160\n",
            "Epoch 487/500\n",
            "79/79 [==============================] - 0s 953us/step - loss: 26.8063 - val_loss: 104.7209\n",
            "Epoch 488/500\n",
            "79/79 [==============================] - 0s 941us/step - loss: 27.2711 - val_loss: 100.8749\n",
            "Epoch 489/500\n",
            "79/79 [==============================] - 0s 991us/step - loss: 21.4762 - val_loss: 131.6356\n",
            "Epoch 490/500\n",
            "79/79 [==============================] - 0s 958us/step - loss: 27.4638 - val_loss: 103.1183\n",
            "Epoch 491/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 23.4345 - val_loss: 123.9791\n",
            "Epoch 492/500\n",
            "79/79 [==============================] - 0s 996us/step - loss: 13.0756 - val_loss: 80.1626\n",
            "Epoch 493/500\n",
            "79/79 [==============================] - 0s 974us/step - loss: 21.0789 - val_loss: 85.2828\n",
            "Epoch 494/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 16.0613 - val_loss: 112.7837\n",
            "Epoch 495/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 32.4342 - val_loss: 86.2332\n",
            "Epoch 496/500\n",
            "79/79 [==============================] - 0s 994us/step - loss: 18.1647 - val_loss: 72.1493\n",
            "Epoch 497/500\n",
            "79/79 [==============================] - 0s 968us/step - loss: 16.7970 - val_loss: 109.0482\n",
            "Epoch 498/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 25.8644 - val_loss: 103.9443\n",
            "Epoch 499/500\n",
            "79/79 [==============================] - 0s 1ms/step - loss: 13.9774 - val_loss: 84.6739\n",
            "Epoch 500/500\n",
            "79/79 [==============================] - 0s 946us/step - loss: 24.5652 - val_loss: 70.0569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kPKizNRLWFPs",
        "colab": {}
      },
      "source": [
        "results = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MJ36b2CcmjXY",
        "outputId": "55909f4d-ca8a-475b-c048-cc4dc02cca9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ezulvCPMmjU-",
        "outputId": "2600f3df-8550-44ee-a7ad-333e051e17e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0pwrsM6UmpMa",
        "outputId": "bf188abd-7281-4607-a6a4-0125ddabee15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "plt.scatter(range(20),results,c='r')\n",
        "plt.scatter(range(20),y_test,c='g')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICd/M7MC+v84S6opk69MyAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WmOp_UZRrEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}