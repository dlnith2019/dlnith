{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15MI403.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwkDYZlKKd7M",
        "colab_type": "code",
        "outputId": "283a730e-b272-49f4-f840-2efa0c5b5a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,LSTM\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8P7R0wBK4G2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=[]\n",
        "Y1=[]\n",
        "for i in range(100):\n",
        "  X.append(i)\n",
        "  Y1.append(i+5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0V0FVCBLyMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y2=[]\n",
        "for i in range(100):\n",
        "  Y2.append(i*i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGoORbOENY7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "inppt = np.column_stack((Y1, Y2))\n",
        "tstt = [Y1*Y2 for Y1,Y2 in zip(Y1,Y2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP5mFr44NgR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inppt = np.array(inppt, dtype=\"float32\")\n",
        "trgt = np.array(tstt, dtype=\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNHZUjYoPX71",
        "colab_type": "code",
        "outputId": "90edae63-7a60-4a00-ba15-03c5ce869f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inppt= np.array(inppt).reshape(100, 2,1)\n",
        "inppt.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmrwEw6DkEWc",
        "colab_type": "code",
        "outputId": "e8a50913-61b8-410f-80c9-b01d36a0d1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trgt.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medTU78GkGQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(inppt,trgt,test_size=0.2,random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C8wYedGkNQP",
        "colab_type": "code",
        "outputId": "7ea8765f-a274-4e9b-f3b1-d924fea19766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(2,1)))\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(LSTM(25, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 2, 200)            161600    \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 2, 100)            120400    \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 2, 50)             30200     \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 25)                7600      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 20)                520       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 320,541\n",
            "Trainable params: 320,541\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhEyK56qkdZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "294a7468-4ad4-46a0-97fe-d8cbb2f562e2"
      },
      "source": [
        "history = model.fit(x_train,y_train,epochs=1000,validation_data=(x_test,y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/1000\n",
            "80/80 [==============================] - 5s 65ms/step - loss: 1175143049068544.0000 - val_loss: 1225684882030592.0000\n",
            "Epoch 2/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1175131560031027.2500 - val_loss: 1225661125492736.0000\n",
            "Epoch 3/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 1175103723274240.0000 - val_loss: 1225641663922176.0000\n",
            "Epoch 4/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1175129103846604.7500 - val_loss: 1225597640507392.0000\n",
            "Epoch 5/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1175015971723673.7500 - val_loss: 1225417788751872.0000\n",
            "Epoch 6/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1174881029219942.2500 - val_loss: 1225020370059264.0000\n",
            "Epoch 7/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1174644477185228.7500 - val_loss: 1225198745419776.0000\n",
            "Epoch 8/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 1174000144849305.7500 - val_loss: 1223268828708864.0000\n",
            "Epoch 9/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1172256482079539.2500 - val_loss: 1222253740032000.0000\n",
            "Epoch 10/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 1170721594985676.7500 - val_loss: 1220087734337536.0000\n",
            "Epoch 11/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 1168668788523008.0000 - val_loss: 1217733018517504.0000\n",
            "Epoch 12/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 1165955523464396.7500 - val_loss: 1213838288486400.0000\n",
            "Epoch 13/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 1161704324569497.7500 - val_loss: 1208102896533504.0000\n",
            "Epoch 14/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1153743777169408.0000 - val_loss: 1198709534621696.0000\n",
            "Epoch 15/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1146211484984934.2500 - val_loss: 1188778093838336.0000\n",
            "Epoch 16/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 1132106107741798.2500 - val_loss: 1172213344501760.0000\n",
            "Epoch 17/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1112487166895718.3750 - val_loss: 1144683677876224.0000\n",
            "Epoch 18/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 1081820040082227.1250 - val_loss: 1114939351629824.0000\n",
            "Epoch 19/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 1045627451264204.8750 - val_loss: 1071478711779328.0000\n",
            "Epoch 20/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1005086771773440.0000 - val_loss: 1016586009837568.0000\n",
            "Epoch 21/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 942164776111308.8750 - val_loss: 941715938607104.0000\n",
            "Epoch 22/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 855478578760908.8750 - val_loss: 851954209128448.0000\n",
            "Epoch 23/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 758233445905203.1250 - val_loss: 728018565726208.0000\n",
            "Epoch 24/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 630940533941862.3750 - val_loss: 587772582690816.0000\n",
            "Epoch 25/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 484223363946905.5625 - val_loss: 428205253066752.0000\n",
            "Epoch 26/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 330536669990092.8125 - val_loss: 263439066857472.0000\n",
            "Epoch 27/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 171357152424755.2188 - val_loss: 117916984934400.0000\n",
            "Epoch 28/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 57630422099558.3984 - val_loss: 23696100032512.0000\n",
            "Epoch 29/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 20343100434022.4023 - val_loss: 14534343393280.0000\n",
            "Epoch 30/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 55211690505011.1953 - val_loss: 28022153412608.0000\n",
            "Epoch 31/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 69770623005491.1953 - val_loss: 19713608384512.0000\n",
            "Epoch 32/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 47114267865907.1953 - val_loss: 11348299743232.0000\n",
            "Epoch 33/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 21244261328486.4023 - val_loss: 39208093745152.0000\n",
            "Epoch 34/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 28144847709798.4023 - val_loss: 55738240073728.0000\n",
            "Epoch 35/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 33385924159078.4023 - val_loss: 55801108496384.0000\n",
            "Epoch 36/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 31618400623001.5977 - val_loss: 74642765119488.0000\n",
            "Epoch 37/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 27512652850790.4023 - val_loss: 30418808078336.0000\n",
            "Epoch 38/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 20192710308659.1992 - val_loss: 31529526886400.0000\n",
            "Epoch 39/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 140845075608371.2188 - val_loss: 240880019570688.0000\n",
            "Epoch 40/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 82602421649408.0000 - val_loss: 12721960517632.0000\n",
            "Epoch 41/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 41568928215859.1953 - val_loss: 205865130917888.0000\n",
            "Epoch 42/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 118059816085094.3906 - val_loss: 32916937637888.0000\n",
            "Epoch 43/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 19763819236556.8008 - val_loss: 11205693407232.0000\n",
            "Epoch 44/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 24780234293248.0000 - val_loss: 25375023300608.0000\n",
            "Epoch 45/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 16736506085376.0000 - val_loss: 23264847986688.0000\n",
            "Epoch 46/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 16504558557593.5996 - val_loss: 23438016118784.0000\n",
            "Epoch 47/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 29486955364352.0000 - val_loss: 16489279127552.0000\n",
            "Epoch 48/1000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 45625885537075.1953 - val_loss: 104225342226432.0000\n",
            "Epoch 49/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 46781892342579.1953 - val_loss: 13178796769280.0000\n",
            "Epoch 50/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 19396947659980.8008 - val_loss: 26752338362368.0000\n",
            "Epoch 51/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 17054290463948.7988 - val_loss: 45399104552960.0000\n",
            "Epoch 52/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 23448699325644.8008 - val_loss: 52765116071936.0000\n",
            "Epoch 53/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 29882381343129.5977 - val_loss: 40530121588736.0000\n",
            "Epoch 54/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 19905496114790.4023 - val_loss: 69290325704704.0000\n",
            "Epoch 55/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 35461288427520.0000 - val_loss: 49339950956544.0000\n",
            "Epoch 56/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19581746597068.8008 - val_loss: 11772398403584.0000\n",
            "Epoch 57/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 23082561018265.5977 - val_loss: 153605847908352.0000\n",
            "Epoch 58/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 121137609782067.2031 - val_loss: 138112055377920.0000\n",
            "Epoch 59/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 84382671332966.3906 - val_loss: 64259044147200.0000\n",
            "Epoch 60/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 31168614576947.1992 - val_loss: 27944443445248.0000\n",
            "Epoch 61/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 17316767911116.7988 - val_loss: 14267805859840.0000\n",
            "Epoch 62/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 19338620829696.0000 - val_loss: 31223166533632.0000\n",
            "Epoch 63/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 21064139879219.1992 - val_loss: 11652449697792.0000\n",
            "Epoch 64/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 245410602732748.7812 - val_loss: 433239357390848.0000\n",
            "Epoch 65/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 353966380967526.4375 - val_loss: 357902208466944.0000\n",
            "Epoch 66/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 276953325738393.5938 - val_loss: 232181502836736.0000\n",
            "Epoch 67/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 163310629342412.7812 - val_loss: 141395406880768.0000\n",
            "Epoch 68/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 80845882366361.6094 - val_loss: 69585449517056.0000\n",
            "Epoch 69/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 35238883360768.0000 - val_loss: 24926937415680.0000\n",
            "Epoch 70/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17649406836736.0000 - val_loss: 11399375880192.0000\n",
            "Epoch 71/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 27715290582220.8008 - val_loss: 13202940231680.0000\n",
            "Epoch 72/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 39530984269414.3984 - val_loss: 12174822998016.0000\n",
            "Epoch 73/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 19516938728243.1992 - val_loss: 14570797137920.0000\n",
            "Epoch 74/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 18207776492748.8008 - val_loss: 90460307587072.0000\n",
            "Epoch 75/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 65635337057075.1953 - val_loss: 35905842708480.0000\n",
            "Epoch 76/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 20298215509196.8008 - val_loss: 25974588571648.0000\n",
            "Epoch 77/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 16196634148864.0000 - val_loss: 19729911644160.0000\n",
            "Epoch 78/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 16628275006668.7988 - val_loss: 15379095093248.0000\n",
            "Epoch 79/1000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 17404353786675.2012 - val_loss: 23711570722816.0000\n",
            "Epoch 80/1000\n",
            "80/80 [==============================] - 0s 909us/step - loss: 17348104814592.0000 - val_loss: 36240019685376.0000\n",
            "Epoch 81/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 21088918778675.1992 - val_loss: 54741874769920.0000\n",
            "Epoch 82/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 29954480170598.4023 - val_loss: 41331929907200.0000\n",
            "Epoch 83/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 21117324073369.5977 - val_loss: 11234451652608.0000\n",
            "Epoch 84/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 143346632333721.5938 - val_loss: 356227037003776.0000\n",
            "Epoch 85/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 192695554906521.5938 - val_loss: 12647717142528.0000\n",
            "Epoch 86/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 30335234893414.4023 - val_loss: 11216707649536.0000\n",
            "Epoch 87/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 40216681250816.0000 - val_loss: 208422565838848.0000\n",
            "Epoch 88/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 148046529481932.7812 - val_loss: 167100819701760.0000\n",
            "Epoch 89/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 107235223430758.3906 - val_loss: 106808916049920.0000\n",
            "Epoch 90/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 58663111124582.3984 - val_loss: 166662867255296.0000\n",
            "Epoch 91/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 68412010345267.1953 - val_loss: 677769595846656.0000\n",
            "Epoch 92/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 589941910274048.0000 - val_loss: 588113965481984.0000\n",
            "Epoch 93/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 498638844539699.1875 - val_loss: 477084799467520.0000\n",
            "Epoch 94/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 377261458142003.1875 - val_loss: 353537112408064.0000\n",
            "Epoch 95/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 252750413273497.5938 - val_loss: 229433361301504.0000\n",
            "Epoch 96/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 150747617637171.2188 - val_loss: 119161116164096.0000\n",
            "Epoch 97/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 61127379648512.0000 - val_loss: 43491316989952.0000\n",
            "Epoch 98/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 23996319504793.5977 - val_loss: 12722496339968.0000\n",
            "Epoch 99/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 25947396479385.5977 - val_loss: 14497454489600.0000\n",
            "Epoch 100/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 46400886380953.6016 - val_loss: 228580223090688.0000\n",
            "Epoch 101/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 224017106599936.0000 - val_loss: 310580191690752.0000\n",
            "Epoch 102/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 255585323214438.4062 - val_loss: 308477905862656.0000\n",
            "Epoch 103/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 239175602064588.7812 - val_loss: 259049744498688.0000\n",
            "Epoch 104/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 187102562929868.7812 - val_loss: 192408662835200.0000\n",
            "Epoch 105/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 179970944217907.2188 - val_loss: 121823341576192.0000\n",
            "Epoch 106/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 69203326192844.8047 - val_loss: 65765336154112.0000\n",
            "Epoch 107/1000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 30801781168537.5977 - val_loss: 29512863580160.0000\n",
            "Epoch 108/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 32482105334169.5977 - val_loss: 61718902341632.0000\n",
            "Epoch 109/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 26752285933568.0000 - val_loss: 16956181708800.0000\n",
            "Epoch 110/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 19761773674496.0000 - val_loss: 417493973729280.0000\n",
            "Epoch 111/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 329014721485209.5625 - val_loss: 21306296762368.0000\n",
            "Epoch 112/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 67658801007820.8047 - val_loss: 32121754222592.0000\n",
            "Epoch 113/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 78135348638515.2031 - val_loss: 24190769954816.0000\n",
            "Epoch 114/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 56837701369856.0000 - val_loss: 12851112574976.0000\n",
            "Epoch 115/1000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 30045581711769.5977 - val_loss: 22053742706688.0000\n",
            "Epoch 116/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 17123629229670.4004 - val_loss: 35557296046080.0000\n",
            "Epoch 117/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 21988209852416.0000 - val_loss: 44864628588544.0000\n",
            "Epoch 118/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26493363997900.8008 - val_loss: 47825123540992.0000\n",
            "Epoch 119/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 27264502307225.5977 - val_loss: 42022769524736.0000\n",
            "Epoch 120/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 211660891907686.4062 - val_loss: 266678143287296.0000\n",
            "Epoch 121/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 208374277655756.7812 - val_loss: 16846601322496.0000\n",
            "Epoch 122/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 17987509262745.5977 - val_loss: 24185535463424.0000\n",
            "Epoch 123/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 116947931260518.3906 - val_loss: 246344425930752.0000\n",
            "Epoch 124/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 179305792195788.7812 - val_loss: 174911972704256.0000\n",
            "Epoch 125/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 111379428986060.7969 - val_loss: 103833065750528.0000\n",
            "Epoch 126/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 55793698838937.6016 - val_loss: 396564128333824.0000\n",
            "Epoch 127/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 169715333018419.2188 - val_loss: 13717563506688.0000\n",
            "Epoch 128/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 279586838203596.7812 - val_loss: 692451673112576.0000\n",
            "Epoch 129/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 578459348762624.0000 - val_loss: 559386808287232.0000\n",
            "Epoch 130/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 439760949818163.1875 - val_loss: 198290368888832.0000\n",
            "Epoch 131/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 222578210976563.2188 - val_loss: 235632878157824.0000\n",
            "Epoch 132/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 147048702882611.2188 - val_loss: 92471946117120.0000\n",
            "Epoch 133/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 42688270932377.6016 - val_loss: 29750577856512.0000\n",
            "Epoch 134/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 20784425939763.1992 - val_loss: 12168806268928.0000\n",
            "Epoch 135/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 43693429594521.6016 - val_loss: 23562840702976.0000\n",
            "Epoch 136/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 370809343521587.1875 - val_loss: 852294249742336.0000\n",
            "Epoch 137/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 675155168722944.0000 - val_loss: 13841808228352.0000\n",
            "Epoch 138/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 28748834196684.8008 - val_loss: 31655234371584.0000\n",
            "Epoch 139/1000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 29379458288844.8008 - val_loss: 76805641863168.0000\n",
            "Epoch 140/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 56100175793356.8047 - val_loss: 107555057565696.0000\n",
            "Epoch 141/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 75984809020620.7969 - val_loss: 125390580350976.0000\n",
            "Epoch 142/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 84459894761062.3906 - val_loss: 119400426373120.0000\n",
            "Epoch 143/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 81505563757772.7969 - val_loss: 122372250140672.0000\n",
            "Epoch 144/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 77206850528870.3906 - val_loss: 102737370939392.0000\n",
            "Epoch 145/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 61670777736396.8047 - val_loss: 78222964293632.0000\n",
            "Epoch 146/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 44883802429849.6016 - val_loss: 55215243919360.0000\n",
            "Epoch 147/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 30795500355584.0000 - val_loss: 37146815627264.0000\n",
            "Epoch 148/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 19679724699648.0000 - val_loss: 25309202087936.0000\n",
            "Epoch 149/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16942953922560.0000 - val_loss: 12102528925696.0000\n",
            "Epoch 150/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 34216784743628.8008 - val_loss: 12445604118528.0000\n",
            "Epoch 151/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 33743223495065.5977 - val_loss: 251685620416512.0000\n",
            "Epoch 152/1000\n",
            "80/80 [==============================] - 0s 1000us/step - loss: 299298218573824.0000 - val_loss: 384343436427264.0000\n",
            "Epoch 153/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 304253013642444.8125 - val_loss: 297732602331136.0000\n",
            "Epoch 154/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 233068386739814.4062 - val_loss: 229574910672896.0000\n",
            "Epoch 155/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 154485807015526.4062 - val_loss: 172614081314816.0000\n",
            "Epoch 156/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 109995295244288.0000 - val_loss: 73038259290112.0000\n",
            "Epoch 157/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 37881429517926.3984 - val_loss: 21995766939648.0000\n",
            "Epoch 158/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 18298223722496.0000 - val_loss: 11176529362944.0000\n",
            "Epoch 159/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 26566881338982.4023 - val_loss: 12563796459520.0000\n",
            "Epoch 160/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 35912246571827.1953 - val_loss: 11961176686592.0000\n",
            "Epoch 161/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 31090249878732.8008 - val_loss: 105211439874048.0000\n",
            "Epoch 162/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 68788672685670.3984 - val_loss: 90678939877376.0000\n",
            "Epoch 163/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 54097248832716.8047 - val_loss: 80410033782784.0000\n",
            "Epoch 164/1000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 40193348337664.0000 - val_loss: 51197738221568.0000\n",
            "Epoch 165/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 24006127045836.8008 - val_loss: 22507377655808.0000\n",
            "Epoch 166/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 17256308906393.5996 - val_loss: 12625232527360.0000\n",
            "Epoch 167/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 19706288485171.1992 - val_loss: 11679702188032.0000\n",
            "Epoch 168/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 21121305306726.4023 - val_loss: 35320171069440.0000\n",
            "Epoch 169/1000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 20198703130214.4023 - val_loss: 27916494700544.0000\n",
            "Epoch 170/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 18056869209702.4023 - val_loss: 21643736907776.0000\n",
            "Epoch 171/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 16361801646080.0000 - val_loss: 18175636799488.0000\n",
            "Epoch 172/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 16362323836928.0000 - val_loss: 15962131660800.0000\n",
            "Epoch 173/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16998548373504.0000 - val_loss: 15512946868224.0000\n",
            "Epoch 174/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 17020084656537.5996 - val_loss: 16180582547456.0000\n",
            "Epoch 175/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 19310103337369.5977 - val_loss: 17123930800128.0000\n",
            "Epoch 176/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 16429071361638.4004 - val_loss: 18267670315008.0000\n",
            "Epoch 177/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 16527675464089.5996 - val_loss: 19814296846336.0000\n",
            "Epoch 178/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 16327385284608.0000 - val_loss: 20181143257088.0000\n",
            "Epoch 179/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15989714033049.5996 - val_loss: 22460458074112.0000\n",
            "Epoch 180/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16501708108595.2012 - val_loss: 21987619504128.0000\n",
            "Epoch 181/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16522627619225.5996 - val_loss: 20677971148800.0000\n",
            "Epoch 182/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 16300933067571.2012 - val_loss: 20141739868160.0000\n",
            "Epoch 183/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 16318966477619.2012 - val_loss: 66924591448064.0000\n",
            "Epoch 184/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 41787875078963.1953 - val_loss: 16050868453376.0000\n",
            "Epoch 185/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 17043095027712.0000 - val_loss: 14721436614656.0000\n",
            "Epoch 186/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17511973335859.2012 - val_loss: 15084937019392.0000\n",
            "Epoch 187/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 17087479572070.4004 - val_loss: 16710808633344.0000\n",
            "Epoch 188/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16630008512512.0000 - val_loss: 18822278938624.0000\n",
            "Epoch 189/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16273313995161.5996 - val_loss: 20513992736768.0000\n",
            "Epoch 190/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 16591532064768.0000 - val_loss: 21586241388544.0000\n",
            "Epoch 191/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 16973246863769.5996 - val_loss: 24175223767040.0000\n",
            "Epoch 192/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 16696152267161.5996 - val_loss: 21621039431680.0000\n",
            "Epoch 193/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16427813280153.5996 - val_loss: 19415689068544.0000\n",
            "Epoch 194/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 16298629555814.4004 - val_loss: 18999159029760.0000\n",
            "Epoch 195/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 16310870841753.5996 - val_loss: 17643788566528.0000\n",
            "Epoch 196/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 16585905615667.2012 - val_loss: 18707875102720.0000\n",
            "Epoch 197/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 29106153245900.8008 - val_loss: 18217336569856.0000\n",
            "Epoch 198/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 16518003399065.5996 - val_loss: 15947126538240.0000\n",
            "Epoch 199/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 16878577647616.0000 - val_loss: 16917834235904.0000\n",
            "Epoch 200/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 16351576352358.4004 - val_loss: 18922562650112.0000\n",
            "Epoch 201/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 16274741316812.7988 - val_loss: 19226517569536.0000\n",
            "Epoch 202/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 16246546785894.4004 - val_loss: 19217944412160.0000\n",
            "Epoch 203/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 17065575238860.7988 - val_loss: 23327892570112.0000\n",
            "Epoch 204/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 24109863927808.0000 - val_loss: 12083171164160.0000\n",
            "Epoch 205/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 30680108184371.1992 - val_loss: 753755083505664.0000\n",
            "Epoch 206/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 551868219732787.1875 - val_loss: 407152162242560.0000\n",
            "Epoch 207/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 307912721720934.4375 - val_loss: 237613864714240.0000\n",
            "Epoch 208/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 142639142168166.4062 - val_loss: 42633481158656.0000\n",
            "Epoch 209/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 18575811988684.8008 - val_loss: 11193106300928.0000\n",
            "Epoch 210/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 24504430493696.0000 - val_loss: 11765839560704.0000\n",
            "Epoch 211/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 34397628871475.1992 - val_loss: 12345046728704.0000\n",
            "Epoch 212/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 31299991502848.0000 - val_loss: 12097120370688.0000\n",
            "Epoch 213/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 19003105450393.5977 - val_loss: 22157367181312.0000\n",
            "Epoch 214/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 18858817408204.8008 - val_loss: 51901160751104.0000\n",
            "Epoch 215/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 29993738856038.4023 - val_loss: 45174931587072.0000\n",
            "Epoch 216/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 21242766059110.4023 - val_loss: 29656711430144.0000\n",
            "Epoch 217/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 18334901652684.8008 - val_loss: 17157684461568.0000\n",
            "Epoch 218/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16981554102272.0000 - val_loss: 13571752722432.0000\n",
            "Epoch 219/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 24250175560089.5977 - val_loss: 11218697846784.0000\n",
            "Epoch 220/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 22680349075046.4023 - val_loss: 108043886919680.0000\n",
            "Epoch 221/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 69547062407987.1953 - val_loss: 86814039736320.0000\n",
            "Epoch 222/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 153988376821760.0000 - val_loss: 274559576047616.0000\n",
            "Epoch 223/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 192119779727769.5938 - val_loss: 117074726748160.0000\n",
            "Epoch 224/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 57959785064038.3984 - val_loss: 24109238976512.0000\n",
            "Epoch 225/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 30047064817664.0000 - val_loss: 90972574711808.0000\n",
            "Epoch 226/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 37322463012454.3984 - val_loss: 24086679912448.0000\n",
            "Epoch 227/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17696299994316.8008 - val_loss: 12360766980096.0000\n",
            "Epoch 228/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 74723109529190.3906 - val_loss: 13248563773440.0000\n",
            "Epoch 229/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 33484323369779.1992 - val_loss: 12918411231232.0000\n",
            "Epoch 230/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 58218266597785.6016 - val_loss: 20367225651200.0000\n",
            "Epoch 231/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 37178052706304.0000 - val_loss: 17983940329472.0000\n",
            "Epoch 232/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 16409365472870.4004 - val_loss: 36590109851648.0000\n",
            "Epoch 233/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 21706094464204.8008 - val_loss: 56146861752320.0000\n",
            "Epoch 234/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 33715503339929.5977 - val_loss: 54379537235968.0000\n",
            "Epoch 235/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 30890108177612.8008 - val_loss: 67973360386048.0000\n",
            "Epoch 236/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 39318251267686.3984 - val_loss: 52945781522432.0000\n",
            "Epoch 237/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 28348120878284.8008 - val_loss: 40784732618752.0000\n",
            "Epoch 238/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23379370101964.8008 - val_loss: 46214477250560.0000\n",
            "Epoch 239/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 24349057115750.4023 - val_loss: 34491965898752.0000\n",
            "Epoch 240/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 19050665502310.4023 - val_loss: 18087749353472.0000\n",
            "Epoch 241/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 18981297586176.0000 - val_loss: 12247684349952.0000\n",
            "Epoch 242/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 20836263762329.5977 - val_loss: 9732788060160.0000\n",
            "Epoch 243/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 24180629805465.5977 - val_loss: 10470684622848.0000\n",
            "Epoch 244/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 22997443490611.1992 - val_loss: 15105657929728.0000\n",
            "Epoch 245/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 17855285506867.1992 - val_loss: 12670732337152.0000\n",
            "Epoch 246/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 18155152028467.1992 - val_loss: 21091279962112.0000\n",
            "Epoch 247/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 16383853618790.4004 - val_loss: 26103708123136.0000\n",
            "Epoch 248/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 15662106175078.4004 - val_loss: 19201183973376.0000\n",
            "Epoch 249/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 20026063480422.4023 - val_loss: 38108967993344.0000\n",
            "Epoch 250/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 21098971267072.0000 - val_loss: 28939644829696.0000\n",
            "Epoch 251/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 17915230080204.8008 - val_loss: 20051346325504.0000\n",
            "Epoch 252/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 16580626035507.2012 - val_loss: 15349587116032.0000\n",
            "Epoch 253/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 16981838895513.5996 - val_loss: 14245077975040.0000\n",
            "Epoch 254/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 19879183148646.4023 - val_loss: 14150893830144.0000\n",
            "Epoch 255/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 26417596412723.1992 - val_loss: 16158205935616.0000\n",
            "Epoch 256/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17150039293952.0000 - val_loss: 13244708159488.0000\n",
            "Epoch 257/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 16235013498470.4004 - val_loss: 31417085984768.0000\n",
            "Epoch 258/1000\n",
            "80/80 [==============================] - 0s 886us/step - loss: 18875372535808.0000 - val_loss: 26890916069376.0000\n",
            "Epoch 259/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 17495767384064.0000 - val_loss: 26136851513344.0000\n",
            "Epoch 260/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 17107602584371.2012 - val_loss: 27164084797440.0000\n",
            "Epoch 261/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 19997443437363.1992 - val_loss: 30865983799296.0000\n",
            "Epoch 262/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 18388617265152.0000 - val_loss: 22881721384960.0000\n",
            "Epoch 263/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 16301453161267.2012 - val_loss: 17541463277568.0000\n",
            "Epoch 264/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 16510066517606.4004 - val_loss: 14950227509248.0000\n",
            "Epoch 265/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 17240752023142.4004 - val_loss: 14491273134080.0000\n",
            "Epoch 266/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 17368212727398.4004 - val_loss: 15155678150656.0000\n",
            "Epoch 267/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 16854471724236.7988 - val_loss: 16483134472192.0000\n",
            "Epoch 268/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 16519349980364.7988 - val_loss: 18559765839872.0000\n",
            "Epoch 269/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16104801750220.7988 - val_loss: 20272535044096.0000\n",
            "Epoch 270/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 16193652208435.2012 - val_loss: 21846451814400.0000\n",
            "Epoch 271/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 16292129642905.5996 - val_loss: 22416046686208.0000\n",
            "Epoch 272/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 16345832043315.2012 - val_loss: 22059964956672.0000\n",
            "Epoch 273/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 16401302342860.7988 - val_loss: 20991432458240.0000\n",
            "Epoch 274/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16172540808396.7988 - val_loss: 20717938671616.0000\n",
            "Epoch 275/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15216124153036.7988 - val_loss: 20752214523904.0000\n",
            "Epoch 276/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16150583836672.0000 - val_loss: 20187688468480.0000\n",
            "Epoch 277/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 23049573656166.4023 - val_loss: 53095669170176.0000\n",
            "Epoch 278/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 27703211825561.5977 - val_loss: 35596227575808.0000\n",
            "Epoch 279/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 20728999890124.8008 - val_loss: 20297715548160.0000\n",
            "Epoch 280/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 15980712637235.2012 - val_loss: 14043281620992.0000\n",
            "Epoch 281/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 17839153794252.8008 - val_loss: 9015826317312.0000\n",
            "Epoch 282/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 17933095298662.4023 - val_loss: 10820093214720.0000\n",
            "Epoch 283/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 17492704178995.2012 - val_loss: 11922605867008.0000\n",
            "Epoch 284/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 17328899935436.7988 - val_loss: 18250991665152.0000\n",
            "Epoch 285/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 15607541845196.7988 - val_loss: 22051230318592.0000\n",
            "Epoch 286/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 16140370496716.7988 - val_loss: 24702672175104.0000\n",
            "Epoch 287/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 111214185152512.0000 - val_loss: 465485938294784.0000\n",
            "Epoch 288/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 384751906272051.1875 - val_loss: 347414267428864.0000\n",
            "Epoch 289/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 260815546482688.0000 - val_loss: 234625842544640.0000\n",
            "Epoch 290/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 151453002262118.4062 - val_loss: 115121204494336.0000\n",
            "Epoch 291/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 55709043523584.0000 - val_loss: 10934694182912.0000\n",
            "Epoch 292/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 25343368049459.1992 - val_loss: 53057631027200.0000\n",
            "Epoch 293/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 24725780902707.1992 - val_loss: 21196934479872.0000\n",
            "Epoch 294/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 15948822924492.7988 - val_loss: 11378906628096.0000\n",
            "Epoch 295/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 30841335552409.5977 - val_loss: 18470093717504.0000\n",
            "Epoch 296/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 50275824330342.3984 - val_loss: 13894456180736.0000\n",
            "Epoch 297/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 35299497764454.3984 - val_loss: 10974205575168.0000\n",
            "Epoch 298/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 21778348900352.0000 - val_loss: 10941761585152.0000\n",
            "Epoch 299/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 329324047210905.5625 - val_loss: 210051918725120.0000\n",
            "Epoch 300/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 278990393494732.7812 - val_loss: 33602834268160.0000\n",
            "Epoch 301/1000\n",
            "80/80 [==============================] - 0s 908us/step - loss: 54453739506892.8047 - val_loss: 18381814104064.0000\n",
            "Epoch 302/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 18284134635929.5977 - val_loss: 68854915006464.0000\n",
            "Epoch 303/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 56204432769024.0000 - val_loss: 112070007717888.0000\n",
            "Epoch 304/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 78281058759475.2031 - val_loss: 119419753725952.0000\n",
            "Epoch 305/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 77125354363289.6094 - val_loss: 100469787590656.0000\n",
            "Epoch 306/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 60984581213388.8047 - val_loss: 70442329047040.0000\n",
            "Epoch 307/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 37930096656384.0000 - val_loss: 44070755893248.0000\n",
            "Epoch 308/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 22210472941977.5977 - val_loss: 26209750614016.0000\n",
            "Epoch 309/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 16655678282137.5996 - val_loss: 34790371753984.0000\n",
            "Epoch 310/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 19079705604915.1992 - val_loss: 23355025522688.0000\n",
            "Epoch 311/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 15877063416217.5996 - val_loss: 16791050911744.0000\n",
            "Epoch 312/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 16032748011520.0000 - val_loss: 7931376959488.0000\n",
            "Epoch 313/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 29053201416192.0000 - val_loss: 8069079105536.0000\n",
            "Epoch 314/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 20270234468352.0000 - val_loss: 15267369320448.0000\n",
            "Epoch 315/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 16486046577459.2012 - val_loss: 17418183245824.0000\n",
            "Epoch 316/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 16269229162496.0000 - val_loss: 19700247429120.0000\n",
            "Epoch 317/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15919723262771.2012 - val_loss: 21098009722880.0000\n",
            "Epoch 318/1000\n",
            "80/80 [==============================] - 0s 1000us/step - loss: 16089537209958.4004 - val_loss: 22262069592064.0000\n",
            "Epoch 319/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 16252594552832.0000 - val_loss: 22838962552832.0000\n",
            "Epoch 320/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 16351327210700.7988 - val_loss: 22695443955712.0000\n",
            "Epoch 321/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 16399189042790.4004 - val_loss: 22523521531904.0000\n",
            "Epoch 322/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 16268070066585.5996 - val_loss: 20798374936576.0000\n",
            "Epoch 323/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 16015009670758.4004 - val_loss: 19555009167360.0000\n",
            "Epoch 324/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 15883780174643.2012 - val_loss: 18769275518976.0000\n",
            "Epoch 325/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15969098609459.2012 - val_loss: 17859306586112.0000\n",
            "Epoch 326/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 16010953149644.7988 - val_loss: 17712996679680.0000\n",
            "Epoch 327/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 15991609858457.5996 - val_loss: 17983118245888.0000\n",
            "Epoch 328/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 15947060687667.2012 - val_loss: 18785910128640.0000\n",
            "Epoch 329/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 15942036121190.4004 - val_loss: 12895910887424.0000\n",
            "Epoch 330/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15687148686540.7988 - val_loss: 20740380295168.0000\n",
            "Epoch 331/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 16113724502835.2012 - val_loss: 21506348285952.0000\n",
            "Epoch 332/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16048586752000.0000 - val_loss: 21417047359488.0000\n",
            "Epoch 333/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 17773407240192.0000 - val_loss: 22432563855360.0000\n",
            "Epoch 334/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 16310185282764.7988 - val_loss: 23321961824256.0000\n",
            "Epoch 335/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 16358247392870.4004 - val_loss: 22967855611904.0000\n",
            "Epoch 336/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 16357703811072.0000 - val_loss: 21299466338304.0000\n",
            "Epoch 337/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 15929739470438.4004 - val_loss: 21176235589632.0000\n",
            "Epoch 338/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 15965247818956.7988 - val_loss: 20335021785088.0000\n",
            "Epoch 339/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15885243147878.4004 - val_loss: 19670342041600.0000\n",
            "Epoch 340/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 15774219082137.5996 - val_loss: 18472872443904.0000\n",
            "Epoch 341/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 15791198883020.7988 - val_loss: 17518864367616.0000\n",
            "Epoch 342/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 15926674482790.4004 - val_loss: 16400500391936.0000\n",
            "Epoch 343/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 16069527024435.2012 - val_loss: 16065538031616.0000\n",
            "Epoch 344/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 17835563050598.4023 - val_loss: 17225182347264.0000\n",
            "Epoch 345/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 16201450048716.7988 - val_loss: 12405286371328.0000\n",
            "Epoch 346/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 17585138355404.7988 - val_loss: 16449475182592.0000\n",
            "Epoch 347/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 16578047377408.0000 - val_loss: 23032382881792.0000\n",
            "Epoch 348/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 16286101169766.4004 - val_loss: 23435587616768.0000\n",
            "Epoch 349/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 16193750145433.5996 - val_loss: 21396088422400.0000\n",
            "Epoch 350/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15986317066240.0000 - val_loss: 19213188071424.0000\n",
            "Epoch 351/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 18300494518681.5977 - val_loss: 19717846728704.0000\n",
            "Epoch 352/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 15684700890726.4004 - val_loss: 10565567119360.0000\n",
            "Epoch 353/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 25344053398732.8008 - val_loss: 40438853533696.0000\n",
            "Epoch 354/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 22190859118182.4023 - val_loss: 28431060303872.0000\n",
            "Epoch 355/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 17304011354931.2012 - val_loss: 19033854312448.0000\n",
            "Epoch 356/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 16291587738828.7988 - val_loss: 14409142370304.0000\n",
            "Epoch 357/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16880136041267.2012 - val_loss: 13723248885760.0000\n",
            "Epoch 358/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 16977704570060.7988 - val_loss: 14755521626112.0000\n",
            "Epoch 359/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 16136919842816.0000 - val_loss: 17369609011200.0000\n",
            "Epoch 360/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 16252654950809.5996 - val_loss: 20571913977856.0000\n",
            "Epoch 361/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 15794804516454.4004 - val_loss: 20729454133248.0000\n",
            "Epoch 362/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 15603083090329.5996 - val_loss: 19367444086784.0000\n",
            "Epoch 363/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15659570508595.2012 - val_loss: 10859675910144.0000\n",
            "Epoch 364/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 15553276359475.2012 - val_loss: 21074045566976.0000\n",
            "Epoch 365/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 15292045878886.4004 - val_loss: 24092912648192.0000\n",
            "Epoch 366/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 16597922925772.7988 - val_loss: 24276614774784.0000\n",
            "Epoch 367/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 16498623407718.4004 - val_loss: 21048296734720.0000\n",
            "Epoch 368/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 15883411914752.0000 - val_loss: 18905221300224.0000\n",
            "Epoch 369/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14207853317324.7988 - val_loss: 6737113382912.0000\n",
            "Epoch 370/1000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 20939229731225.5977 - val_loss: 10436611145728.0000\n",
            "Epoch 371/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17273710025113.5996 - val_loss: 27571129417728.0000\n",
            "Epoch 372/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 17991500562432.0000 - val_loss: 30745013780480.0000\n",
            "Epoch 373/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 18193787650048.0000 - val_loss: 26329590267904.0000\n",
            "Epoch 374/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16532201956966.4004 - val_loss: 20895812812800.0000\n",
            "Epoch 375/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 15399940784128.0000 - val_loss: 17008134455296.0000\n",
            "Epoch 376/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 15705576570880.0000 - val_loss: 14872005836800.0000\n",
            "Epoch 377/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 16503470764851.2012 - val_loss: 14297174376448.0000\n",
            "Epoch 378/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 16395181595033.5996 - val_loss: 14207242207232.0000\n",
            "Epoch 379/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 15954075870822.4004 - val_loss: 15904916111360.0000\n",
            "Epoch 380/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 15548555670323.2012 - val_loss: 18739972014080.0000\n",
            "Epoch 381/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 15664624015769.5996 - val_loss: 21645175554048.0000\n",
            "Epoch 382/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15826799925657.5996 - val_loss: 22443276107776.0000\n",
            "Epoch 383/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16068153389875.2012 - val_loss: 22751878316032.0000\n",
            "Epoch 384/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15994261078016.0000 - val_loss: 21812490534912.0000\n",
            "Epoch 385/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 16066955496652.7988 - val_loss: 19515010187264.0000\n",
            "Epoch 386/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 15693583168307.2012 - val_loss: 18267032780800.0000\n",
            "Epoch 387/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 15659446986342.4004 - val_loss: 19833743736832.0000\n",
            "Epoch 388/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 15645335460249.5996 - val_loss: 15797840773120.0000\n",
            "Epoch 389/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 14385094538035.2012 - val_loss: 15264973324288.0000\n",
            "Epoch 390/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 13877028218470.4004 - val_loss: 14803683770368.0000\n",
            "Epoch 391/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15280172013977.5996 - val_loss: 18357231288320.0000\n",
            "Epoch 392/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 15860153240780.7988 - val_loss: 17188819828736.0000\n",
            "Epoch 393/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15635140784947.2012 - val_loss: 17316729323520.0000\n",
            "Epoch 394/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 16193836548096.0000 - val_loss: 14944240140288.0000\n",
            "Epoch 395/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 15760629956608.0000 - val_loss: 16967387840512.0000\n",
            "Epoch 396/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 15405525499904.0000 - val_loss: 19707071561728.0000\n",
            "Epoch 397/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 15466579466649.5996 - val_loss: 22003861946368.0000\n",
            "Epoch 398/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15844985798656.0000 - val_loss: 23002190184448.0000\n",
            "Epoch 399/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 15983319397171.2012 - val_loss: 22433625014272.0000\n",
            "Epoch 400/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15858079576883.2012 - val_loss: 21000613789696.0000\n",
            "Epoch 401/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 15546361420185.5996 - val_loss: 19697684709376.0000\n",
            "Epoch 402/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 15885247342182.4004 - val_loss: 21976051613696.0000\n",
            "Epoch 403/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 15863139165798.4004 - val_loss: 19285896331264.0000\n",
            "Epoch 404/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 15369825261977.5996 - val_loss: 18105640157184.0000\n",
            "Epoch 405/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 15375498477568.0000 - val_loss: 17147088601088.0000\n",
            "Epoch 406/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 15502457543065.5996 - val_loss: 12427853824000.0000\n",
            "Epoch 407/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16272764121907.2012 - val_loss: 16184172871680.0000\n",
            "Epoch 408/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15563789172736.0000 - val_loss: 16635753660416.0000\n",
            "Epoch 409/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 15528075513036.7988 - val_loss: 17651483017216.0000\n",
            "Epoch 410/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 15272653724057.5996 - val_loss: 18489295241216.0000\n",
            "Epoch 411/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 15356574197350.4004 - val_loss: 18649964347392.0000\n",
            "Epoch 412/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 15326475032985.5996 - val_loss: 19030651961344.0000\n",
            "Epoch 413/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15344818192384.0000 - val_loss: 9107788529664.0000\n",
            "Epoch 414/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 15068439144038.4004 - val_loss: 15758660730880.0000\n",
            "Epoch 415/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 15475154721177.5996 - val_loss: 23690435624960.0000\n",
            "Epoch 416/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 15085119681331.2012 - val_loss: 16937430024192.0000\n",
            "Epoch 417/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 12730974915788.7988 - val_loss: 16291148595200.0000\n",
            "Epoch 418/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 13062532759552.0000 - val_loss: 15266394144768.0000\n",
            "Epoch 419/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 13524253068492.7988 - val_loss: 21514044833792.0000\n",
            "Epoch 420/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 15719795890585.5996 - val_loss: 14925701316608.0000\n",
            "Epoch 421/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 14285597048832.0000 - val_loss: 6885899501568.0000\n",
            "Epoch 422/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 14819948442419.2012 - val_loss: 17129416949760.0000\n",
            "Epoch 423/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 15236766839603.2012 - val_loss: 18180481220608.0000\n",
            "Epoch 424/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 14988973231308.7988 - val_loss: 15895311155200.0000\n",
            "Epoch 425/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 15209544967782.4004 - val_loss: 20908020334592.0000\n",
            "Epoch 426/1000\n",
            "80/80 [==============================] - 0s 935us/step - loss: 15333248624230.4004 - val_loss: 20026732052480.0000\n",
            "Epoch 427/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 15317503207014.4004 - val_loss: 18409687351296.0000\n",
            "Epoch 428/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 15251335335116.7988 - val_loss: 17256175108096.0000\n",
            "Epoch 429/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 15163098084147.2012 - val_loss: 17977105711104.0000\n",
            "Epoch 430/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15281466376192.0000 - val_loss: 18730352377856.0000\n",
            "Epoch 431/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 15234494051123.2012 - val_loss: 16787053740032.0000\n",
            "Epoch 432/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 14583378162483.2012 - val_loss: 15135479431168.0000\n",
            "Epoch 433/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 15039292506112.0000 - val_loss: 18487076454400.0000\n",
            "Epoch 434/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 14974271356928.0000 - val_loss: 19541501411328.0000\n",
            "Epoch 435/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 15054623735808.0000 - val_loss: 20201106046976.0000\n",
            "Epoch 436/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 15412530616729.5996 - val_loss: 20423024574464.0000\n",
            "Epoch 437/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 15127030005760.0000 - val_loss: 17635307683840.0000\n",
            "Epoch 438/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 15134482025676.7988 - val_loss: 16175657385984.0000\n",
            "Epoch 439/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 15061846327296.0000 - val_loss: 16168678064128.0000\n",
            "Epoch 440/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 15049223988838.4004 - val_loss: 16140409503744.0000\n",
            "Epoch 441/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 14974834232524.7988 - val_loss: 16980773961728.0000\n",
            "Epoch 442/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 16507255495065.5996 - val_loss: 20081830526976.0000\n",
            "Epoch 443/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 15019394727936.0000 - val_loss: 20602865844224.0000\n",
            "Epoch 444/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 15080150269952.0000 - val_loss: 20042515218432.0000\n",
            "Epoch 445/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14951725295206.4004 - val_loss: 19153601691648.0000\n",
            "Epoch 446/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 14987241822617.5996 - val_loss: 17249615216640.0000\n",
            "Epoch 447/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 14830368142131.2012 - val_loss: 16957386522624.0000\n",
            "Epoch 448/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14791119523020.7988 - val_loss: 17576269709312.0000\n",
            "Epoch 449/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 14802131668172.7988 - val_loss: 18978986524672.0000\n",
            "Epoch 450/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14769927382630.4004 - val_loss: 20029458350080.0000\n",
            "Epoch 451/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 14899260424192.0000 - val_loss: 20017735270400.0000\n",
            "Epoch 452/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14869078841753.5996 - val_loss: 19573243904000.0000\n",
            "Epoch 453/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14766133634662.4004 - val_loss: 18425747341312.0000\n",
            "Epoch 454/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14730177124761.5996 - val_loss: 16554392551424.0000\n",
            "Epoch 455/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 14718200289689.5996 - val_loss: 16654149877760.0000\n",
            "Epoch 456/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14638211137536.0000 - val_loss: 16103117946880.0000\n",
            "Epoch 457/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 14728303319449.5996 - val_loss: 15620037935104.0000\n",
            "Epoch 458/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 14735809026457.5996 - val_loss: 16875560894464.0000\n",
            "Epoch 459/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 14587339682611.2012 - val_loss: 16955621769216.0000\n",
            "Epoch 460/1000\n",
            "80/80 [==============================] - 0s 892us/step - loss: 14503092263321.5996 - val_loss: 18539159224320.0000\n",
            "Epoch 461/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 15067170786508.7988 - val_loss: 21085064003584.0000\n",
            "Epoch 462/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 15129175392256.0000 - val_loss: 16610892972032.0000\n",
            "Epoch 463/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 14796237412761.5996 - val_loss: 15180343803904.0000\n",
            "Epoch 464/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 14281823852953.5996 - val_loss: 10434443739136.0000\n",
            "Epoch 465/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 11797226376396.7988 - val_loss: 21556134674432.0000\n",
            "Epoch 466/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14797332755251.2012 - val_loss: 23471620882432.0000\n",
            "Epoch 467/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 15701205476966.4004 - val_loss: 22461045276672.0000\n",
            "Epoch 468/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 16035403425382.4004 - val_loss: 15404623724544.0000\n",
            "Epoch 469/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 14663162632601.5996 - val_loss: 13576113750016.0000\n",
            "Epoch 470/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 14829857066188.7988 - val_loss: 14269242408960.0000\n",
            "Epoch 471/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 14218975353241.5996 - val_loss: 17227999870976.0000\n",
            "Epoch 472/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 14465020985344.0000 - val_loss: 20924443131904.0000\n",
            "Epoch 473/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 14627008361267.2012 - val_loss: 20471441522688.0000\n",
            "Epoch 474/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 14500514653798.4004 - val_loss: 19169800093696.0000\n",
            "Epoch 475/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 13974303145984.0000 - val_loss: 15989052801024.0000\n",
            "Epoch 476/1000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 14319190697574.4004 - val_loss: 13530548928512.0000\n",
            "Epoch 477/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 14556456393113.5996 - val_loss: 13641163210752.0000\n",
            "Epoch 478/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 14550006182707.2012 - val_loss: 15537195188224.0000\n",
            "Epoch 479/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 14058316313395.2012 - val_loss: 16521200926720.0000\n",
            "Epoch 480/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 12696266001612.7988 - val_loss: 18329926369280.0000\n",
            "Epoch 481/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 14099741356851.2012 - val_loss: 20033828814848.0000\n",
            "Epoch 482/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 14203130111590.4004 - val_loss: 18891300405248.0000\n",
            "Epoch 483/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 13934733243187.2012 - val_loss: 16792338563072.0000\n",
            "Epoch 484/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 13930415416934.4004 - val_loss: 15129498353664.0000\n",
            "Epoch 485/1000\n",
            "80/80 [==============================] - 0s 914us/step - loss: 13993893901107.2012 - val_loss: 15140794662912.0000\n",
            "Epoch 486/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 13898540174540.7988 - val_loss: 15260693037056.0000\n",
            "Epoch 487/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 13721919920537.5996 - val_loss: 12964008558592.0000\n",
            "Epoch 488/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 14154515192217.5996 - val_loss: 16598825959424.0000\n",
            "Epoch 489/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 13728097501184.0000 - val_loss: 17080836423680.0000\n",
            "Epoch 490/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 13564486929612.7988 - val_loss: 18780956655616.0000\n",
            "Epoch 491/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 13771038785536.0000 - val_loss: 18735767224320.0000\n",
            "Epoch 492/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 13690692698112.0000 - val_loss: 16045442072576.0000\n",
            "Epoch 493/1000\n",
            "80/80 [==============================] - 0s 922us/step - loss: 11083677538713.5996 - val_loss: 7229076930560.0000\n",
            "Epoch 494/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 13696075667865.5996 - val_loss: 9658479673344.0000\n",
            "Epoch 495/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 12409563512832.0000 - val_loss: 20351731892224.0000\n",
            "Epoch 496/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 15386855394508.7988 - val_loss: 26394631340032.0000\n",
            "Epoch 497/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 18078120384921.5977 - val_loss: 26169019727872.0000\n",
            "Epoch 498/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 13920611860480.0000 - val_loss: 13863407845376.0000\n",
            "Epoch 499/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 14099641322700.7988 - val_loss: 9410841673728.0000\n",
            "Epoch 500/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 16369338810368.0000 - val_loss: 10399126650880.0000\n",
            "Epoch 501/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 14523813173657.5996 - val_loss: 16870301237248.0000\n",
            "Epoch 502/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 13164068470784.0000 - val_loss: 22459566784512.0000\n",
            "Epoch 503/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 14613120286720.0000 - val_loss: 22668965314560.0000\n",
            "Epoch 504/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 14258707995033.5996 - val_loss: 18835214172160.0000\n",
            "Epoch 505/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 13251830507110.4004 - val_loss: 13984655736832.0000\n",
            "Epoch 506/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 13388831155814.4004 - val_loss: 12230423740416.0000\n",
            "Epoch 507/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 13440772787404.7988 - val_loss: 13750973235200.0000\n",
            "Epoch 508/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 12994263121920.0000 - val_loss: 17231759015936.0000\n",
            "Epoch 509/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 13386590558617.5996 - val_loss: 18410532503552.0000\n",
            "Epoch 510/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 13237258236723.2012 - val_loss: 14902286614528.0000\n",
            "Epoch 511/1000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 14422818527641.5996 - val_loss: 7808077004800.0000\n",
            "Epoch 512/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 23161741718323.1992 - val_loss: 10801345724416.0000\n",
            "Epoch 513/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 19195420999680.0000 - val_loss: 55033320177664.0000\n",
            "Epoch 514/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 11176725027225.5996 - val_loss: 13471916752896.0000\n",
            "Epoch 515/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 12903597159219.2012 - val_loss: 13156299571200.0000\n",
            "Epoch 516/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 12841694265344.0000 - val_loss: 14402696773632.0000\n",
            "Epoch 517/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 12600649082470.4004 - val_loss: 16702362353664.0000\n",
            "Epoch 518/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 13026880546406.4004 - val_loss: 18113709998080.0000\n",
            "Epoch 519/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 12933058789376.0000 - val_loss: 16254065704960.0000\n",
            "Epoch 520/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 12593685069824.0000 - val_loss: 14862113570816.0000\n",
            "Epoch 521/1000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 12498847871795.2012 - val_loss: 13982570119168.0000\n",
            "Epoch 522/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 12505339396096.0000 - val_loss: 13698522415104.0000\n",
            "Epoch 523/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 12542223476326.4004 - val_loss: 13841943494656.0000\n",
            "Epoch 524/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 12346842100531.2012 - val_loss: 15415268868096.0000\n",
            "Epoch 525/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 12372284538880.0000 - val_loss: 16569348390912.0000\n",
            "Epoch 526/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 12503068599910.4004 - val_loss: 11534320271360.0000\n",
            "Epoch 527/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 10785297897881.5996 - val_loss: 15887981608960.0000\n",
            "Epoch 528/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 12290240177766.4004 - val_loss: 16195078062080.0000\n",
            "Epoch 529/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 12640369246208.0000 - val_loss: 14170909048832.0000\n",
            "Epoch 530/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 12113747640320.0000 - val_loss: 14123830083584.0000\n",
            "Epoch 531/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 12061082281574.4004 - val_loss: 14398877859840.0000\n",
            "Epoch 532/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 12004052539801.5996 - val_loss: 15236247584768.0000\n",
            "Epoch 533/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 12376486812057.5996 - val_loss: 17017192054784.0000\n",
            "Epoch 534/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 12068523606016.0000 - val_loss: 14525917036544.0000\n",
            "Epoch 535/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 11671905815756.7988 - val_loss: 11879723302912.0000\n",
            "Epoch 536/1000\n",
            "80/80 [==============================] - 0s 928us/step - loss: 12511576745574.4004 - val_loss: 10729211035648.0000\n",
            "Epoch 537/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 12241689011814.4004 - val_loss: 12889735823360.0000\n",
            "Epoch 538/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 11788785549312.0000 - val_loss: 14935854678016.0000\n",
            "Epoch 539/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 11268610483814.4004 - val_loss: 13056945946624.0000\n",
            "Epoch 540/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 11395278884044.7988 - val_loss: 12621516374016.0000\n",
            "Epoch 541/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 11590871023616.0000 - val_loss: 13520434364416.0000\n",
            "Epoch 542/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11763122490572.7988 - val_loss: 14556197814272.0000\n",
            "Epoch 543/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11353784844288.0000 - val_loss: 12284288040960.0000\n",
            "Epoch 544/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11449329411686.4004 - val_loss: 11872413679616.0000\n",
            "Epoch 545/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11649371707801.5996 - val_loss: 14118852493312.0000\n",
            "Epoch 546/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11385704126873.5996 - val_loss: 13202620416000.0000\n",
            "Epoch 547/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 11041507167436.7988 - val_loss: 14241211875328.0000\n",
            "Epoch 548/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 11180792453529.5996 - val_loss: 14591067160576.0000\n",
            "Epoch 549/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 10951278251212.7988 - val_loss: 12357956796416.0000\n",
            "Epoch 550/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 10990314114252.7988 - val_loss: 10599850311680.0000\n",
            "Epoch 551/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 11027983263334.4004 - val_loss: 11763180371968.0000\n",
            "Epoch 552/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 11092879631974.4004 - val_loss: 14114753609728.0000\n",
            "Epoch 553/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 11344736262553.5996 - val_loss: 15822323974144.0000\n",
            "Epoch 554/1000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 10763934315315.2012 - val_loss: 11517109993472.0000\n",
            "Epoch 555/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 10802392622694.4004 - val_loss: 9921322024960.0000\n",
            "Epoch 556/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 10818429334323.2012 - val_loss: 11356782723072.0000\n",
            "Epoch 557/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 10397073853644.7988 - val_loss: 13217684258816.0000\n",
            "Epoch 558/1000\n",
            "80/80 [==============================] - 0s 916us/step - loss: 10395497739059.2012 - val_loss: 13818544521216.0000\n",
            "Epoch 559/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 10393040715776.0000 - val_loss: 13404782723072.0000\n",
            "Epoch 560/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 10294386701107.2012 - val_loss: 13169889116160.0000\n",
            "Epoch 561/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 10303104981401.5996 - val_loss: 12213962145792.0000\n",
            "Epoch 562/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 10159728571187.2012 - val_loss: 12110254833664.0000\n",
            "Epoch 563/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 10167761397350.4004 - val_loss: 9947057225728.0000\n",
            "Epoch 564/1000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 10209317565235.2012 - val_loss: 10269442965504.0000\n",
            "Epoch 565/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 9902908191539.2012 - val_loss: 7301001379840.0000\n",
            "Epoch 566/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 10681160879308.7988 - val_loss: 15443799572480.0000\n",
            "Epoch 567/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 10348044746752.0000 - val_loss: 23065207504896.0000\n",
            "Epoch 568/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 11862420540620.7988 - val_loss: 14223725821952.0000\n",
            "Epoch 569/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 9721534113382.4004 - val_loss: 6221510213632.0000\n",
            "Epoch 570/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 12171680625459.2012 - val_loss: 5749846048768.0000\n",
            "Epoch 571/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 11541937756569.5996 - val_loss: 9206020177920.0000\n",
            "Epoch 572/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 9724851598131.2012 - val_loss: 15968950550528.0000\n",
            "Epoch 573/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 10885455360819.2012 - val_loss: 16505036079104.0000\n",
            "Epoch 574/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 9758479640166.4004 - val_loss: 9267854704640.0000\n",
            "Epoch 575/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 10085045003878.4004 - val_loss: 6671532294144.0000\n",
            "Epoch 576/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 10091462813286.4004 - val_loss: 8431609053184.0000\n",
            "Epoch 577/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8743358876876.7998 - val_loss: 13106543591424.0000\n",
            "Epoch 578/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 9547201432780.7988 - val_loss: 16986078707712.0000\n",
            "Epoch 579/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 10296333067878.4004 - val_loss: 14414215380992.0000\n",
            "Epoch 580/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 7558699837030.3994 - val_loss: 2982059180032.0000\n",
            "Epoch 581/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 9675689322086.4004 - val_loss: 8646462799872.0000\n",
            "Epoch 582/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 8920083044761.5996 - val_loss: 8980666515456.0000\n",
            "Epoch 583/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 8541227397939.2002 - val_loss: 11960008572928.0000\n",
            "Epoch 584/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8615097479987.2002 - val_loss: 12451993092096.0000\n",
            "Epoch 585/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 8639763762380.7998 - val_loss: 11230072799232.0000\n",
            "Epoch 586/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 8462211953459.2002 - val_loss: 9388833112064.0000\n",
            "Epoch 587/1000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 8263738746470.3994 - val_loss: 9148334866432.0000\n",
            "Epoch 588/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 8248905066086.3994 - val_loss: 9240672468992.0000\n",
            "Epoch 589/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 8056787907379.2002 - val_loss: 10707204571136.0000\n",
            "Epoch 590/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 8082789236736.0000 - val_loss: 10910452154368.0000\n",
            "Epoch 591/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 8181763067084.7998 - val_loss: 9472217972736.0000\n",
            "Epoch 592/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 7840301318144.0000 - val_loss: 8910852325376.0000\n",
            "Epoch 593/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 7813345155481.6006 - val_loss: 8568909594624.0000\n",
            "Epoch 594/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 7696686986035.2002 - val_loss: 9629611327488.0000\n",
            "Epoch 595/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 7840038597427.2002 - val_loss: 10831276277760.0000\n",
            "Epoch 596/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 7823172619468.7998 - val_loss: 9125201182720.0000\n",
            "Epoch 597/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 7523184253337.6006 - val_loss: 8913044897792.0000\n",
            "Epoch 598/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 7336641324646.3994 - val_loss: 9853251616768.0000\n",
            "Epoch 599/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 7401269257830.3994 - val_loss: 9539992682496.0000\n",
            "Epoch 600/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 7274617425100.7998 - val_loss: 8872051867648.0000\n",
            "Epoch 601/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 7324734953881.6006 - val_loss: 9072516530176.0000\n",
            "Epoch 602/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 7292281526681.6006 - val_loss: 6724932599808.0000\n",
            "Epoch 603/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 7050089201664.0000 - val_loss: 7638012133376.0000\n",
            "Epoch 604/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 6833883316224.0000 - val_loss: 8644790845440.0000\n",
            "Epoch 605/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 6773417810329.6006 - val_loss: 10100355891200.0000\n",
            "Epoch 606/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 6898171615641.6006 - val_loss: 9176701992960.0000\n",
            "Epoch 607/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 6682796097536.0000 - val_loss: 8404226015232.0000\n",
            "Epoch 608/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 6631016156364.7998 - val_loss: 6725503025152.0000\n",
            "Epoch 609/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6433587855360.0000 - val_loss: 6540547325952.0000\n",
            "Epoch 610/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 6395239229030.3994 - val_loss: 7234380103680.0000\n",
            "Epoch 611/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 6315470697267.2002 - val_loss: 7266805219328.0000\n",
            "Epoch 612/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 6124269889126.3994 - val_loss: 6140766191616.0000\n",
            "Epoch 613/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6182738382028.7998 - val_loss: 6134338420736.0000\n",
            "Epoch 614/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 6224335916236.7998 - val_loss: 8028903440384.0000\n",
            "Epoch 615/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5909482412441.6006 - val_loss: 7039905431552.0000\n",
            "Epoch 616/1000\n",
            "80/80 [==============================] - 0s 902us/step - loss: 5882507060838.3994 - val_loss: 6441572237312.0000\n",
            "Epoch 617/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5945208864768.0000 - val_loss: 4743472087040.0000\n",
            "Epoch 618/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 5850233883852.7998 - val_loss: 5776407003136.0000\n",
            "Epoch 619/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 5498631670988.7998 - val_loss: 7104967475200.0000\n",
            "Epoch 620/1000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 5635196007219.2002 - val_loss: 8070294929408.0000\n",
            "Epoch 621/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 5524194485862.3994 - val_loss: 6427732606976.0000\n",
            "Epoch 622/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 5309064439398.3994 - val_loss: 5906128437248.0000\n",
            "Epoch 623/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 5207323521843.2002 - val_loss: 5793326825472.0000\n",
            "Epoch 624/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 5071621324800.0000 - val_loss: 5123141533696.0000\n",
            "Epoch 625/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 5121358430208.0000 - val_loss: 4406961504256.0000\n",
            "Epoch 626/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 5020971329126.3994 - val_loss: 5654188654592.0000\n",
            "Epoch 627/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4917491872563.2002 - val_loss: 6976628064256.0000\n",
            "Epoch 628/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 4815145245081.6006 - val_loss: 6259834093568.0000\n",
            "Epoch 629/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 4730353562419.2002 - val_loss: 4790757621760.0000\n",
            "Epoch 630/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 4617970411110.3994 - val_loss: 5088606683136.0000\n",
            "Epoch 631/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 4507183454617.6006 - val_loss: 5379431333888.0000\n",
            "Epoch 632/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 4478072677990.3994 - val_loss: 5025662238720.0000\n",
            "Epoch 633/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 4357719064576.0000 - val_loss: 4684532154368.0000\n",
            "Epoch 634/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 4301216389529.6001 - val_loss: 5201185472512.0000\n",
            "Epoch 635/1000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 4283393585971.1997 - val_loss: 4821229764608.0000\n",
            "Epoch 636/1000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 4396808300134.3999 - val_loss: 3293200777216.0000\n",
            "Epoch 637/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 4230278807552.0000 - val_loss: 4155979071488.0000\n",
            "Epoch 638/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 4171253887795.1997 - val_loss: 5284769562624.0000\n",
            "Epoch 639/1000\n",
            "80/80 [==============================] - 0s 893us/step - loss: 3848568360140.8003 - val_loss: 4189880582144.0000\n",
            "Epoch 640/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 4022188847923.1997 - val_loss: 3416526946304.0000\n",
            "Epoch 641/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 3843186753536.0000 - val_loss: 4751169683456.0000\n",
            "Epoch 642/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 3639244750848.0000 - val_loss: 4959021039616.0000\n",
            "Epoch 643/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 3689331136921.6001 - val_loss: 4955793522688.0000\n",
            "Epoch 644/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 3633293085900.8003 - val_loss: 4720909352960.0000\n",
            "Epoch 645/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 3485575571046.3999 - val_loss: 4037901811712.0000\n",
            "Epoch 646/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 3373111364812.8003 - val_loss: 4256998883328.0000\n",
            "Epoch 647/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 3480105674342.3999 - val_loss: 4173408501760.0000\n",
            "Epoch 648/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 3697216900300.8003 - val_loss: 2276573315072.0000\n",
            "Epoch 649/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 3451442115379.1997 - val_loss: 3391866273792.0000\n",
            "Epoch 650/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 3284080787456.0000 - val_loss: 4528963846144.0000\n",
            "Epoch 651/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 3110505178726.3999 - val_loss: 4406969368576.0000\n",
            "Epoch 652/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 3032566752870.3999 - val_loss: 3622889324544.0000\n",
            "Epoch 653/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 3002895486156.8003 - val_loss: 2862986035200.0000\n",
            "Epoch 654/1000\n",
            "80/80 [==============================] - 0s 906us/step - loss: 3006072461721.6001 - val_loss: 3282770067456.0000\n",
            "Epoch 655/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 2851318975692.8003 - val_loss: 3016100937728.0000\n",
            "Epoch 656/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 2771791878553.6001 - val_loss: 3386239352832.0000\n",
            "Epoch 657/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2741211863449.6001 - val_loss: 3830177071104.0000\n",
            "Epoch 658/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 2722534575308.8003 - val_loss: 2959226961920.0000\n",
            "Epoch 659/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 2651541130444.8003 - val_loss: 2191793979392.0000\n",
            "Epoch 660/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 2761711314534.3999 - val_loss: 2323530383360.0000\n",
            "Epoch 661/1000\n",
            "80/80 [==============================] - 0s 912us/step - loss: 2678546392678.3999 - val_loss: 4251949203456.0000\n",
            "Epoch 662/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 2595814899712.0000 - val_loss: 3565857538048.0000\n",
            "Epoch 663/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 2403512772198.3999 - val_loss: 2223007858688.0000\n",
            "Epoch 664/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 2491812728012.8003 - val_loss: 2331216707584.0000\n",
            "Epoch 665/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 2261168449126.3999 - val_loss: 1724930457600.0000\n",
            "Epoch 666/1000\n",
            "80/80 [==============================] - 0s 925us/step - loss: 1569617975705.5999 - val_loss: 2929420926976.0000\n",
            "Epoch 667/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 2706428710092.8003 - val_loss: 4319500566528.0000\n",
            "Epoch 668/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2368709171609.6001 - val_loss: 1320512913408.0000\n",
            "Epoch 669/1000\n",
            "80/80 [==============================] - 0s 915us/step - loss: 2730732027904.0000 - val_loss: 1239413293056.0000\n",
            "Epoch 670/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 2677985928806.3999 - val_loss: 3145331113984.0000\n",
            "Epoch 671/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 2181747284377.5999 - val_loss: 2764463144960.0000\n",
            "Epoch 672/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 2109053783244.8000 - val_loss: 1912137318400.0000\n",
            "Epoch 673/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 2063032962252.8000 - val_loss: 1958760153088.0000\n",
            "Epoch 674/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1995180343296.0000 - val_loss: 3032595824640.0000\n",
            "Epoch 675/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 1967628222464.0000 - val_loss: 3103643140096.0000\n",
            "Epoch 676/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 1894082910617.5999 - val_loss: 2275504029696.0000\n",
            "Epoch 677/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 1869295963340.8000 - val_loss: 1409000669184.0000\n",
            "Epoch 678/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 1896967726694.4001 - val_loss: 1854266408960.0000\n",
            "Epoch 679/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 1676078507622.4001 - val_loss: 3038752014336.0000\n",
            "Epoch 680/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1987276387123.2000 - val_loss: 2432412680192.0000\n",
            "Epoch 681/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1649573987942.4001 - val_loss: 784458973184.0000\n",
            "Epoch 682/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1884337890918.4001 - val_loss: 877759430656.0000\n",
            "Epoch 683/1000\n",
            "80/80 [==============================] - 0s 911us/step - loss: 1557199021670.4001 - val_loss: 1987329654784.0000\n",
            "Epoch 684/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 1450162126848.0000 - val_loss: 3456102825984.0000\n",
            "Epoch 685/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 1808790742630.4001 - val_loss: 2772993310720.0000\n",
            "Epoch 686/1000\n",
            "80/80 [==============================] - 0s 989us/step - loss: 1565717757952.0000 - val_loss: 1671813791744.0000\n",
            "Epoch 687/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 1468738908979.2000 - val_loss: 1029383585792.0000\n",
            "Epoch 688/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 1511540051148.8000 - val_loss: 1774831534080.0000\n",
            "Epoch 689/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1398151459635.2000 - val_loss: 2196147273728.0000\n",
            "Epoch 690/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1427930323353.5999 - val_loss: 1803316232192.0000\n",
            "Epoch 691/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 1473835525734.4001 - val_loss: 822232088576.0000\n",
            "Epoch 692/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1450482440601.5999 - val_loss: 1489913249792.0000\n",
            "Epoch 693/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1343229263872.0000 - val_loss: 2755449585664.0000\n",
            "Epoch 694/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 1494769179033.5999 - val_loss: 2930641469440.0000\n",
            "Epoch 695/1000\n",
            "80/80 [==============================] - 0s 894us/step - loss: 1373626145177.5999 - val_loss: 1239711219712.0000\n",
            "Epoch 696/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1277570606694.4001 - val_loss: 681615228928.0000\n",
            "Epoch 697/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1328307162316.8000 - val_loss: 1413513871360.0000\n",
            "Epoch 698/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 1148478331289.5999 - val_loss: 2447157755904.0000\n",
            "Epoch 699/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 1370203580006.4001 - val_loss: 2015180357632.0000\n",
            "Epoch 700/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 1125651159449.5999 - val_loss: 1154085289984.0000\n",
            "Epoch 701/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 1138780438528.0000 - val_loss: 981352644608.0000\n",
            "Epoch 702/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 1090342315622.4000 - val_loss: 1425125670912.0000\n",
            "Epoch 703/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 1137937750425.5999 - val_loss: 1827264921600.0000\n",
            "Epoch 704/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 1127443582156.8000 - val_loss: 1368483430400.0000\n",
            "Epoch 705/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1015207264256.0000 - val_loss: 501789589504.0000\n",
            "Epoch 706/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1188820339916.8000 - val_loss: 801392295936.0000\n",
            "Epoch 707/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 964843431526.4000 - val_loss: 1568591446016.0000\n",
            "Epoch 708/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 1123046273843.2000 - val_loss: 1624520785920.0000\n",
            "Epoch 709/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 941365172633.6000 - val_loss: 596632600576.0000\n",
            "Epoch 710/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 1116054539468.8000 - val_loss: 651667832832.0000\n",
            "Epoch 711/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 919884200345.6000 - val_loss: 1806087880704.0000\n",
            "Epoch 712/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 1087987594035.2001 - val_loss: 1866102865920.0000\n",
            "Epoch 713/1000\n",
            "80/80 [==============================] - 0s 891us/step - loss: 951805421158.4000 - val_loss: 850964774912.0000\n",
            "Epoch 714/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 877895784857.6000 - val_loss: 717332545536.0000\n",
            "Epoch 715/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 881469082828.7999 - val_loss: 1177564479488.0000\n",
            "Epoch 716/1000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 1005932360499.2001 - val_loss: 1920276889600.0000\n",
            "Epoch 717/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 939606415769.6000 - val_loss: 983186407424.0000\n",
            "Epoch 718/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 800322328985.6000 - val_loss: 670534139904.0000\n",
            "Epoch 719/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 801795080192.0000 - val_loss: 965730828288.0000\n",
            "Epoch 720/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 736291002777.6000 - val_loss: 1276461580288.0000\n",
            "Epoch 721/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 780440122163.2001 - val_loss: 1095084736512.0000\n",
            "Epoch 722/1000\n",
            "80/80 [==============================] - 0s 958us/step - loss: 716164562944.0000 - val_loss: 870185566208.0000\n",
            "Epoch 723/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 723355854438.4000 - val_loss: 667931443200.0000\n",
            "Epoch 724/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 700472629657.6000 - val_loss: 947342802944.0000\n",
            "Epoch 725/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 713714198118.4000 - val_loss: 1144287133696.0000\n",
            "Epoch 726/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 748744710553.6000 - val_loss: 872273018880.0000\n",
            "Epoch 727/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 661223361740.7999 - val_loss: 307276840960.0000\n",
            "Epoch 728/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 854615523328.0000 - val_loss: 448450527232.0000\n",
            "Epoch 729/1000\n",
            "80/80 [==============================] - 0s 985us/step - loss: 673201887641.6000 - val_loss: 608213532672.0000\n",
            "Epoch 730/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 667819075174.4000 - val_loss: 1081165611008.0000\n",
            "Epoch 731/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 739590812467.2001 - val_loss: 629315272704.0000\n",
            "Epoch 732/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 603881917644.7999 - val_loss: 678494404608.0000\n",
            "Epoch 733/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 586174490214.4000 - val_loss: 642476015616.0000\n",
            "Epoch 734/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 591114089267.2001 - val_loss: 865849180160.0000\n",
            "Epoch 735/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 614674110873.6000 - val_loss: 825211551744.0000\n",
            "Epoch 736/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 559155350732.7999 - val_loss: 365145653248.0000\n",
            "Epoch 737/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 569195094016.0000 - val_loss: 530746802176.0000\n",
            "Epoch 738/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 564301515980.7999 - val_loss: 734116839424.0000\n",
            "Epoch 739/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 511351835852.8000 - val_loss: 484031922176.0000\n",
            "Epoch 740/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 542713207193.6000 - val_loss: 326971883520.0000\n",
            "Epoch 741/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 514992386867.2000 - val_loss: 641033437184.0000\n",
            "Epoch 742/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 500663992320.0000 - val_loss: 1064017199104.0000\n",
            "Epoch 743/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 579574648012.7999 - val_loss: 705162379264.0000\n",
            "Epoch 744/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 567007392563.2001 - val_loss: 264922628096.0000\n",
            "Epoch 745/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 554720178995.2001 - val_loss: 550290784256.0000\n",
            "Epoch 746/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 454986131046.4000 - val_loss: 476626911232.0000\n",
            "Epoch 747/1000\n",
            "80/80 [==============================] - 0s 953us/step - loss: 449360891084.8000 - val_loss: 437699051520.0000\n",
            "Epoch 748/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 439474801868.8000 - val_loss: 447030296576.0000\n",
            "Epoch 749/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 436927417548.8000 - val_loss: 416854605824.0000\n",
            "Epoch 750/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 451735001497.6000 - val_loss: 629795848192.0000\n",
            "Epoch 751/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 452701303603.2000 - val_loss: 371092324352.0000\n",
            "Epoch 752/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 410777717964.8000 - val_loss: 407514251264.0000\n",
            "Epoch 753/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 392720711680.0000 - val_loss: 551441596416.0000\n",
            "Epoch 754/1000\n",
            "80/80 [==============================] - 0s 977us/step - loss: 402163426918.4000 - val_loss: 500727971840.0000\n",
            "Epoch 755/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 393999807283.2000 - val_loss: 424768339968.0000\n",
            "Epoch 756/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 375449432883.2000 - val_loss: 545108066304.0000\n",
            "Epoch 757/1000\n",
            "80/80 [==============================] - 0s 932us/step - loss: 390105176473.6000 - val_loss: 604304965632.0000\n",
            "Epoch 758/1000\n",
            "80/80 [==============================] - 0s 943us/step - loss: 375051314790.4000 - val_loss: 370468782080.0000\n",
            "Epoch 759/1000\n",
            "80/80 [==============================] - 0s 913us/step - loss: 345220179558.4000 - val_loss: 199039434752.0000\n",
            "Epoch 760/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 382788861952.0000 - val_loss: 254487396352.0000\n",
            "Epoch 761/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 341444195123.2000 - val_loss: 495083159552.0000\n",
            "Epoch 762/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 407286769254.4000 - val_loss: 453757566976.0000\n",
            "Epoch 763/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 309790067916.8000 - val_loss: 147030638592.0000\n",
            "Epoch 764/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 414009720832.0000 - val_loss: 173421199360.0000\n",
            "Epoch 765/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 373249684275.2000 - val_loss: 414691721216.0000\n",
            "Epoch 766/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 324533551104.0000 - val_loss: 212635140096.0000\n",
            "Epoch 767/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 404449119436.8000 - val_loss: 113308180480.0000\n",
            "Epoch 768/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 296769624473.6000 - val_loss: 599062478848.0000\n",
            "Epoch 769/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 398287829401.6000 - val_loss: 494798700544.0000\n",
            "Epoch 770/1000\n",
            "80/80 [==============================] - 0s 982us/step - loss: 359662079180.8000 - val_loss: 186761248768.0000\n",
            "Epoch 771/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 359293845504.0000 - val_loss: 305208360960.0000\n",
            "Epoch 772/1000\n",
            "80/80 [==============================] - 0s 937us/step - loss: 266968695603.2000 - val_loss: 609831354368.0000\n",
            "Epoch 773/1000\n",
            "80/80 [==============================] - 0s 926us/step - loss: 363788155289.6000 - val_loss: 363602837504.0000\n",
            "Epoch 774/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 273295609036.8000 - val_loss: 157728096256.0000\n",
            "Epoch 775/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 316689363763.2000 - val_loss: 193180696576.0000\n",
            "Epoch 776/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 277327423078.4000 - val_loss: 256565886976.0000\n",
            "Epoch 777/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 272950430924.8000 - val_loss: 239934734336.0000\n",
            "Epoch 778/1000\n",
            "80/80 [==============================] - 0s 904us/step - loss: 258497478656.0000 - val_loss: 121043156992.0000\n",
            "Epoch 779/1000\n",
            "80/80 [==============================] - 0s 910us/step - loss: 280936769126.4000 - val_loss: 109497057280.0000\n",
            "Epoch 780/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 269753371852.8000 - val_loss: 289396817920.0000\n",
            "Epoch 781/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 308485573836.8000 - val_loss: 299596775424.0000\n",
            "Epoch 782/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 243324213657.6000 - val_loss: 91525971968.0000\n",
            "Epoch 783/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 319688559820.8000 - val_loss: 110633009152.0000\n",
            "Epoch 784/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 244187666841.6000 - val_loss: 200080048128.0000\n",
            "Epoch 785/1000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 240715019059.2000 - val_loss: 233878798336.0000\n",
            "Epoch 786/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 248375613849.6000 - val_loss: 197775065088.0000\n",
            "Epoch 787/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 243389366272.0000 - val_loss: 98901876736.0000\n",
            "Epoch 788/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 238680342528.0000 - val_loss: 243842121728.0000\n",
            "Epoch 789/1000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 225434820608.0000 - val_loss: 276912472064.0000\n",
            "Epoch 790/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 226570326835.2000 - val_loss: 163295559680.0000\n",
            "Epoch 791/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 227984189030.4000 - val_loss: 98573393920.0000\n",
            "Epoch 792/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 233551888384.0000 - val_loss: 211803471872.0000\n",
            "Epoch 793/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 220547784704.0000 - val_loss: 154232373248.0000\n",
            "Epoch 794/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 216699958067.2000 - val_loss: 120325947392.0000\n",
            "Epoch 795/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 199703466803.2000 - val_loss: 198185828352.0000\n",
            "Epoch 796/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 213469223321.6000 - val_loss: 218259488768.0000\n",
            "Epoch 797/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 211722462822.4000 - val_loss: 92167372800.0000\n",
            "Epoch 798/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 201031935590.4000 - val_loss: 199112212480.0000\n",
            "Epoch 799/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 218133118976.0000 - val_loss: 252033236992.0000\n",
            "Epoch 800/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 207896246681.6000 - val_loss: 147938836480.0000\n",
            "Epoch 801/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 187089516953.6000 - val_loss: 108432441344.0000\n",
            "Epoch 802/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 181769175040.0000 - val_loss: 136301297664.0000\n",
            "Epoch 803/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 176669322444.8000 - val_loss: 153851297792.0000\n",
            "Epoch 804/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 178251277926.4000 - val_loss: 116434640896.0000\n",
            "Epoch 805/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 183487352012.8000 - val_loss: 88666423296.0000\n",
            "Epoch 806/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 178354333286.4000 - val_loss: 179020627968.0000\n",
            "Epoch 807/1000\n",
            "80/80 [==============================] - 0s 924us/step - loss: 182335491276.8000 - val_loss: 162900213760.0000\n",
            "Epoch 808/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 173431497523.2000 - val_loss: 151225548800.0000\n",
            "Epoch 809/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 168073619046.4000 - val_loss: 155120795648.0000\n",
            "Epoch 810/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 174024027340.8000 - val_loss: 126752718848.0000\n",
            "Epoch 811/1000\n",
            "80/80 [==============================] - 0s 934us/step - loss: 170721569996.8000 - val_loss: 105360801792.0000\n",
            "Epoch 812/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 165337897369.6000 - val_loss: 131444097024.0000\n",
            "Epoch 813/1000\n",
            "80/80 [==============================] - 0s 927us/step - loss: 171801611468.8000 - val_loss: 60175953920.0000\n",
            "Epoch 814/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 153764993433.6000 - val_loss: 105272172544.0000\n",
            "Epoch 815/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 162127787622.4000 - val_loss: 118834610176.0000\n",
            "Epoch 816/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 156051950796.8000 - val_loss: 83831603200.0000\n",
            "Epoch 817/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 149076553728.0000 - val_loss: 90005454848.0000\n",
            "Epoch 818/1000\n",
            "80/80 [==============================] - 0s 974us/step - loss: 151450645299.2000 - val_loss: 112893378560.0000\n",
            "Epoch 819/1000\n",
            "80/80 [==============================] - 0s 952us/step - loss: 165043565363.2000 - val_loss: 128728252416.0000\n",
            "Epoch 820/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 140658093260.8000 - val_loss: 69891203072.0000\n",
            "Epoch 821/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 147182981939.2000 - val_loss: 58994008064.0000\n",
            "Epoch 822/1000\n",
            "80/80 [==============================] - 0s 899us/step - loss: 146686594252.8000 - val_loss: 87483908096.0000\n",
            "Epoch 823/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 156445412556.8000 - val_loss: 59714318336.0000\n",
            "Epoch 824/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 145262275788.8000 - val_loss: 76664373248.0000\n",
            "Epoch 825/1000\n",
            "80/80 [==============================] - 0s 919us/step - loss: 132318583193.6000 - val_loss: 58430148608.0000\n",
            "Epoch 826/1000\n",
            "80/80 [==============================] - 0s 918us/step - loss: 133490720768.0000 - val_loss: 71700971520.0000\n",
            "Epoch 827/1000\n",
            "80/80 [==============================] - 0s 900us/step - loss: 126996565196.8000 - val_loss: 120796856320.0000\n",
            "Epoch 828/1000\n",
            "80/80 [==============================] - 0s 905us/step - loss: 152698413056.0000 - val_loss: 126098259968.0000\n",
            "Epoch 829/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 130993487872.0000 - val_loss: 33896781824.0000\n",
            "Epoch 830/1000\n",
            "80/80 [==============================] - 0s 944us/step - loss: 142871402905.6000 - val_loss: 44723224576.0000\n",
            "Epoch 831/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 133188953702.4000 - val_loss: 49207054336.0000\n",
            "Epoch 832/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 124436764672.0000 - val_loss: 19987550208.0000\n",
            "Epoch 833/1000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 138665192652.8000 - val_loss: 53709053952.0000\n",
            "Epoch 834/1000\n",
            "80/80 [==============================] - 0s 941us/step - loss: 122277928140.8000 - val_loss: 127786237952.0000\n",
            "Epoch 835/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 131115162009.6000 - val_loss: 62987034624.0000\n",
            "Epoch 836/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 110965444608.0000 - val_loss: 37326336000.0000\n",
            "Epoch 837/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 130878347673.6000 - val_loss: 42201321472.0000\n",
            "Epoch 838/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 106052235264.0000 - val_loss: 125107159040.0000\n",
            "Epoch 839/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 134971939225.6000 - val_loss: 58177859584.0000\n",
            "Epoch 840/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 95590937395.2000 - val_loss: 29598799872.0000\n",
            "Epoch 841/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 166103824793.6000 - val_loss: 32826730496.0000\n",
            "Epoch 842/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 107085676544.0000 - val_loss: 449888485376.0000\n",
            "Epoch 843/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 231004808806.4000 - val_loss: 250371866624.0000\n",
            "Epoch 844/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 119558330777.6000 - val_loss: 46369214464.0000\n",
            "Epoch 845/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 168865113702.4000 - val_loss: 17283518464.0000\n",
            "Epoch 846/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 114789878988.8000 - val_loss: 59016970240.0000\n",
            "Epoch 847/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 105884029747.2000 - val_loss: 55070633984.0000\n",
            "Epoch 848/1000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 109038197145.6000 - val_loss: 47286632448.0000\n",
            "Epoch 849/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 96286384128.0000 - val_loss: 73043173376.0000\n",
            "Epoch 850/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 107386459750.4000 - val_loss: 55442788352.0000\n",
            "Epoch 851/1000\n",
            "80/80 [==============================] - 0s 978us/step - loss: 96692561510.4000 - val_loss: 29834203136.0000\n",
            "Epoch 852/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 95488547225.6000 - val_loss: 34356359168.0000\n",
            "Epoch 853/1000\n",
            "80/80 [==============================] - 0s 898us/step - loss: 94630535168.0000 - val_loss: 38818111488.0000\n",
            "Epoch 854/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 90494515609.6000 - val_loss: 19266695168.0000\n",
            "Epoch 855/1000\n",
            "80/80 [==============================] - 0s 948us/step - loss: 103098866073.6000 - val_loss: 26409611264.0000\n",
            "Epoch 856/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 84053898035.2000 - val_loss: 25603432448.0000\n",
            "Epoch 857/1000\n",
            "80/80 [==============================] - 0s 951us/step - loss: 94288776396.8000 - val_loss: 21417027584.0000\n",
            "Epoch 858/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 85305537331.2000 - val_loss: 58001469440.0000\n",
            "Epoch 859/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 99068041625.6000 - val_loss: 51325198336.0000\n",
            "Epoch 860/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 89782212198.4000 - val_loss: 21221838848.0000\n",
            "Epoch 861/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 89398660300.8000 - val_loss: 27353618432.0000\n",
            "Epoch 862/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 80606085939.2000 - val_loss: 31918985216.0000\n",
            "Epoch 863/1000\n",
            "80/80 [==============================] - 0s 938us/step - loss: 84470952755.2000 - val_loss: 26515574784.0000\n",
            "Epoch 864/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 87578309427.2000 - val_loss: 18502952960.0000\n",
            "Epoch 865/1000\n",
            "80/80 [==============================] - 0s 930us/step - loss: 86441653043.2000 - val_loss: 33521932288.0000\n",
            "Epoch 866/1000\n",
            "80/80 [==============================] - 0s 931us/step - loss: 77431064166.4000 - val_loss: 15659406336.0000\n",
            "Epoch 867/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 80988531507.2000 - val_loss: 14956681216.0000\n",
            "Epoch 868/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 75633212620.8000 - val_loss: 34717794304.0000\n",
            "Epoch 869/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 76880160358.4000 - val_loss: 24077975552.0000\n",
            "Epoch 870/1000\n",
            "80/80 [==============================] - 0s 979us/step - loss: 73469419520.0000 - val_loss: 18588504064.0000\n",
            "Epoch 871/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 73587900416.0000 - val_loss: 17779800064.0000\n",
            "Epoch 872/1000\n",
            "80/80 [==============================] - 0s 963us/step - loss: 73021610393.6000 - val_loss: 16687215616.0000\n",
            "Epoch 873/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 69916431155.2000 - val_loss: 23661291520.0000\n",
            "Epoch 874/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 75454314086.4000 - val_loss: 23811153920.0000\n",
            "Epoch 875/1000\n",
            "80/80 [==============================] - 0s 923us/step - loss: 73657194905.6000 - val_loss: 22689187840.0000\n",
            "Epoch 876/1000\n",
            "80/80 [==============================] - 0s 907us/step - loss: 97263986278.4000 - val_loss: 23191205888.0000\n",
            "Epoch 877/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 77186947481.6000 - val_loss: 60759748608.0000\n",
            "Epoch 878/1000\n",
            "80/80 [==============================] - 0s 947us/step - loss: 78861303808.0000 - val_loss: 46994624512.0000\n",
            "Epoch 879/1000\n",
            "80/80 [==============================] - 0s 964us/step - loss: 71420281651.2000 - val_loss: 15609698304.0000\n",
            "Epoch 880/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 78622333337.6000 - val_loss: 11534411776.0000\n",
            "Epoch 881/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 65292199526.4000 - val_loss: 17293760512.0000\n",
            "Epoch 882/1000\n",
            "80/80 [==============================] - 0s 998us/step - loss: 76137996288.0000 - val_loss: 17719033856.0000\n",
            "Epoch 883/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 69677867008.0000 - val_loss: 14516071424.0000\n",
            "Epoch 884/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 67117076480.0000 - val_loss: 72539693056.0000\n",
            "Epoch 885/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 75181092044.8000 - val_loss: 61336866816.0000\n",
            "Epoch 886/1000\n",
            "80/80 [==============================] - 0s 999us/step - loss: 67428984422.4000 - val_loss: 8795071488.0000\n",
            "Epoch 887/1000\n",
            "80/80 [==============================] - 0s 991us/step - loss: 61654019276.8000 - val_loss: 15362433024.0000\n",
            "Epoch 888/1000\n",
            "80/80 [==============================] - 0s 945us/step - loss: 56240270540.8000 - val_loss: 22552449024.0000\n",
            "Epoch 889/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 61580837683.2000 - val_loss: 21393379328.0000\n",
            "Epoch 890/1000\n",
            "80/80 [==============================] - 0s 920us/step - loss: 58875578368.0000 - val_loss: 16630913024.0000\n",
            "Epoch 891/1000\n",
            "80/80 [==============================] - 0s 956us/step - loss: 55659028889.6000 - val_loss: 13998172160.0000\n",
            "Epoch 892/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 54259114803.2000 - val_loss: 11799520256.0000\n",
            "Epoch 893/1000\n",
            "80/80 [==============================] - 0s 921us/step - loss: 52575292620.8000 - val_loss: 11519019008.0000\n",
            "Epoch 894/1000\n",
            "80/80 [==============================] - 0s 933us/step - loss: 55377759436.8000 - val_loss: 10523270144.0000\n",
            "Epoch 895/1000\n",
            "80/80 [==============================] - 0s 993us/step - loss: 52215416422.4000 - val_loss: 15497202688.0000\n",
            "Epoch 896/1000\n",
            "80/80 [==============================] - 0s 936us/step - loss: 55973895782.4000 - val_loss: 16479232000.0000\n",
            "Epoch 897/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 58435110092.8000 - val_loss: 16549063680.0000\n",
            "Epoch 898/1000\n",
            "80/80 [==============================] - 0s 972us/step - loss: 62386731417.6000 - val_loss: 15153397760.0000\n",
            "Epoch 899/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 50771763200.0000 - val_loss: 17322575872.0000\n",
            "Epoch 900/1000\n",
            "80/80 [==============================] - 0s 973us/step - loss: 52262210764.8000 - val_loss: 12150724608.0000\n",
            "Epoch 901/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 52423149977.6000 - val_loss: 13220273152.0000\n",
            "Epoch 902/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 50588840755.2000 - val_loss: 15327467520.0000\n",
            "Epoch 903/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 54593870233.6000 - val_loss: 13933864960.0000\n",
            "Epoch 904/1000\n",
            "80/80 [==============================] - 0s 946us/step - loss: 48614852198.4000 - val_loss: 21142464512.0000\n",
            "Epoch 905/1000\n",
            "80/80 [==============================] - 0s 988us/step - loss: 57344213401.6000 - val_loss: 11570186240.0000\n",
            "Epoch 906/1000\n",
            "80/80 [==============================] - 0s 967us/step - loss: 61851505459.2000 - val_loss: 12154294272.0000\n",
            "Epoch 907/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 51762667520.0000 - val_loss: 37018095616.0000\n",
            "Epoch 908/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 63144404582.4000 - val_loss: 13546696704.0000\n",
            "Epoch 909/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 49743469772.8000 - val_loss: 34383306752.0000\n",
            "Epoch 910/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 60369438310.4000 - val_loss: 10598908928.0000\n",
            "Epoch 911/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 49214779392.0000 - val_loss: 7440029184.0000\n",
            "Epoch 912/1000\n",
            "80/80 [==============================] - 0s 955us/step - loss: 55457242316.8000 - val_loss: 11598431232.0000\n",
            "Epoch 913/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 42829591347.2000 - val_loss: 32956114944.0000\n",
            "Epoch 914/1000\n",
            "80/80 [==============================] - 0s 970us/step - loss: 57514572185.6000 - val_loss: 8661866496.0000\n",
            "Epoch 915/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 47754344857.6000 - val_loss: 7195202560.0000\n",
            "Epoch 916/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 48107424972.8000 - val_loss: 13499287552.0000\n",
            "Epoch 917/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 49565608345.6000 - val_loss: 14840744960.0000\n",
            "Epoch 918/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 43769230950.4000 - val_loss: 6695090688.0000\n",
            "Epoch 919/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 42585593446.4000 - val_loss: 7516218880.0000\n",
            "Epoch 920/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 40423781990.4000 - val_loss: 19109445632.0000\n",
            "Epoch 921/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 41906747801.6000 - val_loss: 7784109056.0000\n",
            "Epoch 922/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 39756972851.2000 - val_loss: 7232967680.0000\n",
            "Epoch 923/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 38898377523.2000 - val_loss: 8608831488.0000\n",
            "Epoch 924/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 42127772057.6000 - val_loss: 8670955520.0000\n",
            "Epoch 925/1000\n",
            "80/80 [==============================] - 0s 992us/step - loss: 38359341465.6000 - val_loss: 8695811072.0000\n",
            "Epoch 926/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 43119860121.6000 - val_loss: 9964477440.0000\n",
            "Epoch 927/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 38656560332.8000 - val_loss: 12369587200.0000\n",
            "Epoch 928/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 39932706816.0000 - val_loss: 13912994816.0000\n",
            "Epoch 929/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 37375460147.2000 - val_loss: 9618412544.0000\n",
            "Epoch 930/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 43469334118.4000 - val_loss: 6636028928.0000\n",
            "Epoch 931/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 39937291878.4000 - val_loss: 18666999808.0000\n",
            "Epoch 932/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 43882278092.8000 - val_loss: 9636989952.0000\n",
            "Epoch 933/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 47688577024.0000 - val_loss: 5563193856.0000\n",
            "Epoch 934/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 32172127027.2000 - val_loss: 46712201216.0000\n",
            "Epoch 935/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 54374197657.6000 - val_loss: 8982141952.0000\n",
            "Epoch 936/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 45487015936.0000 - val_loss: 22869506048.0000\n",
            "Epoch 937/1000\n",
            "80/80 [==============================] - 0s 969us/step - loss: 41233926144.0000 - val_loss: 13600388096.0000\n",
            "Epoch 938/1000\n",
            "80/80 [==============================] - 0s 954us/step - loss: 50881900544.0000 - val_loss: 44867440640.0000\n",
            "Epoch 939/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 43094840115.2000 - val_loss: 11206207488.0000\n",
            "Epoch 940/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 49602972467.2000 - val_loss: 6097146880.0000\n",
            "Epoch 941/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 39237558272.0000 - val_loss: 26096396288.0000\n",
            "Epoch 942/1000\n",
            "80/80 [==============================] - 0s 980us/step - loss: 41582981939.2000 - val_loss: 5312427008.0000\n",
            "Epoch 943/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 37650741248.0000 - val_loss: 9556198400.0000\n",
            "Epoch 944/1000\n",
            "80/80 [==============================] - 0s 942us/step - loss: 34073694208.0000 - val_loss: 22271627264.0000\n",
            "Epoch 945/1000\n",
            "80/80 [==============================] - 0s 929us/step - loss: 38891067801.6000 - val_loss: 8282371072.0000\n",
            "Epoch 946/1000\n",
            "80/80 [==============================] - 0s 949us/step - loss: 44782875443.2000 - val_loss: 12933105664.0000\n",
            "Epoch 947/1000\n",
            "80/80 [==============================] - 0s 939us/step - loss: 51249896652.8000 - val_loss: 24934486016.0000\n",
            "Epoch 948/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 48301706444.8000 - val_loss: 4833786880.0000\n",
            "Epoch 949/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 32316857548.8000 - val_loss: 4182067712.0000\n",
            "Epoch 950/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 30994455347.2000 - val_loss: 13999194112.0000\n",
            "Epoch 951/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 32878763212.8000 - val_loss: 8796036096.0000\n",
            "Epoch 952/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 28743044710.4000 - val_loss: 6195090432.0000\n",
            "Epoch 953/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 33837339852.8000 - val_loss: 13332002816.0000\n",
            "Epoch 954/1000\n",
            "80/80 [==============================] - 0s 917us/step - loss: 29364482048.0000 - val_loss: 30578905088.0000\n",
            "Epoch 955/1000\n",
            "80/80 [==============================] - 0s 995us/step - loss: 33341826662.4000 - val_loss: 3985248768.0000\n",
            "Epoch 956/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 31920439705.6000 - val_loss: 11651386368.0000\n",
            "Epoch 957/1000\n",
            "80/80 [==============================] - 0s 994us/step - loss: 37639365836.8000 - val_loss: 9058919424.0000\n",
            "Epoch 958/1000\n",
            "80/80 [==============================] - 0s 966us/step - loss: 30495741542.4000 - val_loss: 7611737600.0000\n",
            "Epoch 959/1000\n",
            "80/80 [==============================] - 0s 976us/step - loss: 29610842931.2000 - val_loss: 7242390528.0000\n",
            "Epoch 960/1000\n",
            "80/80 [==============================] - 0s 987us/step - loss: 29733080678.4000 - val_loss: 31028432896.0000\n",
            "Epoch 961/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 31608056627.2000 - val_loss: 12087825408.0000\n",
            "Epoch 962/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 33836935987.2000 - val_loss: 6653137920.0000\n",
            "Epoch 963/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 32282634649.6000 - val_loss: 13352888320.0000\n",
            "Epoch 964/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 27802949222.4000 - val_loss: 4327384576.0000\n",
            "Epoch 965/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 28793887539.2000 - val_loss: 4640841728.0000\n",
            "Epoch 966/1000\n",
            "80/80 [==============================] - 0s 940us/step - loss: 26770570035.2000 - val_loss: 7495207936.0000\n",
            "Epoch 967/1000\n",
            "80/80 [==============================] - 0s 960us/step - loss: 29620616396.8000 - val_loss: 8440582144.0000\n",
            "Epoch 968/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26790318080.0000 - val_loss: 10793148416.0000\n",
            "Epoch 969/1000\n",
            "80/80 [==============================] - 0s 957us/step - loss: 27131486208.0000 - val_loss: 9934106624.0000\n",
            "Epoch 970/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 26819044966.4000 - val_loss: 6013801472.0000\n",
            "Epoch 971/1000\n",
            "80/80 [==============================] - 0s 981us/step - loss: 25728803635.2000 - val_loss: 14139640832.0000\n",
            "Epoch 972/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 25891600793.6000 - val_loss: 12594980864.0000\n",
            "Epoch 973/1000\n",
            "80/80 [==============================] - 0s 962us/step - loss: 26767029043.2000 - val_loss: 7065676800.0000\n",
            "Epoch 974/1000\n",
            "80/80 [==============================] - 0s 990us/step - loss: 25807047884.8000 - val_loss: 5256568832.0000\n",
            "Epoch 975/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 25704446771.2000 - val_loss: 4231236864.0000\n",
            "Epoch 976/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26197478604.8000 - val_loss: 19950800896.0000\n",
            "Epoch 977/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 25630311219.2000 - val_loss: 20791152640.0000\n",
            "Epoch 978/1000\n",
            "80/80 [==============================] - 0s 983us/step - loss: 25247379456.0000 - val_loss: 8138590720.0000\n",
            "Epoch 979/1000\n",
            "80/80 [==============================] - 0s 971us/step - loss: 25791180800.0000 - val_loss: 6206954496.0000\n",
            "Epoch 980/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 28312223744.0000 - val_loss: 10073022464.0000\n",
            "Epoch 981/1000\n",
            "80/80 [==============================] - 0s 996us/step - loss: 30080412876.8000 - val_loss: 6566111744.0000\n",
            "Epoch 982/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 29226563584.0000 - val_loss: 18564165632.0000\n",
            "Epoch 983/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 28008222310.4000 - val_loss: 22587097088.0000\n",
            "Epoch 984/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 35590952550.4000 - val_loss: 8688329728.0000\n",
            "Epoch 985/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 27113984409.6000 - val_loss: 3357028608.0000\n",
            "Epoch 986/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 28929648230.4000 - val_loss: 7118231040.0000\n",
            "Epoch 987/1000\n",
            "80/80 [==============================] - 0s 968us/step - loss: 21755554201.6000 - val_loss: 17785681920.0000\n",
            "Epoch 988/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26970469990.4000 - val_loss: 12754926592.0000\n",
            "Epoch 989/1000\n",
            "80/80 [==============================] - 0s 965us/step - loss: 26075235942.4000 - val_loss: 4396432384.0000\n",
            "Epoch 990/1000\n",
            "80/80 [==============================] - 0s 950us/step - loss: 27584943308.8000 - val_loss: 29067532288.0000\n",
            "Epoch 991/1000\n",
            "80/80 [==============================] - 0s 975us/step - loss: 24568689049.6000 - val_loss: 10438174720.0000\n",
            "Epoch 992/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 22647543398.4000 - val_loss: 7356380672.0000\n",
            "Epoch 993/1000\n",
            "80/80 [==============================] - 0s 959us/step - loss: 23059886899.2000 - val_loss: 7074865152.0000\n",
            "Epoch 994/1000\n",
            "80/80 [==============================] - 0s 984us/step - loss: 21896208384.0000 - val_loss: 5204051968.0000\n",
            "Epoch 995/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 25588368179.2000 - val_loss: 7282335744.0000\n",
            "Epoch 996/1000\n",
            "80/80 [==============================] - 0s 997us/step - loss: 20786685952.0000 - val_loss: 45239050240.0000\n",
            "Epoch 997/1000\n",
            "80/80 [==============================] - 0s 986us/step - loss: 25919142297.6000 - val_loss: 13621757952.0000\n",
            "Epoch 998/1000\n",
            "80/80 [==============================] - 0s 961us/step - loss: 22075048345.6000 - val_loss: 7068775936.0000\n",
            "Epoch 999/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 20657073356.8000 - val_loss: 11539904512.0000\n",
            "Epoch 1000/1000\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 26513450188.8000 - val_loss: 9945444352.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCh_x4H_kzmc",
        "colab_type": "code",
        "outputId": "9b583df4-c006-4405-d33c-df3c42f72903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "results = model.predict(x_test)\n",
        "results.shape\n",
        "y_test.shape\n",
        "plt.scatter(range(20),results,c='r')\n",
        "plt.scatter(range(20),y_test,c='g')\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEDCAYAAAA2k7/eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6RC/B9Uttn4wp9LFQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFO4iIgn0/wHYe+jj1GPvigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGUO1yHzll4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}